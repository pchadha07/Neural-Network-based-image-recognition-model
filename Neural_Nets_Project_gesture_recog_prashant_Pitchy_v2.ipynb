{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN3D Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "#from scipy.misc import imread, imresize\n",
    "import imageio as imgio\n",
    "from skimage.transform import resize\n",
    "import datetime\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(30)\n",
    "#tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/disks/user/project/PROJECT/Project_data'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('/mnt/disks/user/project/PROJECT/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('/mnt/disks/user/project/PROJECT/Project_data/val.csv').readlines())\n",
    "#train_doc = np.random.permutation(open('C://Users/pchadha/Neural_case_study/Project_data/train.csv').readlines())\n",
    "#val_doc = np.random.permutation(open('C://Users/pchadha/Neural_case_study/Project_data/val.csv').readlines())\n",
    "batch_size = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_path\n",
    "#folder_list\n",
    "#source_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29] #Selecting second half of images as the differentiating actions within video would most likely be represented by these images  \n",
    "    #img_idx = [x for x in range(12,30)] # Selecting three more images apart from second 15\n",
    "    #img_idx = [x for x in range(0,30,2)] #selected for second and third model and fifth model\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),100,100,3))# x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            #batch_data = np.zeros((batch_size,len(img_idx),64,64,3))\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    #image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = imgio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    # Cropping the image for second model analysis by 5%\n",
    "                    #center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                    #width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                    #left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                    #top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                    #image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:] Cropped image for model 2 to model 6. Removed it for model 7\n",
    "                    image = resize(image, (100,100,3)) # first model instance and fourth model\n",
    "                    # resizing image to 64x64 for second  and third iteration\n",
    "                    #image = resize(image, (64,64,3))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                #print(batch_labels.shape(0))\n",
    "                #print(image.shape(0))\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        diff = len(folder_list) - (num_batches*batch_size)\n",
    "        last_batch_size = diff\n",
    "        batch_data = np.zeros((last_batch_size,len(img_idx),100,100,3)) # first model instance and fourth model\n",
    "        #batch_data = np.zeros((last_batch_size,len(img_idx),64,64,3)) #second and third model instance\n",
    "        batch_labels = np.zeros((last_batch_size,5))\n",
    "        for fd in range(diff):\n",
    "            imgs = os.listdir(source_path+'/'+ t[fd + (num_batches*batch_size)].split(';')[0])\n",
    "            for idx,item in enumerate(img_idx):\n",
    "                image = imgio.imread(source_path+'/'+ t[fd + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                # Cropping the image for second model analysis by 5%\n",
    "                #center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                #width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                #left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                #top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                #image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:]\n",
    "                image = resize(image, (100,100,3)) #first model instance and fourth model\n",
    "                #image = resize(image, (64,64,3)) # second and third model instance\n",
    "                batch_data[fd,idx,:,:,0] = image[:,:,0]/255 \n",
    "                batch_data[fd,idx,:,:,1] = image[:,:,1]/255 \n",
    "                batch_data[fd,idx,:,:,2] = image[:,:,2]/255\n",
    "            batch_labels[fd, int(t[fd + (num_batches*batch_size)].strip().split(';')[2])] = 1    \n",
    "        yield batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "#print(len(train_doc)//20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object generator at 0x000002507682C048>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generator(source_path, train_doc, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 30\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/mnt/disks/user/project/PROJECT/Project_data/train'\n",
    "#train_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "val_path = '/mnt/disks/user/project/PROJECT/Project_data/val'\n",
    "#val_path = 'C://Users/pchadha/Neural_case_study/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 30# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Dropout, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras import optimizers, regularizers\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "\n",
    "#write your model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizing CNN3D first for model architecture\n",
    "We will first, build a network with:\n",
    "- Three convolutional layers having 32, 32 and 64 filters respectively, \n",
    "- followed by a max pooling layer, \n",
    "- and then `Flatten` the output of the pooling layer to give us a long vector, \n",
    "- then add a fully connected `Dense` layer with 128 neurons, and finally\n",
    "- add a `softmax` layer with 5 neurons\n",
    "\n",
    "The generic way to build a model in Keras is to instantiate a `Sequential` model and keep adding `keras.layers` to it. We will also use some dropouts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Conv3D in module keras.layers.convolutional:\n",
      "\n",
      "class Conv3D(_Conv)\n",
      " |  Conv3D(filters, kernel_size, strides=(1, 1, 1), padding='valid', data_format=None, dilation_rate=(1, 1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
      " |  \n",
      " |  3D convolution layer (e.g. spatial convolution over volumes).\n",
      " |  \n",
      " |  This layer creates a convolution kernel that is convolved\n",
      " |  with the layer input to produce a tensor of\n",
      " |  outputs. If `use_bias` is True,\n",
      " |  a bias vector is created and added to the outputs. Finally, if\n",
      " |  `activation` is not `None`, it is applied to the outputs as well.\n",
      " |  \n",
      " |  When using this layer as the first layer in a model,\n",
      " |  provide the keyword argument `input_shape`\n",
      " |  (tuple of integers, does not include the batch axis),\n",
      " |  e.g. `input_shape=(128, 128, 128, 1)` for 128x128x128 volumes\n",
      " |  with a single channel,\n",
      " |  in `data_format=\"channels_last\"`.\n",
      " |  \n",
      " |  # Arguments\n",
      " |      filters: Integer, the dimensionality of the output space\n",
      " |          (i.e. the number of output filters in the convolution).\n",
      " |      kernel_size: An integer or tuple/list of 3 integers, specifying the\n",
      " |          depth, height and width of the 3D convolution window.\n",
      " |          Can be a single integer to specify the same value for\n",
      " |          all spatial dimensions.\n",
      " |      strides: An integer or tuple/list of 3 integers,\n",
      " |          specifying the strides of the convolution along each spatial dimension.\n",
      " |          Can be a single integer to specify the same value for\n",
      " |          all spatial dimensions.\n",
      " |          Specifying any stride value != 1 is incompatible with specifying\n",
      " |          any `dilation_rate` value != 1.\n",
      " |      padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n",
      " |      data_format: A string,\n",
      " |          one of `\"channels_last\"` or `\"channels_first\"`.\n",
      " |          The ordering of the dimensions in the inputs.\n",
      " |          `\"channels_last\"` corresponds to inputs with shape\n",
      " |          `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n",
      " |          while `\"channels_first\"` corresponds to inputs with shape\n",
      " |          `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n",
      " |          It defaults to the `image_data_format` value found in your\n",
      " |          Keras config file at `~/.keras/keras.json`.\n",
      " |          If you never set it, then it will be \"channels_last\".\n",
      " |      dilation_rate: an integer or tuple/list of 3 integers, specifying\n",
      " |          the dilation rate to use for dilated convolution.\n",
      " |          Can be a single integer to specify the same value for\n",
      " |          all spatial dimensions.\n",
      " |          Currently, specifying any `dilation_rate` value != 1 is\n",
      " |          incompatible with specifying any stride value != 1.\n",
      " |      activation: Activation function to use\n",
      " |          (see [activations](../activations.md)).\n",
      " |          If you don't specify anything, no activation is applied\n",
      " |          (ie. \"linear\" activation: `a(x) = x`).\n",
      " |      use_bias: Boolean, whether the layer uses a bias vector.\n",
      " |      kernel_initializer: Initializer for the `kernel` weights matrix\n",
      " |          (see [initializers](../initializers.md)).\n",
      " |      bias_initializer: Initializer for the bias vector\n",
      " |          (see [initializers](../initializers.md)).\n",
      " |      kernel_regularizer: Regularizer function applied to\n",
      " |          the `kernel` weights matrix\n",
      " |          (see [regularizer](../regularizers.md)).\n",
      " |      bias_regularizer: Regularizer function applied to the bias vector\n",
      " |          (see [regularizer](../regularizers.md)).\n",
      " |      activity_regularizer: Regularizer function applied to\n",
      " |          the output of the layer (its \"activation\").\n",
      " |          (see [regularizer](../regularizers.md)).\n",
      " |      kernel_constraint: Constraint function applied to the kernel matrix\n",
      " |          (see [constraints](../constraints.md)).\n",
      " |      bias_constraint: Constraint function applied to the bias vector\n",
      " |          (see [constraints](../constraints.md)).\n",
      " |  \n",
      " |  # Input shape\n",
      " |      5D tensor with shape:\n",
      " |      `(batch, channels, conv_dim1, conv_dim2, conv_dim3)`\n",
      " |      if `data_format` is `\"channels_first\"`\n",
      " |      or 5D tensor with shape:\n",
      " |      `(batch, conv_dim1, conv_dim2, conv_dim3, channels)`\n",
      " |      if `data_format` is `\"channels_last\"`.\n",
      " |  \n",
      " |  # Output shape\n",
      " |      5D tensor with shape:\n",
      " |      `(batch, filters, new_conv_dim1, new_conv_dim2, new_conv_dim3)`\n",
      " |      if `data_format` is `\"channels_first\"`\n",
      " |      or 5D tensor with shape:\n",
      " |      `(batch, new_conv_dim1, new_conv_dim2, new_conv_dim3, filters)`\n",
      " |      if `data_format` is `\"channels_last\"`.\n",
      " |      `new_conv_dim1`, `new_conv_dim2` and `new_conv_dim3` values might have\n",
      " |      changed due to padding.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Conv3D\n",
      " |      _Conv\n",
      " |      keras.engine.base_layer.Layer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, filters, kernel_size, strides=(1, 1, 1), padding='valid', data_format=None, dilation_rate=(1, 1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      # Returns\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _Conv:\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the layer weights.\n",
      " |      \n",
      " |      Must be implemented on all layers that have weights.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          input_shape: Keras tensor (future input to layer)\n",
      " |              or list/tuple of Keras tensors to reference\n",
      " |              for weight shape computations.\n",
      " |  \n",
      " |  call(self, inputs)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Input tensor, or list/tuple of input tensors.\n",
      " |          **kwargs: Additional keyword arguments.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      Assumes that the layer will be built\n",
      " |      to match that input shape provided.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          An output shape tuple.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, inputs, **kwargs)\n",
      " |      Wrapper around self.call(), for handling internal references.\n",
      " |      \n",
      " |      If a Keras tensor is passed:\n",
      " |          - We call self._add_inbound_node().\n",
      " |          - If necessary, we `build` the layer to match\n",
      " |              the _keras_shape of the input(s).\n",
      " |          - We update the _keras_shape of every input tensor with\n",
      " |              its new shape (obtained via self.compute_output_shape).\n",
      " |              This is done as part of _add_inbound_node().\n",
      " |          - We update the _keras_history of the output tensor(s)\n",
      " |              with the current layer.\n",
      " |              This is done as part of _add_inbound_node().\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Can be a tensor or list/tuple of tensors.\n",
      " |          **kwargs: Additional keyword arguments to be passed to `call()`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output of the layer's `call` method.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: in case the layer is missing shape information\n",
      " |              for its `build` call.\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  add_loss(self, losses, inputs=None)\n",
      " |      Adds losses to the layer.\n",
      " |      \n",
      " |      The loss may potentially be conditional on some inputs tensors,\n",
      " |      for instance activity losses are conditional on the layer's inputs.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          losses: loss tensor or list of loss tensors\n",
      " |              to add to the layer.\n",
      " |          inputs: input tensor or list of inputs tensors to mark\n",
      " |              the losses as conditional on these inputs.\n",
      " |              If None is passed, the loss is assumed unconditional\n",
      " |              (e.g. L2 weight regularization, which only depends\n",
      " |              on the layer's weights variables, not on any inputs tensors).\n",
      " |  \n",
      " |  add_metric(self, value, name=None)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          value: Metric tensor.\n",
      " |          name: String metric name.\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Adds updates to the layer.\n",
      " |      \n",
      " |      The updates may potentially be conditional on some inputs tensors,\n",
      " |      for instance batch norm updates are conditional on the layer's inputs.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          updates: update op or list of update ops\n",
      " |              to add to the layer.\n",
      " |          inputs: input tensor or list of inputs tensors to mark\n",
      " |              the updates as conditional on these inputs.\n",
      " |              If None is passed, the updates are assumed unconditional.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=True, constraint=None)\n",
      " |      Adds a weight variable to the layer.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          name: String, the name for the weight variable.\n",
      " |          shape: The shape tuple of the weight.\n",
      " |          dtype: The dtype of the weight.\n",
      " |          initializer: An Initializer instance (callable).\n",
      " |          regularizer: An optional Regularizer instance.\n",
      " |          trainable: A boolean, whether the weight should\n",
      " |              be trained via backprop or not (assuming\n",
      " |              that the layer itself is also trainable).\n",
      " |          constraint: An optional Constraint instance.\n",
      " |      \n",
      " |      # Returns\n",
      " |          The created weight variable.\n",
      " |  \n",
      " |  assert_input_compatibility(self, inputs)\n",
      " |      Checks compatibility between the layer and provided inputs.\n",
      " |      \n",
      " |      This checks that the tensor(s) `input`\n",
      " |      verify the input assumptions of the layer\n",
      " |      (if any). If not, exceptions are raised.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: input tensor or list of input tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: in case of mismatch between\n",
      " |              the provided inputs and the expectations of the layer.\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      # Returns\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Counts the total number of scalars composing the weights.\n",
      " |      \n",
      " |      # Returns\n",
      " |          An integer count.\n",
      " |      \n",
      " |      # Raises\n",
      " |          RuntimeError: if the layer isn't yet built\n",
      " |              (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Weights values as a list of numpy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from Numpy arrays.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the layer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: If the provided weights list does not match the\n",
      " |              layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      # Arguments\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  built\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape tuple(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input shape tuple\n",
      " |          (or list of input shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  losses\n",
      " |  \n",
      " |  metrics\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output tensor or list of output tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape tuple(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one inbound node,\n",
      " |      or if all inbound nodes have the same output shape.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output shape tuple\n",
      " |          (or list of input shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  trainable_weights\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  weights\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Conv3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MaxPooling3D in module keras.layers.pooling:\n",
      "\n",
      "class MaxPooling3D(_Pooling3D)\n",
      " |  MaxPooling3D(pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None, **kwargs)\n",
      " |  \n",
      " |  Max pooling operation for 3D data (spatial or spatio-temporal).\n",
      " |  \n",
      " |  # Arguments\n",
      " |      pool_size: tuple of 3 integers,\n",
      " |          factors by which to downscale (dim1, dim2, dim3).\n",
      " |          (2, 2, 2) will halve the size of the 3D input in each dimension.\n",
      " |      strides: tuple of 3 integers, or None. Strides values.\n",
      " |      padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n",
      " |      data_format: A string,\n",
      " |          one of `channels_last` (default) or `channels_first`.\n",
      " |          The ordering of the dimensions in the inputs.\n",
      " |          `channels_last` corresponds to inputs with shape\n",
      " |          `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n",
      " |          while `channels_first` corresponds to inputs with shape\n",
      " |          `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n",
      " |          It defaults to the `image_data_format` value found in your\n",
      " |          Keras config file at `~/.keras/keras.json`.\n",
      " |          If you never set it, then it will be \"channels_last\".\n",
      " |  \n",
      " |  # Input shape\n",
      " |      - If `data_format='channels_last'`:\n",
      " |          5D tensor with shape:\n",
      " |          `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n",
      " |      - If `data_format='channels_first'`:\n",
      " |          5D tensor with shape:\n",
      " |          `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`\n",
      " |  \n",
      " |  # Output shape\n",
      " |      - If `data_format='channels_last'`:\n",
      " |          5D tensor with shape:\n",
      " |          `(batch_size, pooled_dim1, pooled_dim2, pooled_dim3, channels)`\n",
      " |      - If `data_format='channels_first'`:\n",
      " |          5D tensor with shape:\n",
      " |          `(batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3)`\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MaxPooling3D\n",
      " |      _Pooling3D\n",
      " |      keras.engine.base_layer.Layer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _Pooling3D:\n",
      " |  \n",
      " |  call(self, inputs)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Input tensor, or list/tuple of input tensors.\n",
      " |          **kwargs: Additional keyword arguments.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      Assumes that the layer will be built\n",
      " |      to match that input shape provided.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          An output shape tuple.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      # Returns\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, inputs, **kwargs)\n",
      " |      Wrapper around self.call(), for handling internal references.\n",
      " |      \n",
      " |      If a Keras tensor is passed:\n",
      " |          - We call self._add_inbound_node().\n",
      " |          - If necessary, we `build` the layer to match\n",
      " |              the _keras_shape of the input(s).\n",
      " |          - We update the _keras_shape of every input tensor with\n",
      " |              its new shape (obtained via self.compute_output_shape).\n",
      " |              This is done as part of _add_inbound_node().\n",
      " |          - We update the _keras_history of the output tensor(s)\n",
      " |              with the current layer.\n",
      " |              This is done as part of _add_inbound_node().\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Can be a tensor or list/tuple of tensors.\n",
      " |          **kwargs: Additional keyword arguments to be passed to `call()`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output of the layer's `call` method.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: in case the layer is missing shape information\n",
      " |              for its `build` call.\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  add_loss(self, losses, inputs=None)\n",
      " |      Adds losses to the layer.\n",
      " |      \n",
      " |      The loss may potentially be conditional on some inputs tensors,\n",
      " |      for instance activity losses are conditional on the layer's inputs.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          losses: loss tensor or list of loss tensors\n",
      " |              to add to the layer.\n",
      " |          inputs: input tensor or list of inputs tensors to mark\n",
      " |              the losses as conditional on these inputs.\n",
      " |              If None is passed, the loss is assumed unconditional\n",
      " |              (e.g. L2 weight regularization, which only depends\n",
      " |              on the layer's weights variables, not on any inputs tensors).\n",
      " |  \n",
      " |  add_metric(self, value, name=None)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          value: Metric tensor.\n",
      " |          name: String metric name.\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Adds updates to the layer.\n",
      " |      \n",
      " |      The updates may potentially be conditional on some inputs tensors,\n",
      " |      for instance batch norm updates are conditional on the layer's inputs.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          updates: update op or list of update ops\n",
      " |              to add to the layer.\n",
      " |          inputs: input tensor or list of inputs tensors to mark\n",
      " |              the updates as conditional on these inputs.\n",
      " |              If None is passed, the updates are assumed unconditional.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=True, constraint=None)\n",
      " |      Adds a weight variable to the layer.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          name: String, the name for the weight variable.\n",
      " |          shape: The shape tuple of the weight.\n",
      " |          dtype: The dtype of the weight.\n",
      " |          initializer: An Initializer instance (callable).\n",
      " |          regularizer: An optional Regularizer instance.\n",
      " |          trainable: A boolean, whether the weight should\n",
      " |              be trained via backprop or not (assuming\n",
      " |              that the layer itself is also trainable).\n",
      " |          constraint: An optional Constraint instance.\n",
      " |      \n",
      " |      # Returns\n",
      " |          The created weight variable.\n",
      " |  \n",
      " |  assert_input_compatibility(self, inputs)\n",
      " |      Checks compatibility between the layer and provided inputs.\n",
      " |      \n",
      " |      This checks that the tensor(s) `input`\n",
      " |      verify the input assumptions of the layer\n",
      " |      (if any). If not, exceptions are raised.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: input tensor or list of input tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: in case of mismatch between\n",
      " |              the provided inputs and the expectations of the layer.\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the layer weights.\n",
      " |      \n",
      " |      Must be implemented on all layers that have weights.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          input_shape: Keras tensor (future input to layer)\n",
      " |              or list/tuple of Keras tensors to reference\n",
      " |              for weight shape computations.\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      # Returns\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Counts the total number of scalars composing the weights.\n",
      " |      \n",
      " |      # Returns\n",
      " |          An integer count.\n",
      " |      \n",
      " |      # Raises\n",
      " |          RuntimeError: if the layer isn't yet built\n",
      " |              (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Weights values as a list of numpy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from Numpy arrays.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the layer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: If the provided weights list does not match the\n",
      " |              layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      # Arguments\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  built\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape tuple(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input shape tuple\n",
      " |          (or list of input shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  losses\n",
      " |  \n",
      " |  metrics\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output tensor or list of output tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape tuple(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one inbound node,\n",
      " |      or if all inbound nodes have the same output shape.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output shape tuple\n",
      " |          (or list of input shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  trainable_weights\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  weights\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MaxPooling3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model1 = Sequential()\n",
    "#help(Conv3D)\n",
    "# a keras convolutional layer is called Conv3D\n",
    "\n",
    "# note that the first layer needs to be told the input shape explicitly\n",
    "\n",
    "# first conv layer\n",
    "img_rows = 100\n",
    "img_cols = 100\n",
    "imnum = 15 # len(img_idx)\n",
    "num_classes = 5\n",
    "input_shape = (imnum, img_rows, img_cols, 3)\n",
    "model1.add(Conv3D(32, kernel_size=(3, 3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape)) # input shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# second conv layer\n",
    "model1.add(Conv3D(32, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model1.add(Dropout(0.25))\n",
    "\n",
    "# Third conv layer\n",
    "model1.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model1.add(Dropout(0.25))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(128, activation='relu')) # fully connected\n",
    "model1.add(Dropout(0.5))\n",
    "\n",
    "# softmax layer\n",
    "model1.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model summary\n",
    "#model1.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 13, 98, 98, 32)    2624      \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 11, 96, 96, 32)    27680     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 5, 48, 48, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 48, 48, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 3, 46, 46, 64)     55360     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 1, 23, 23, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 23, 23, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 33856)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               4333696   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 4,420,005\n",
      "Trainable params: 4,420,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#learning rate = 0.01 first model value\n",
    "learning_rate = 0.002 # second, third and fourth model value\n",
    "optimiser = Adam(lr=learning_rate,clipvalue=1.0)\n",
    "model1.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pras_model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0227 18:49:19.868011 139832974251840 deprecation.py:323] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0227 18:49:20.164610 139832974251840 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0227 18:49:20.297953 139832974251840 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "W0227 18:49:20.430096 139832974251840 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/val ; batch size = 15\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0227 18:49:22.825787 139832974251840 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0227 18:49:22.828643 139832974251840 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0227 18:49:25.215645 139832974251840 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W0227 18:49:25.216946 139832974251840 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "W0227 18:49:26.005730 139832974251840 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 70s 2s/step - loss: 1.7085 - categorical_accuracy: 0.2177 - val_loss: 1.4390 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00001: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00001-1.72119-0.21569-1.43902-0.24000.h5\n",
      "Epoch 2/30\n",
      "45/45 [==============================] - 82s 2s/step - loss: 1.4626 - categorical_accuracy: 0.3246 - val_loss: 1.2958 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00002: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00002-1.45844-0.33032-1.29579-0.44000.h5\n",
      "Epoch 3/30\n",
      "45/45 [==============================] - 71s 2s/step - loss: 1.3504 - categorical_accuracy: 0.4074 - val_loss: 1.2456 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00003: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00003-1.35062-0.40875-1.24558-0.53000.h5\n",
      "Epoch 4/30\n",
      "45/45 [==============================] - 82s 2s/step - loss: 1.2073 - categorical_accuracy: 0.5081 - val_loss: 1.1552 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00004: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00004-1.20984-0.50528-1.15518-0.47000.h5\n",
      "Epoch 5/30\n",
      "45/45 [==============================] - 64s 1s/step - loss: 1.0435 - categorical_accuracy: 0.5350 - val_loss: 1.1104 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00005: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00005-1.03036-0.54449-1.11040-0.55000.h5\n",
      "Epoch 6/30\n",
      "45/45 [==============================] - 66s 1s/step - loss: 0.8736 - categorical_accuracy: 0.6547 - val_loss: 1.0979 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00006: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00006-0.88073-0.64857-1.09792-0.58000.h5\n",
      "Epoch 7/30\n",
      "45/45 [==============================] - 65s 1s/step - loss: 0.7307 - categorical_accuracy: 0.7052 - val_loss: 1.0430 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00007: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00007-0.73366-0.70588-1.04304-0.61000.h5\n",
      "Epoch 8/30\n",
      "45/45 [==============================] - 82s 2s/step - loss: 0.5845 - categorical_accuracy: 0.7732 - val_loss: 1.1681 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00008: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00008-0.58689-0.76923-1.16807-0.55000.h5\n",
      "Epoch 9/30\n",
      "45/45 [==============================] - 74s 2s/step - loss: 0.5918 - categorical_accuracy: 0.7658 - val_loss: 1.0179 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00009: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00009-0.60024-0.76169-1.01790-0.63000.h5\n",
      "Epoch 10/30\n",
      "45/45 [==============================] - 73s 2s/step - loss: 0.4246 - categorical_accuracy: 0.8651 - val_loss: 1.3751 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00010: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00010-0.43209-0.86275-1.37507-0.59000.h5\n",
      "Epoch 11/30\n",
      "45/45 [==============================] - 84s 2s/step - loss: 0.3747 - categorical_accuracy: 0.8638 - val_loss: 1.2761 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00011: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00011-0.36287-0.86727-1.27609-0.62000.h5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 12/30\n",
      "45/45 [==============================] - 61s 1s/step - loss: 0.2818 - categorical_accuracy: 0.9111 - val_loss: 1.0890 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00012: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00012-0.28650-0.90950-1.08902-0.62000.h5\n",
      "Epoch 13/30\n",
      "45/45 [==============================] - 72s 2s/step - loss: 0.1834 - categorical_accuracy: 0.9481 - val_loss: 1.2088 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00013: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00013-0.18238-0.94721-1.20876-0.60000.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 14/30\n",
      "45/45 [==============================] - 62s 1s/step - loss: 0.1871 - categorical_accuracy: 0.9289 - val_loss: 1.1771 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00014: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00014-0.18772-0.92760-1.17707-0.63000.h5\n",
      "Epoch 15/30\n",
      "45/45 [==============================] - 69s 2s/step - loss: 0.1885 - categorical_accuracy: 0.9274 - val_loss: 1.1869 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00015: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00015-0.18924-0.92609-1.18687-0.63000.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 16/30\n",
      "45/45 [==============================] - 81s 2s/step - loss: 0.1426 - categorical_accuracy: 0.9570 - val_loss: 1.1853 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00016: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00016-0.14472-0.95626-1.18529-0.63000.h5\n",
      "Epoch 17/30\n",
      "45/45 [==============================] - 84s 2s/step - loss: 0.1797 - categorical_accuracy: 0.9303 - val_loss: 1.1797 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00017: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00017-0.18277-0.92911-1.17968-0.62000.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 18/30\n",
      "45/45 [==============================] - 72s 2s/step - loss: 0.1673 - categorical_accuracy: 0.9466 - val_loss: 1.1785 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00018: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00018-0.16768-0.94570-1.17848-0.62000.h5\n",
      "Epoch 19/30\n",
      "45/45 [==============================] - 84s 2s/step - loss: 0.1609 - categorical_accuracy: 0.9407 - val_loss: 1.1796 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00019: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00019-0.15911-0.93967-1.17964-0.62000.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 20/30\n",
      "45/45 [==============================] - 83s 2s/step - loss: 0.2097 - categorical_accuracy: 0.9186 - val_loss: 1.1797 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00020: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00020-0.19874-0.92308-1.17974-0.62000.h5\n",
      "Epoch 21/30\n",
      "45/45 [==============================] - 83s 2s/step - loss: 0.1693 - categorical_accuracy: 0.9363 - val_loss: 1.1796 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00021: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00021-0.17075-0.93514-1.17961-0.62000.h5\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "Epoch 22/30\n",
      "45/45 [==============================] - 61s 1s/step - loss: 0.1746 - categorical_accuracy: 0.9363 - val_loss: 1.1796 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00022: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00022-0.17676-0.93514-1.17958-0.62000.h5\n",
      "Epoch 23/30\n",
      "45/45 [==============================] - 82s 2s/step - loss: 0.1685 - categorical_accuracy: 0.9496 - val_loss: 1.1796 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00023: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00023-0.17094-0.94872-1.17959-0.62000.h5\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
      "Epoch 24/30\n",
      "45/45 [==============================] - 79s 2s/step - loss: 0.1759 - categorical_accuracy: 0.9318 - val_loss: 1.1796 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00024: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00024-0.17708-0.93062-1.17960-0.62000.h5\n",
      "Epoch 25/30\n",
      "45/45 [==============================] - 80s 2s/step - loss: 0.1656 - categorical_accuracy: 0.9541 - val_loss: 1.1796 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00025: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00025-0.16747-0.95324-1.17960-0.62000.h5\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
      "Epoch 26/30\n",
      "45/45 [==============================] - 67s 1s/step - loss: 0.1569 - categorical_accuracy: 0.9481 - val_loss: 1.1796 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00026: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00026-0.15624-0.94721-1.17960-0.62000.h5\n",
      "Epoch 27/30\n",
      "45/45 [==============================] - 65s 1s/step - loss: 0.1727 - categorical_accuracy: 0.9408 - val_loss: 1.1796 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00027: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00027-0.16542-0.94570-1.17960-0.62000.h5\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 84s 2s/step - loss: 0.1705 - categorical_accuracy: 0.9437 - val_loss: 1.1796 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00028: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00028-0.17256-0.94268-1.17960-0.62000.h5\n",
      "Epoch 29/30\n",
      "45/45 [==============================] - 82s 2s/step - loss: 0.1718 - categorical_accuracy: 0.9453 - val_loss: 1.1796 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00029: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00029-0.16645-0.95023-1.17960-0.62000.h5\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
      "Epoch 30/30\n",
      "45/45 [==============================] - 67s 1s/step - loss: 0.1957 - categorical_accuracy: 0.9392 - val_loss: 1.1796 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00030: saving model to pras_model_init_2020-02-2718_48_37.503224/model-00030-0.19800-0.93816-1.17960-0.62000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2cd59e8940>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit or training process\n",
    "model1.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selected last 15 images for first trial and resized the image to 100x100.\n",
    "Tried with batch size from 50 to 15 and epoch size set to 40. Larger the batch size, its generally better however, was getting 'kernal has died' issues which was not resolved even after getting container machine tensorflow updated to version 1.5.\n",
    "In the end, batch size 15 resolved the issue. Epoch size was kept to be 30.\n",
    "It was observed that majority of loss reduction was done by epoch 11-12. After that learning rate was being reduced regularly due to no loss reduction for 2 consecutive epochs (original settings)\n",
    "Will check the accuracy at the end and play with hyper parameters if its not high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_categorical_accuracy', 'loss', 'categorical_accuracy', 'lr'])\n"
     ]
    }
   ],
   "source": [
    "# Checking the accuracy\n",
    "print(model1.history.history.keys())\n",
    "#print(model1.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2156862815108774, 0.33031675324046234, 0.4087481276549365, 0.5052790500189923, 0.5444947390804463, 0.6485671353825616, 0.7058823697976937, 0.7692307829856873, 0.761689305305481, 0.8627451062202454, 0.8672699928283691, 0.9095022678375244, 0.9472096562385559, 0.9276018142700195, 0.9260935187339783, 0.9562594294548035, 0.9291101098060608, 0.9457013607025146, 0.9396681785583496, 0.9230769276618958, 0.9351432919502258, 0.9351432919502258, 0.9487179517745972, 0.930618405342102, 0.953242838382721, 0.9472096562385559, 0.9457013607025146, 0.9426847696304321, 0.9502262473106384, 0.9381598830223083]\n"
     ]
    }
   ],
   "source": [
    "print(model1.history.history['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24000000730156898, 0.4400000125169754, 0.530000014603138, 0.47000001519918444, 0.5500000104308128, 0.5800000131130219, 0.6100000113248825, 0.5500000149011612, 0.6300000175833702, 0.5900000125169754, 0.6200000137090683, 0.6200000137090683, 0.600000011920929, 0.6300000131130219, 0.6300000190734864, 0.6300000146031379, 0.6200000092387199, 0.6200000181794166, 0.6200000151991845, 0.6200000181794166, 0.6200000226497651, 0.6200000151991845, 0.6200000181794166, 0.6200000226497651, 0.6200000137090683, 0.6200000137090683, 0.6200000151991845, 0.6200000151991845, 0.620000010728836, 0.620000010728836]\n"
     ]
    }
   ],
   "source": [
    "print(model1.history.history['val_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there's lot of difference between training data accuracy and validation data accuracy, training data accuracy being 94%, while validation data, accuracy is just 62%. This is clearly case of overfitting. Therefore, will choose different set of hyper parameter values, and will also select alternate 15 images, instead of last 15.\n",
    "Will resize the image to 64x64 as well after cropping it by 15 % (centered cropping) using a crop function\n",
    "Also, will add batch normalization to the two hidden layer network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second model iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2\n",
    "model2 = Sequential()\n",
    "#help(Conv3D)\n",
    "# a keras convolutional layer is called Conv3D\n",
    "\n",
    "# note that the first layer needs to be told the input shape explicitly\n",
    "\n",
    "# first conv layer\n",
    "img_rows = 64\n",
    "img_cols = 64\n",
    "imnum = 15 # len(img_idx)\n",
    "num_classes = 5\n",
    "input_shape = (imnum, img_rows, img_cols, 3)\n",
    "model2.add(Conv3D(32, kernel_size=(3, 3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape)) # input shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# second conv layer\n",
    "model2.add(Conv3D(32, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model2.add(Dropout(0.25))\n",
    "\n",
    "# Third conv layer\n",
    "model2.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model2.add(Dropout(0.25))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(128, activation='relu')) # fully connected\n",
    "model2.add(Dropout(0.5))\n",
    "\n",
    "# softmax layer\n",
    "model2.add(Dense(num_classes, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_10 (Conv3D)           (None, 13, 62, 62, 32)    2624      \n",
      "_________________________________________________________________\n",
      "conv3d_11 (Conv3D)           (None, 11, 60, 60, 32)    27680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 11, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_7 (MaxPooling3 (None, 5, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 5, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_12 (Conv3D)           (None, 3, 28, 28, 64)     55360     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 3, 28, 28, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_8 (MaxPooling3 (None, 1, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               1605760   \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 1,692,453\n",
      "Trainable params: 1,692,261\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.002 # second model value\n",
    "optimiser = Adam(lr=learning_rate,clipvalue=1.0)\n",
    "model2.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator` for second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pras_model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/val ; batch size = 15\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 15\n",
      "45/45 [==============================] - 133s 3s/step - loss: 11.5531 - categorical_accuracy: 0.2235 - val_loss: 12.9008 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00001: saving model to pras_model_init_2020-02-2721_08_51.128419/model-00001-11.66245-0.21569-12.90077-0.20000.h5\n",
      "Epoch 2/30\n",
      "45/45 [==============================] - 64s 1s/step - loss: 12.0238 - categorical_accuracy: 0.2312 - val_loss: 10.3598 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00002: saving model to pras_model_init_2020-02-2721_08_51.128419/model-00002-11.95130-0.23529-10.35982-0.35000.h5\n",
      "Epoch 3/30\n",
      "45/45 [==============================] - 53s 1s/step - loss: 11.8746 - categorical_accuracy: 0.2505 - val_loss: 13.2706 - val_categorical_accuracy: 0.1700\n",
      "\n",
      "Epoch 00003: saving model to pras_model_init_2020-02-2721_08_51.128419/model-00003-11.79949-0.25490-13.27056-0.17000.h5\n",
      "Epoch 4/30\n",
      "45/45 [==============================] - 52s 1s/step - loss: 11.9890 - categorical_accuracy: 0.2342 - val_loss: 12.7925 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00004: saving model to pras_model_init_2020-02-2721_08_51.128419/model-00004-11.91593-0.23831-12.79252-0.20000.h5\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "Epoch 5/30\n",
      "45/45 [==============================] - 67s 1s/step - loss: 11.8519 - categorical_accuracy: 0.2460 - val_loss: 12.4178 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00005: saving model to pras_model_init_2020-02-2721_08_51.128419/model-00005-11.77636-0.25038-12.41775-0.23000.h5\n",
      "Epoch 6/30\n",
      "45/45 [==============================] - 61s 1s/step - loss: 11.1111 - categorical_accuracy: 0.2962 - val_loss: 11.9338 - val_categorical_accuracy: 0.2600\n",
      "\n",
      "Epoch 00006: saving model to pras_model_init_2020-02-2721_08_51.128419/model-00006-11.21263-0.28959-11.93382-0.26000.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 8.000000379979611e-05.\n",
      "Epoch 7/30\n",
      "45/45 [==============================] - 48s 1s/step - loss: 10.9432 - categorical_accuracy: 0.3064 - val_loss: 11.9274 - val_categorical_accuracy: 0.2600\n",
      "\n",
      "Epoch 00007: saving model to pras_model_init_2020-02-2721_08_51.128419/model-00007-11.13676-0.29412-11.92739-0.26000.h5\n",
      "Epoch 8/30\n",
      "45/45 [==============================] - 50s 1s/step - loss: 11.2030 - categorical_accuracy: 0.2904 - val_loss: 11.7708 - val_categorical_accuracy: 0.2600\n",
      "\n",
      "Epoch 00008: saving model to pras_model_init_2020-02-2721_08_51.128419/model-00008-11.21114-0.28959-11.77084-0.26000.h5\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.6000001050997525e-05.\n",
      "Epoch 9/30\n",
      "45/45 [==============================] - 53s 1s/step - loss: 10.9490 - categorical_accuracy: 0.3067 - val_loss: 12.2818 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00009: saving model to pras_model_init_2020-02-2721_08_51.128419/model-00009-10.95262-0.30618-12.28181-0.23000.h5\n",
      "Epoch 10/30\n",
      "45/45 [==============================] - 47s 1s/step - loss: 11.1571 - categorical_accuracy: 0.2934 - val_loss: 11.0086 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00010: saving model to pras_model_init_2020-02-2721_08_51.128419/model-00010-11.06933-0.29864-11.00864-0.29000.h5\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.2000003557186575e-06.\n",
      "Epoch 11/30\n",
      "45/45 [==============================] - 66s 1s/step - loss: 11.0764 - categorical_accuracy: 0.2978 - val_loss: 11.2900 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00011: saving model to pras_model_init_2020-02-2721_08_51.128419/model-00011-11.08230-0.29713-11.28998-0.27000.h5\n",
      "Epoch 12/30\n",
      "45/45 [==============================] - 64s 1s/step - loss: 10.5298 - categorical_accuracy: 0.3332 - val_loss: 11.2101 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00012: saving model to pras_model_init_2020-02-2721_08_51.128419/model-00012-10.62109-0.32730-11.21010-0.29000.h5\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-07.\n",
      "Epoch 13/30\n",
      "45/45 [==============================] - 47s 1s/step - loss: 11.0201 - categorical_accuracy: 0.2963 - val_loss: 11.3040 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00013: saving model to pras_model_init_2020-02-2721_08_51.128419/model-00013-11.02501-0.29563-11.30396-0.28000.h5\n",
      "Epoch 14/30\n",
      "39/45 [=========================>....] - ETA: 7s - loss: 11.0100 - categorical_accuracy: 0.2991"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-5f4127d67789>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m model2.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n\u001b[1;32m      3\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n\u001b[0m",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit or training process\n",
    "model2.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2 was not showing improvement, in fact it was showing worse performance\n",
    "Will now keep the image size as 64x64 ony, but will make dense layer more dense, i.e., increase the number of neurons in this layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model3\n",
    "model3 = Sequential()\n",
    "#help(Conv3D)\n",
    "# a keras convolutional layer is called Conv3D\n",
    "\n",
    "# note that the first layer needs to be told the input shape explicitly\n",
    "\n",
    "# first conv layer\n",
    "img_rows = 64\n",
    "img_cols = 64\n",
    "imnum = 15 # len(img_idx)\n",
    "num_classes = 5\n",
    "input_shape = (imnum, img_rows, img_cols, 3)\n",
    "model3.add(Conv3D(32, kernel_size=(3, 3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape)) # input shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# second conv layer\n",
    "model3.add(Conv3D(32, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model3.add(Dropout(0.25))\n",
    "\n",
    "# Third conv layer\n",
    "model3.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model3.add(Dropout(0.25))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(256, activation='relu')) # fully connected\n",
    "model3.add(Dropout(0.5))\n",
    "\n",
    "# softmax layer\n",
    "model3.add(Dense(num_classes, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_16 (Conv3D)           (None, 13, 62, 62, 32)    2624      \n",
      "_________________________________________________________________\n",
      "conv3d_17 (Conv3D)           (None, 11, 60, 60, 32)    27680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 11, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_11 (MaxPooling (None, 5, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 5, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_18 (Conv3D)           (None, 3, 28, 28, 64)     55360     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 3, 28, 28, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_12 (MaxPooling (None, 1, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 1, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               3211520   \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 3,298,853\n",
      "Trainable params: 3,298,661\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001 # Third model value\n",
    "optimiser = Adam(lr=learning_rate,clipvalue=1.0)\n",
    "model3.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator` for second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pras_model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/val ; batch size = 15\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 15\n",
      "45/45 [==============================] - 69s 2s/step - loss: 10.3764 - categorical_accuracy: 0.2534 - val_loss: 12.1830 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00001: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00001-10.36417-0.25792-12.18305-0.23000.h5\n",
      "Epoch 2/30\n",
      "45/45 [==============================] - 65s 1s/step - loss: 11.4839 - categorical_accuracy: 0.2577 - val_loss: 12.5671 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00002: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00002-11.49696-0.25641-12.56706-0.20000.h5\n",
      "Epoch 3/30\n",
      "45/45 [==============================] - 68s 2s/step - loss: 12.6661 - categorical_accuracy: 0.2001 - val_loss: 13.6516 - val_categorical_accuracy: 0.1400\n",
      "\n",
      "Epoch 00003: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00003-12.60497-0.20362-13.65159-0.14000.h5\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 4/30\n",
      "45/45 [==============================] - 46s 1s/step - loss: 12.5050 - categorical_accuracy: 0.2148 - val_loss: 13.7177 - val_categorical_accuracy: 0.1400\n",
      "\n",
      "Epoch 00004: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00004-12.53572-0.21267-13.71774-0.14000.h5\n",
      "Epoch 5/30\n",
      "45/45 [==============================] - 63s 1s/step - loss: 11.8645 - categorical_accuracy: 0.2326 - val_loss: 13.6141 - val_categorical_accuracy: 0.1400\n",
      "\n",
      "Epoch 00005: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00005-11.88430-0.23077-13.61409-0.14000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 6/30\n",
      "45/45 [==============================] - 56s 1s/step - loss: 11.7455 - categorical_accuracy: 0.2474 - val_loss: 13.1609 - val_categorical_accuracy: 0.1500\n",
      "\n",
      "Epoch 00006: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00006-11.76063-0.24585-13.16094-0.15000.h5\n",
      "Epoch 7/30\n",
      "45/45 [==============================] - 52s 1s/step - loss: 11.5354 - categorical_accuracy: 0.2475 - val_loss: 12.6114 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00007: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00007-11.53921-0.25189-12.61142-0.20000.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 8/30\n",
      "45/45 [==============================] - 55s 1s/step - loss: 11.4015 - categorical_accuracy: 0.2697 - val_loss: 12.5322 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00008: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00008-11.31801-0.27451-12.53222-0.22000.h5\n",
      "Epoch 9/30\n",
      "45/45 [==============================] - 68s 2s/step - loss: 11.4608 - categorical_accuracy: 0.2579 - val_loss: 12.6629 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00009: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00009-11.37837-0.26244-12.66289-0.21000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 10/30\n",
      "45/45 [==============================] - 60s 1s/step - loss: 11.5516 - categorical_accuracy: 0.2563 - val_loss: 12.4933 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00010: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00010-11.56591-0.25490-12.49333-0.22000.h5\n",
      "Epoch 11/30\n",
      "45/45 [==============================] - 68s 2s/step - loss: 11.8919 - categorical_accuracy: 0.2311 - val_loss: 12.5728 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00011: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00011-11.91887-0.22926-12.57279-0.22000.h5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 12/30\n",
      "45/45 [==============================] - 51s 1s/step - loss: 11.8713 - categorical_accuracy: 0.2356 - val_loss: 12.4827 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00012: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00012-11.79609-0.23982-12.48269-0.22000.h5\n",
      "Epoch 13/30\n",
      "45/45 [==============================] - 66s 1s/step - loss: 11.7553 - categorical_accuracy: 0.2385 - val_loss: 12.4787 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00013: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00013-11.80952-0.23680-12.47873-0.22000.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "Epoch 14/30\n",
      "45/45 [==============================] - 62s 1s/step - loss: 11.6814 - categorical_accuracy: 0.2591 - val_loss: 12.3189 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00014: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00014-11.79308-0.25189-12.31885-0.23000.h5\n",
      "Epoch 15/30\n",
      "45/45 [==============================] - 49s 1s/step - loss: 11.7644 - categorical_accuracy: 0.2475 - val_loss: 12.2858 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00015: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00015-11.68730-0.25189-12.28582-0.23000.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
      "Epoch 16/30\n",
      "45/45 [==============================] - 63s 1s/step - loss: 12.1433 - categorical_accuracy: 0.2207 - val_loss: 12.4648 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00016: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00016-12.16800-0.21870-12.46476-0.22000.h5\n",
      "Epoch 17/30\n",
      "45/45 [==============================] - 68s 2s/step - loss: 11.4792 - categorical_accuracy: 0.2608 - val_loss: 12.8007 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00017: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00017-11.39714-0.26546-12.80071-0.20000.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
      "Epoch 18/30\n",
      "45/45 [==============================] - 47s 1s/step - loss: 11.6632 - categorical_accuracy: 0.2561 - val_loss: 12.8686 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00018: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00018-11.77453-0.24887-12.86857-0.20000.h5\n",
      "Epoch 19/30\n",
      "45/45 [==============================] - 60s 1s/step - loss: 11.4846 - categorical_accuracy: 0.2650 - val_loss: 12.4297 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00019: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00019-11.59279-0.25792-12.42973-0.22000.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
      "Epoch 20/30\n",
      "45/45 [==============================] - 46s 1s/step - loss: 11.7681 - categorical_accuracy: 0.2371 - val_loss: 12.3005 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00020: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00020-11.69110-0.24133-12.30046-0.23000.h5\n",
      "Epoch 21/30\n",
      "45/45 [==============================] - 48s 1s/step - loss: 11.4822 - categorical_accuracy: 0.2594 - val_loss: 12.3052 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00021: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00021-11.43245-0.26395-12.30523-0.23000.h5\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
      "Epoch 22/30\n",
      "45/45 [==============================] - 58s 1s/step - loss: 11.7343 - categorical_accuracy: 0.2416 - val_loss: 12.6077 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00022: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00022-11.72646-0.24585-12.60773-0.21000.h5\n",
      "Epoch 23/30\n",
      "45/45 [==============================] - 59s 1s/step - loss: 11.5650 - categorical_accuracy: 0.2547 - val_loss: 12.4448 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00023: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00023-11.67459-0.24736-12.44480-0.22000.h5\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.\n",
      "Epoch 24/30\n",
      "45/45 [==============================] - 50s 1s/step - loss: 11.3690 - categorical_accuracy: 0.2696 - val_loss: 12.2892 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00024: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00024-11.44642-0.26848-12.28922-0.23000.h5\n",
      "Epoch 25/30\n",
      "45/45 [==============================] - 68s 2s/step - loss: 11.5253 - categorical_accuracy: 0.2637 - val_loss: 12.2896 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00025: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00025-11.53912-0.26244-12.28956-0.23000.h5\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 4.096000622011431e-12.\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 69s 2s/step - loss: 11.5686 - categorical_accuracy: 0.2431 - val_loss: 12.4648 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00026: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00026-11.48806-0.24736-12.46480-0.22000.h5\n",
      "Epoch 27/30\n",
      "45/45 [==============================] - 48s 1s/step - loss: 11.7690 - categorical_accuracy: 0.2431 - val_loss: 12.3020 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00027: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00027-11.69204-0.24736-12.30201-0.23000.h5\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 8.192000897078167e-13.\n",
      "Epoch 28/30\n",
      "45/45 [==============================] - 66s 1s/step - loss: 11.4917 - categorical_accuracy: 0.2576 - val_loss: 12.4709 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00028: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00028-11.59996-0.25038-12.47095-0.22000.h5\n",
      "Epoch 29/30\n",
      "45/45 [==============================] - 45s 1s/step - loss: 11.5288 - categorical_accuracy: 0.2638 - val_loss: 12.6247 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00029: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00029-11.44757-0.26848-12.62467-0.21000.h5\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.6384001360475466e-13.\n",
      "Epoch 30/30\n",
      "45/45 [==============================] - 49s 1s/step - loss: 11.4197 - categorical_accuracy: 0.2621 - val_loss: 12.3118 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00030: saving model to pras_model_init_2020-02-2721_33_38.963072/model-00030-11.52675-0.25490-12.31175-0.23000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2c99bcf588>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit or training process\n",
    "model3.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has again, shown worse performance. Will now keep the image size as 100x100, select the original last 15 images instead of alternate images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model4\n",
    "model4 = Sequential()\n",
    "#help(Conv3D)\n",
    "# a keras convolutional layer is called Conv3D\n",
    "\n",
    "# note that the first layer needs to be told the input shape explicitly\n",
    "\n",
    "# first conv layer\n",
    "img_rows = 100\n",
    "img_cols = 100\n",
    "imnum = 15 # len(img_idx)\n",
    "num_classes = 5\n",
    "input_shape = (imnum, img_rows, img_cols, 3)\n",
    "model4.add(Conv3D(32, kernel_size=(3, 3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape)) # input shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# second conv layer\n",
    "model4.add(Conv3D(32, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model4.add(Dropout(0.25))\n",
    "\n",
    "# Third conv layer\n",
    "model4.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model4.add(Dropout(0.25))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model4.add(Flatten())\n",
    "model4.add(Dense(256, activation='relu')) # fully connected\n",
    "model4.add(Dropout(0.5))\n",
    "\n",
    "# softmax layer\n",
    "model4.add(Dense(num_classes, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_22 (Conv3D)           (None, 13, 98, 98, 32)    2624      \n",
      "_________________________________________________________________\n",
      "conv3d_23 (Conv3D)           (None, 11, 96, 96, 32)    27680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 11, 96, 96, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_15 (MaxPooling (None, 5, 48, 48, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 5, 48, 48, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_24 (Conv3D)           (None, 3, 46, 46, 64)     55360     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 3, 46, 46, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_16 (MaxPooling (None, 1, 23, 23, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 1, 23, 23, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 33856)             0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 256)               8667392   \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 8,754,725\n",
      "Trainable params: 8,754,533\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001 # Third model value\n",
    "optimiser = Adam(lr=learning_rate,clipvalue=1.0)\n",
    "model4.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model4.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator` for second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pras_model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/val ; batch size = 15\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 15\n",
      "45/45 [==============================] - 66s 1s/step - loss: 12.2390 - categorical_accuracy: 0.2075 - val_loss: 12.0436 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00001: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00001-12.17030-0.21116-12.04365-0.25000.h5\n",
      "Epoch 2/30\n",
      "45/45 [==============================] - 76s 2s/step - loss: 12.0728 - categorical_accuracy: 0.2489 - val_loss: 10.4768 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00002: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00002-12.09630-0.24736-10.47676-0.35000.h5\n",
      "Epoch 3/30\n",
      "45/45 [==============================] - 63s 1s/step - loss: 11.9969 - categorical_accuracy: 0.2503 - val_loss: 11.7859 - val_categorical_accuracy: 0.2600\n",
      "\n",
      "Epoch 00003: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00003-12.01909-0.24887-11.78591-0.26000.h5\n",
      "Epoch 4/30\n",
      "45/45 [==============================] - 80s 2s/step - loss: 12.0910 - categorical_accuracy: 0.2460 - val_loss: 10.7991 - val_categorical_accuracy: 0.3300\n",
      "\n",
      "Epoch 00004: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00004-12.01974-0.25038-10.79912-0.33000.h5\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 5/30\n",
      "45/45 [==============================] - 75s 2s/step - loss: 11.8444 - categorical_accuracy: 0.2623 - val_loss: 11.3154 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00005: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00005-11.76873-0.26697-11.31539-0.29000.h5\n",
      "Epoch 6/30\n",
      "45/45 [==============================] - 60s 1s/step - loss: 11.6395 - categorical_accuracy: 0.2724 - val_loss: 11.4438 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00006: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00006-11.75038-0.26546-11.44385-0.29000.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 7/30\n",
      "45/45 [==============================] - 66s 1s/step - loss: 11.2873 - categorical_accuracy: 0.2947 - val_loss: 11.6050 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00007: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00007-11.39196-0.28808-11.60503-0.28000.h5\n",
      "Epoch 8/30\n",
      "45/45 [==============================] - 69s 2s/step - loss: 11.3270 - categorical_accuracy: 0.2920 - val_loss: 11.1215 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00008: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00008-11.24217-0.29713-11.12149-0.31000.h5\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 9/30\n",
      "45/45 [==============================] - 59s 1s/step - loss: 11.4724 - categorical_accuracy: 0.2829 - val_loss: 10.5126 - val_categorical_accuracy: 0.3400\n",
      "\n",
      "Epoch 00009: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00009-11.48524-0.28205-10.51262-0.34000.h5\n",
      "Epoch 10/30\n",
      "45/45 [==============================] - 61s 1s/step - loss: 11.3493 - categorical_accuracy: 0.2918 - val_loss: 10.6544 - val_categorical_accuracy: 0.3300\n",
      "\n",
      "Epoch 00010: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00010-11.36000-0.29110-10.65444-0.33000.h5\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 11/30\n",
      "45/45 [==============================] - 79s 2s/step - loss: 11.4005 - categorical_accuracy: 0.2887 - val_loss: 11.0532 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00011: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00011-11.50721-0.28205-11.05323-0.30000.h5\n",
      "Epoch 12/30\n",
      "45/45 [==============================] - 72s 2s/step - loss: 11.5036 - categorical_accuracy: 0.2770 - val_loss: 11.3687 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00012: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00012-11.51698-0.27602-11.36873-0.29000.h5\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 13/30\n",
      "45/45 [==============================] - 61s 1s/step - loss: 11.2554 - categorical_accuracy: 0.2991 - val_loss: 11.2744 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00013: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00013-11.35954-0.29261-11.27443-0.30000.h5\n",
      "Epoch 14/30\n",
      "45/45 [==============================] - 73s 2s/step - loss: 11.4135 - categorical_accuracy: 0.2831 - val_loss: 11.2827 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00014: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00014-11.33027-0.28808-11.28267-0.30000.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "Epoch 15/30\n",
      "45/45 [==============================] - 79s 2s/step - loss: 11.3135 - categorical_accuracy: 0.2947 - val_loss: 11.3274 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00015: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00015-11.41869-0.28808-11.32737-0.29000.h5\n",
      "Epoch 16/30\n",
      "45/45 [==============================] - 76s 2s/step - loss: 11.4356 - categorical_accuracy: 0.2859 - val_loss: 11.2827 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00016: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00016-11.44777-0.28507-11.28267-0.30000.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
      "Epoch 17/30\n",
      "45/45 [==============================] - 80s 2s/step - loss: 11.2612 - categorical_accuracy: 0.2991 - val_loss: 11.2827 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00017: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00017-11.36546-0.29261-11.28267-0.30000.h5\n",
      "Epoch 18/30\n",
      "45/45 [==============================] - 61s 1s/step - loss: 11.6697 - categorical_accuracy: 0.2711 - val_loss: 11.4228 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00018: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00018-11.68608-0.26998-11.42276-0.29000.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
      "Epoch 19/30\n",
      "45/45 [==============================] - 80s 2s/step - loss: 11.3923 - categorical_accuracy: 0.2874 - val_loss: 11.2827 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00019: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00019-11.40374-0.28658-11.28267-0.30000.h5\n",
      "Epoch 20/30\n",
      "45/45 [==============================] - 61s 1s/step - loss: 11.3680 - categorical_accuracy: 0.2918 - val_loss: 11.1215 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00020: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00020-11.37901-0.29110-11.12149-0.31000.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
      "Epoch 21/30\n",
      "45/45 [==============================] - 81s 2s/step - loss: 11.7355 - categorical_accuracy: 0.2653 - val_loss: 11.2827 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00021: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00021-11.65789-0.26998-11.28267-0.30000.h5\n",
      "Epoch 22/30\n",
      "45/45 [==============================] - 79s 2s/step - loss: 11.5823 - categorical_accuracy: 0.2786 - val_loss: 11.1615 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00022: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00022-11.50199-0.28356-11.16151-0.30000.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
      "Epoch 23/30\n",
      "45/45 [==============================] - 79s 2s/step - loss: 11.2310 - categorical_accuracy: 0.2920 - val_loss: 11.4439 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00023: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00023-11.14455-0.29713-11.44389-0.29000.h5\n",
      "Epoch 24/30\n",
      "45/45 [==============================] - 59s 1s/step - loss: 11.3225 - categorical_accuracy: 0.2904 - val_loss: 11.2453 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00024: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00024-11.33268-0.28959-11.24534-0.30000.h5\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.\n",
      "Epoch 25/30\n",
      "45/45 [==============================] - 74s 2s/step - loss: 11.4171 - categorical_accuracy: 0.2873 - val_loss: 11.2827 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00025: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00025-11.52410-0.28054-11.28267-0.30000.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30\n",
      "45/45 [==============================] - 60s 1s/step - loss: 11.7507 - categorical_accuracy: 0.2683 - val_loss: 11.1215 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00026: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00026-11.67345-0.27300-11.12149-0.31000.h5\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 4.096000622011431e-12.\n",
      "Epoch 27/30\n",
      "45/45 [==============================] - 80s 2s/step - loss: 11.1828 - categorical_accuracy: 0.2991 - val_loss: 11.1617 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00027: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00027-11.28559-0.29261-11.16168-0.30000.h5\n",
      "Epoch 28/30\n",
      "45/45 [==============================] - 80s 2s/step - loss: 11.6759 - categorical_accuracy: 0.2727 - val_loss: 11.4438 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00028: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00028-11.59724-0.27753-11.44385-0.29000.h5\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 8.192000897078167e-13.\n",
      "Epoch 29/30\n",
      "45/45 [==============================] - 63s 1s/step - loss: 11.2064 - categorical_accuracy: 0.3021 - val_loss: 11.2827 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00029: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00029-11.30961-0.29563-11.28267-0.30000.h5\n",
      "Epoch 30/30\n",
      "45/45 [==============================] - 69s 2s/step - loss: 11.5940 - categorical_accuracy: 0.2727 - val_loss: 11.1503 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00030: saving model to pras_model_init_2020-02-2722_45_38.179634/model-00030-11.51388-0.27753-11.15032-0.30000.h5\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.6384001360475466e-13.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2c992dc278>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit or training process\n",
    "model4.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has again, shown worse performance. Will now keep the image size as 100x100, select the original last 15 images instead of alternate images. Also, was not croppig the image correctly, so corrected the mistake. \n",
    "The issue with cropping was that wrong scaling factor (0.05) instead of 0.95 was being used. Will keep higher learning rate as well for model 5 iteration  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model5 Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0228 19:36:58.237448 140257245230912 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0228 19:36:58.240003 140257245230912 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0228 19:36:58.399692 140257245230912 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0228 19:36:58.849140 140257245230912 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0228 19:36:58.874700 140257245230912 deprecation.py:506] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# model5\n",
    "model5 = Sequential()\n",
    "#help(Conv3D)\n",
    "# a keras convolutional layer is called Conv3D\n",
    "\n",
    "# note that the first layer needs to be told the input shape explicitly\n",
    "\n",
    "# first conv layer\n",
    "img_rows = 100\n",
    "img_cols = 100\n",
    "imnum = 15 # len(img_idx)\n",
    "num_classes = 5\n",
    "input_shape = (imnum, img_rows, img_cols, 3)\n",
    "model5.add(Conv3D(32, kernel_size=(3, 3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape)) # input shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# second conv layer\n",
    "model5.add(Conv3D(32, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model5.add(Dropout(0.25))\n",
    "\n",
    "# Third conv layer\n",
    "model5.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model5.add(Dropout(0.25))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model5.add(Flatten())\n",
    "model5.add(Dense(128, activation='relu')) # fully connected\n",
    "model5.add(Dropout(0.5))\n",
    "\n",
    "# softmax layer\n",
    "model5.add(Dense(num_classes, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0228 19:37:06.866137 140257245230912 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0228 19:37:06.873594 140257245230912 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 13, 98, 98, 32)    2624      \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 11, 96, 96, 32)    27680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 11, 96, 96, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 5, 48, 48, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 48, 48, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 3, 46, 46, 64)     55360     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 3, 46, 46, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 1, 23, 23, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 23, 23, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 33856)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               4333696   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 4,420,389\n",
      "Trainable params: 4,420,197\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.005 # Third model value\n",
    "optimiser = Adam(lr=learning_rate,clipvalue=1.0)\n",
    "model5.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model5.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator` for second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pras_model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0228 19:37:57.933302 140257245230912 deprecation.py:323] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0228 19:37:58.575947 140257245230912 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0228 19:37:58.750530 140257245230912 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "W0228 19:37:58.949256 140257245230912 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/val ; batch size = 15\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0228 19:38:04.189810 140257245230912 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0228 19:38:04.191336 140257245230912 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0228 19:38:10.775106 140257245230912 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W0228 19:38:10.778711 140257245230912 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "W0228 19:38:12.396611 140257245230912 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 189s 4s/step - loss: 12.2284 - categorical_accuracy: 0.2134 - val_loss: 13.3668 - val_categorical_accuracy: 0.1700\n",
      "\n",
      "Epoch 00001: saving model to pras_model_init_2020-02-2819_36_01.751728/model-00001-12.24341-0.21719-13.36685-0.17000.h5\n",
      "Epoch 2/30\n",
      "45/45 [==============================] - 87s 2s/step - loss: 11.6694 - categorical_accuracy: 0.2711 - val_loss: 12.7333 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00002: saving model to pras_model_init_2020-02-2819_36_01.751728/model-00002-11.68577-0.26998-12.73330-0.21000.h5\n",
      "Epoch 3/30\n",
      "45/45 [==============================] - 91s 2s/step - loss: 10.9017 - categorical_accuracy: 0.3213 - val_loss: 13.3807 - val_categorical_accuracy: 0.1700\n",
      "\n",
      "Epoch 00003: saving model to pras_model_init_2020-02-2819_36_01.751728/model-00003-10.99957-0.31523-13.38067-0.17000.h5\n",
      "Epoch 4/30\n",
      "45/45 [==============================] - 92s 2s/step - loss: 12.1925 - categorical_accuracy: 0.2400 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00004: saving model to pras_model_init_2020-02-2819_36_01.751728/model-00004-12.21807-0.23831-12.41095-0.23000.h5\n",
      "Epoch 5/30\n",
      "45/45 [==============================] - 89s 2s/step - loss: 12.2068 - categorical_accuracy: 0.2414 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00005: saving model to pras_model_init_2020-02-2819_36_01.751728/model-00005-12.23267-0.23982-12.41093-0.23000.h5\n",
      "Epoch 6/30\n",
      "45/45 [==============================] - 98s 2s/step - loss: 12.2629 - categorical_accuracy: 0.2371 - val_loss: 13.0557 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00006: saving model to pras_model_init_2020-02-2819_36_01.751728/model-00006-12.19465-0.24133-13.05566-0.19000.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 7/30\n",
      "45/45 [==============================] - 113s 3s/step - loss: 12.6545 - categorical_accuracy: 0.2149 - val_loss: 12.7111 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00007: saving model to pras_model_init_2020-02-2819_36_01.751728/model-00007-12.59318-0.21870-12.71112-0.21000.h5\n",
      "Epoch 8/30\n",
      "45/45 [==============================] - 68s 2s/step - loss: 12.6066 - categorical_accuracy: 0.2179 - val_loss: 12.2498 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00008: saving model to pras_model_init_2020-02-2819_36_01.751728/model-00008-12.54440-0.22172-12.24975-0.24000.h5\n",
      "Epoch 9/30\n",
      "45/45 [==============================] - 66s 1s/step - loss: 12.7724 - categorical_accuracy: 0.2075 - val_loss: 12.8571 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00009: saving model to pras_model_init_2020-02-2819_36_01.751728/model-00009-12.71314-0.21116-12.85706-0.20000.h5\n",
      "Epoch 10/30\n",
      "45/45 [==============================] - 79s 2s/step - loss: 12.7260 - categorical_accuracy: 0.2105 - val_loss: 13.0557 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00010: saving model to pras_model_init_2020-02-2819_36_01.751728/model-00010-12.66595-0.21418-13.05566-0.19000.h5\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001999999862164259.\n",
      "Epoch 11/30\n",
      "45/45 [==============================] - 64s 1s/step - loss: 12.7136 - categorical_accuracy: 0.2105 - val_loss: 12.9521 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00011: saving model to pras_model_init_2020-02-2819_36_01.751728/model-00011-12.65337-0.21418-12.95207-0.19000.h5\n",
      "Epoch 12/30\n",
      "45/45 [==============================] - 72s 2s/step - loss: 12.6066 - categorical_accuracy: 0.2179 - val_loss: 12.8945 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00012: saving model to pras_model_init_2020-02-2819_36_01.751728/model-00012-12.54440-0.22172-12.89448-0.20000.h5\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.9999996079131965e-05.\n",
      "Epoch 13/30\n",
      "45/45 [==============================] - 77s 2s/step - loss: 12.3699 - categorical_accuracy: 0.2326 - val_loss: 12.8952 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00013: saving model to pras_model_init_2020-02-2819_36_01.751728/model-00013-12.39863-0.23077-12.89524-0.20000.h5\n",
      "Epoch 14/30\n",
      "45/45 [==============================] - 75s 2s/step - loss: 12.6326 - categorical_accuracy: 0.2163 - val_loss: 12.8948 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00014: saving model to pras_model_init_2020-02-2819_36_01.751728/model-00014-12.66595-0.21418-12.89479-0.20000.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 7.99999907030724e-06.\n",
      "Epoch 15/30\n",
      "45/45 [==============================] - 63s 1s/step - loss: 12.6305 - categorical_accuracy: 0.2164 - val_loss: 12.8945 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00015: saving model to pras_model_init_2020-02-2819_36_01.751728/model-00015-12.56881-0.22021-12.89452-0.20000.h5\n",
      "Epoch 16/30\n",
      "45/45 [==============================] - 61s 1s/step - loss: 12.7272 - categorical_accuracy: 0.2090 - val_loss: 12.8947 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00016: saving model to pras_model_init_2020-02-2819_36_01.751728/model-00016-12.66716-0.21267-12.89467-0.20000.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-06.\n",
      "Epoch 17/30\n",
      "45/45 [==============================] - 62s 1s/step - loss: 12.3076 - categorical_accuracy: 0.2354 - val_loss: 12.8945 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00017: saving model to pras_model_init_2020-02-2819_36_01.751728/model-00017-12.43037-0.22775-12.89449-0.20000.h5\n",
      "Epoch 18/30\n",
      "45/45 [==============================] - 81s 2s/step - loss: 12.7708 - categorical_accuracy: 0.2060 - val_loss: 12.8945 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00018: saving model to pras_model_init_2020-02-2819_36_01.751728/model-00018-12.71157-0.20965-12.89448-0.20000.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.1999995826481613e-07.\n",
      "Epoch 19/30\n",
      "45/45 [==============================] - 83s 2s/step - loss: 12.5303 - categorical_accuracy: 0.2207 - val_loss: 12.8948 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00019: saving model to pras_model_init_2020-02-2819_36_01.751728/model-00019-12.56191-0.21870-12.89484-0.20000.h5\n",
      "Epoch 20/30\n",
      "45/45 [==============================] - 61s 1s/step - loss: 12.5803 - categorical_accuracy: 0.2193 - val_loss: 12.9457 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00020: saving model to pras_model_init_2020-02-2819_36_01.751728/model-00020-12.51766-0.22323-12.94573-0.19000.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.399999392669997e-08.\n",
      "Epoch 21/30\n",
      "45/45 [==============================] - 75s 2s/step - loss: 12.5593 - categorical_accuracy: 0.2208 - val_loss: 13.0464 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00021: saving model to pras_model_init_2020-02-2819_36_01.751728/model-00021-12.49632-0.22474-13.04639-0.19000.h5\n",
      "Epoch 22/30\n",
      "45/45 [==============================] - 82s 2s/step - loss: 12.6637 - categorical_accuracy: 0.2134 - val_loss: 13.0557 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00022: saving model to pras_model_init_2020-02-2819_36_01.751728/model-00022-12.60256-0.21719-13.05566-0.19000.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.2799998216905806e-08.\n",
      "Epoch 23/30\n",
      "45/45 [==============================] - 82s 2s/step - loss: 12.5605 - categorical_accuracy: 0.2207 - val_loss: 13.0557 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00023: saving model to pras_model_init_2020-02-2819_36_01.751728/model-00023-12.59265-0.21870-13.05566-0.19000.h5\n",
      "Epoch 24/30\n",
      "45/45 [==============================] - 62s 1s/step - loss: 12.8217 - categorical_accuracy: 0.2045 - val_loss: 13.0557 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00024: saving model to pras_model_init_2020-02-2819_36_01.751728/model-00024-12.76331-0.20814-13.05566-0.19000.h5\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 2.5599996789082983e-09.\n",
      "Epoch 25/30\n",
      "45/45 [==============================] - 79s 2s/step - loss: 12.4414 - categorical_accuracy: 0.2281 - val_loss: 13.0557 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00025: saving model to pras_model_init_2020-02-2819_36_01.751728/model-00025-12.47147-0.22624-13.05566-0.19000.h5\n",
      "Epoch 26/30\n",
      "45/45 [==============================] - 62s 1s/step - loss: 12.6782 - categorical_accuracy: 0.2134 - val_loss: 13.0557 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00026: saving model to pras_model_init_2020-02-2819_36_01.751728/model-00026-12.61734-0.21719-13.05566-0.19000.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 5.11999953545228e-10.\n",
      "Epoch 27/30\n",
      "45/45 [==============================] - 73s 2s/step - loss: 12.6635 - categorical_accuracy: 0.2119 - val_loss: 13.0557 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00027: saving model to pras_model_init_2020-02-2819_36_01.751728/model-00027-12.60238-0.21569-13.05566-0.19000.h5\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unable to flush file's cached information (file write failed: time = Fri Feb 28 20:14:33 2020\n, filename = 'pras_model_init_2020-02-2819_36_01.751728/model-00027-12.60238-0.21569-13.05566-0.19000.h5', file descriptor = 50, errno = 28, error message = 'No space left on device', buf = 0x5577a4386dc0, total write size = 6144, bytes this sub-write = 6144, bytes actually written = 18446744073709551615, offset = 4096)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0m_serialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36m_serialize_model\u001b[0;34m(model, f, include_optimizer)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mlayer_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minclude_optimizer\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/utils/io_utils.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, attr, val)\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, args, val)\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfspace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdxpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dxpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5d.pyx\u001b[0m in \u001b[0;36mh5py.h5d.DatasetID.write\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_proxy.pyx\u001b[0m in \u001b[0;36mh5py._proxy.dset_rw\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_proxy.pyx\u001b[0m in \u001b[0;36mh5py._proxy.H5PY_H5Dwrite\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't write data (file write failed: time = Fri Feb 28 20:14:33 2020\n, filename = 'pras_model_init_2020-02-2819_36_01.751728/model-00027-12.60238-0.21569-13.05566-0.19000.h5', file descriptor = 50, errno = 28, error message = 'No space left on device', buf = 0x203992c28, total write size = 9130968, bytes this sub-write = 9130968, bytes actually written = 18446744073709551615, offset = 8581120)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-66e4809c3325>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m model5.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n\u001b[1;32m      3\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n\u001b[0m",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    249\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    455\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/utils/io_utils.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \"\"\"\n\u001b[1;32m    432\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m             \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwith_phil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.flush\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to flush file's cached information (file write failed: time = Fri Feb 28 20:14:33 2020\n, filename = 'pras_model_init_2020-02-2819_36_01.751728/model-00027-12.60238-0.21569-13.05566-0.19000.h5', file descriptor = 50, errno = 28, error message = 'No space left on device', buf = 0x5577a4386dc0, total write size = 6144, bytes this sub-write = 6144, bytes actually written = 18446744073709551615, offset = 4096)"
     ]
    }
   ],
   "source": [
    "# Fit or training process\n",
    "model5.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model again is performing worse. The changes to selection of images, cropping of images seem to be having adverse effect on performance. Will therefore, first revert to original set of images and then remove cropping if model continues performing poorly  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model6 Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model6\n",
    "model6 = Sequential()\n",
    "#help(Conv3D)\n",
    "# a keras convolutional layer is called Conv3D\n",
    "\n",
    "# note that the first layer needs to be told the input shape explicitly\n",
    "\n",
    "# first conv layer\n",
    "img_rows = 100\n",
    "img_cols = 100\n",
    "imnum = 15 # len(img_idx)\n",
    "num_classes = 5\n",
    "input_shape = (imnum, img_rows, img_cols, 3)\n",
    "model6.add(Conv3D(32, kernel_size=(3, 3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape)) # input shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# second conv layer\n",
    "model6.add(Conv3D(32, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model6.add(Dropout(0.25))\n",
    "\n",
    "# Third conv layer\n",
    "model6.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model6.add(Dropout(0.25))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model6.add(Flatten())\n",
    "model6.add(Dense(128, activation='relu')) # fully connected\n",
    "model6.add(Dropout(0.5))\n",
    "\n",
    "# softmax layer\n",
    "model6.add(Dense(num_classes, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_4 (Conv3D)            (None, 13, 98, 98, 32)    2624      \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 11, 96, 96, 32)    27680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 11, 96, 96, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 5, 48, 48, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 5, 48, 48, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 3, 46, 46, 64)     55360     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 3, 46, 46, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 1, 23, 23, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1, 23, 23, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 33856)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               4333696   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 4,420,389\n",
      "Trainable params: 4,420,197\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.003 # Third model value\n",
    "optimiser = Adam(lr=learning_rate,clipvalue=1.0)\n",
    "model6.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model6.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator` for second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pras_model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/val ; batch size = 15\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 15\n",
      "45/45 [==============================] - 153s 3s/step - loss: 12.5839 - categorical_accuracy: 0.1882 - val_loss: 12.5721 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00001: saving model to pras_model_init_2020-02-2820_28_42.834158/model-00001-12.52133-0.19155-12.57212-0.22000.h5\n",
      "Epoch 2/30\n",
      "45/45 [==============================] - 79s 2s/step - loss: 12.2306 - categorical_accuracy: 0.2371 - val_loss: 12.1885 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00002: saving model to pras_model_init_2020-02-2820_28_42.834158/model-00002-12.16181-0.24133-12.18846-0.24000.h5\n",
      "Epoch 3/30\n",
      "45/45 [==============================] - 60s 1s/step - loss: 11.5927 - categorical_accuracy: 0.2741 - val_loss: 11.7662 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00003: saving model to pras_model_init_2020-02-2820_28_42.834158/model-00003-11.60771-0.27300-11.76621-0.27000.h5\n",
      "Epoch 4/30\n",
      "45/45 [==============================] - 62s 1s/step - loss: 10.7119 - categorical_accuracy: 0.3244 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00004: saving model to pras_model_init_2020-02-2820_28_42.834158/model-00004-10.71132-0.32428-12.41093-0.23000.h5\n",
      "Epoch 5/30\n",
      "45/45 [==============================] - 82s 2s/step - loss: 10.7605 - categorical_accuracy: 0.3275 - val_loss: 11.0919 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00005: saving model to pras_model_init_2020-02-2820_28_42.834158/model-00005-10.66570-0.33333-11.09189-0.31000.h5\n",
      "Epoch 6/30\n",
      "45/45 [==============================] - 69s 2s/step - loss: 10.5949 - categorical_accuracy: 0.3391 - val_loss: 11.4438 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00006: saving model to pras_model_init_2020-02-2820_28_42.834158/model-00006-10.68733-0.33333-11.44385-0.29000.h5\n",
      "Epoch 7/30\n",
      "45/45 [==============================] - 83s 2s/step - loss: 10.9966 - categorical_accuracy: 0.3155 - val_loss: 11.4438 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00007: saving model to pras_model_init_2020-02-2820_28_42.834158/model-00007-11.00109-0.31523-11.44385-0.29000.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0006000000052154065.\n",
      "Epoch 8/30\n",
      "45/45 [==============================] - 65s 1s/step - loss: 11.4446 - categorical_accuracy: 0.2890 - val_loss: 11.2494 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00008: saving model to pras_model_init_2020-02-2820_28_42.834158/model-00008-11.36191-0.29412-11.24937-0.30000.h5\n",
      "Epoch 9/30\n",
      "45/45 [==============================] - 62s 1s/step - loss: 11.1607 - categorical_accuracy: 0.3052 - val_loss: 11.2827 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00009: saving model to pras_model_init_2020-02-2820_28_42.834158/model-00009-11.16801-0.30468-11.28267-0.30000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00012000000569969416.\n",
      "Epoch 10/30\n",
      "45/45 [==============================] - 82s 2s/step - loss: 11.3445 - categorical_accuracy: 0.2917 - val_loss: 11.1215 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00010: saving model to pras_model_init_2020-02-2820_28_42.834158/model-00010-11.45021-0.28507-11.12149-0.31000.h5\n",
      "Epoch 11/30\n",
      "45/45 [==============================] - 83s 2s/step - loss: 11.4187 - categorical_accuracy: 0.2875 - val_loss: 10.9489 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00011: saving model to pras_model_init_2020-02-2820_28_42.834158/model-00011-11.33552-0.29261-10.94895-0.32000.h5\n",
      "Epoch 12/30\n",
      "45/45 [==============================] - 60s 1s/step - loss: 11.2518 - categorical_accuracy: 0.2992 - val_loss: 10.8117 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00012: saving model to pras_model_init_2020-02-2820_28_42.834158/model-00012-11.26077-0.29864-10.81171-0.32000.h5\n",
      "Epoch 13/30\n",
      "45/45 [==============================] - 62s 1s/step - loss: 11.2265 - categorical_accuracy: 0.3023 - val_loss: 10.9612 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00013: saving model to pras_model_init_2020-02-2820_28_42.834158/model-00013-11.13994-0.30769-10.96116-0.32000.h5\n",
      "Epoch 14/30\n",
      "45/45 [==============================] - 74s 2s/step - loss: 11.2665 - categorical_accuracy: 0.2992 - val_loss: 10.9786 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00014: saving model to pras_model_init_2020-02-2820_28_42.834158/model-00014-11.27577-0.29864-10.97863-0.31000.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 2.4000000848900527e-05.\n",
      "Epoch 15/30\n",
      "45/45 [==============================] - 71s 2s/step - loss: 11.2235 - categorical_accuracy: 0.3022 - val_loss: 11.1215 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00015: saving model to pras_model_init_2020-02-2820_28_42.834158/model-00015-11.23196-0.30166-11.12149-0.31000.h5\n",
      "Epoch 16/30\n",
      "45/45 [==============================] - 62s 1s/step - loss: 11.2383 - categorical_accuracy: 0.2992 - val_loss: 11.1215 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00016: saving model to pras_model_init_2020-02-2820_28_42.834158/model-00016-11.24703-0.29864-11.12149-0.31000.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 4.800000169780105e-06.\n",
      "Epoch 17/30\n",
      "45/45 [==============================] - 69s 2s/step - loss: 11.2212 - categorical_accuracy: 0.3022 - val_loss: 11.1215 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00017: saving model to pras_model_init_2020-02-2820_28_42.834158/model-00017-11.22965-0.30166-11.12149-0.31000.h5\n",
      "Epoch 18/30\n",
      "45/45 [==============================] - 68s 2s/step - loss: 10.9532 - categorical_accuracy: 0.3184 - val_loss: 11.0971 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00018: saving model to pras_model_init_2020-02-2820_28_42.834158/model-00018-11.05200-0.31222-11.09706-0.31000.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 9.60000033956021e-07.\n",
      "Epoch 19/30\n",
      "45/45 [==============================] - 83s 2s/step - loss: 11.2930 - categorical_accuracy: 0.2976 - val_loss: 11.0461 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00019: saving model to pras_model_init_2020-02-2820_28_42.834158/model-00019-11.39776-0.29110-11.04609-0.31000.h5\n",
      "Epoch 20/30\n",
      "45/45 [==============================] - 79s 2s/step - loss: 11.0653 - categorical_accuracy: 0.3095 - val_loss: 10.9996 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00020: saving model to pras_model_init_2020-02-2820_28_42.834158/model-00020-11.16605-0.30317-10.99955-0.31000.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.9200001588615123e-07.\n",
      "Epoch 21/30\n",
      "45/45 [==============================] - 72s 2s/step - loss: 11.1321 - categorical_accuracy: 0.3065 - val_loss: 10.9954 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00021: saving model to pras_model_init_2020-02-2820_28_42.834158/model-00021-11.23402-0.30015-10.99537-0.31000.h5\n",
      "Epoch 22/30\n",
      "45/45 [==============================] - 74s 2s/step - loss: 10.7973 - categorical_accuracy: 0.3289 - val_loss: 10.9754 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00022: saving model to pras_model_init_2020-02-2820_28_42.834158/model-00022-10.79824-0.32881-10.97543-0.30000.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.8400003177230246e-08.\n",
      "Epoch 23/30\n",
      "45/45 [==============================] - 85s 2s/step - loss: 11.2803 - categorical_accuracy: 0.2979 - val_loss: 10.9834 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00023: saving model to pras_model_init_2020-02-2820_28_42.834158/model-00023-11.19472-0.30317-10.98341-0.30000.h5\n",
      "Epoch 24/30\n",
      "45/45 [==============================] - 86s 2s/step - loss: 10.9852 - categorical_accuracy: 0.3153 - val_loss: 10.9761 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00024: saving model to pras_model_init_2020-02-2820_28_42.834158/model-00024-11.17960-0.30317-10.97613-0.30000.h5\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 7.68000063544605e-09.\n",
      "Epoch 25/30\n",
      "11/45 [======>.......................] - ETA: 1:08 - loss: 11.5285 - categorical_accuracy: 0.2788"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-c5af47da261c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m model6.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n\u001b[1;32m      3\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n\u001b[0m",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit or training process\n",
    "model6.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopped the model iteration as its not performing well. Selecting the original set of images has not helped much. Will remove the cropping now as well. This will make parameters the same as first model. Therefore, now will add one more hidden layer to improve validation score that we got in first model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model7 Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model7\n",
    "model7 = Sequential()\n",
    "#help(Conv3D)\n",
    "# a keras convolutional layer is called Conv3D\n",
    "\n",
    "# note that the first layer needs to be told the input shape explicitly\n",
    "\n",
    "# first conv layer\n",
    "img_rows = 100\n",
    "img_cols = 100\n",
    "imnum = 15 # len(img_idx)\n",
    "num_classes = 5\n",
    "input_shape = (imnum, img_rows, img_cols, 3)\n",
    "model7.add(Conv3D(32, kernel_size=(3, 3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape)) # input shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# second conv layer\n",
    "model7.add(Conv3D(32, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model7.add(BatchNormalization())\n",
    "#model7.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model7.add(Dropout(0.25))\n",
    "\n",
    "# Third conv layer\n",
    "model7.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model7.add(BatchNormalization())\n",
    "model7.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model7.add(Dropout(0.25))\n",
    "\n",
    "# Fourth conv layer\n",
    "model7.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model7.add(BatchNormalization())\n",
    "model7.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model7.add(Dropout(0.25))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model7.add(Flatten())\n",
    "model7.add(Dense(128, activation='relu')) # fully connected\n",
    "model7.add(Dropout(0.5))\n",
    "\n",
    "# softmax layer\n",
    "model7.add(Dense(num_classes, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0228 21:09:10.384916 140700025685824 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_5 (Conv3D)            (None, 13, 98, 98, 32)    2624      \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 11, 96, 96, 32)    27680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 11, 96, 96, 32)    128       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 11, 96, 96, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_7 (Conv3D)            (None, 9, 94, 94, 64)     55360     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 9, 94, 94, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 4, 47, 47, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 47, 47, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_8 (Conv3D)            (None, 2, 45, 45, 64)     110656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 2, 45, 45, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 1, 22, 22, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1, 22, 22, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 30976)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               3965056   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 4,162,661\n",
      "Trainable params: 4,162,341\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.003 # Third model value\n",
    "optimiser = Adam(lr=learning_rate,clipvalue=1.0)\n",
    "model7.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model7.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator` for second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pras_model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0228 21:10:25.283373 140700025685824 deprecation.py:323] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0228 21:10:25.893525 140700025685824 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0228 21:10:26.120834 140700025685824 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "W0228 21:10:26.357380 140700025685824 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/val ; batch size = 15\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0228 21:10:28.550758 140700025685824 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0228 21:10:28.552479 140700025685824 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0228 21:10:30.933385 140700025685824 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W0228 21:10:30.934820 140700025685824 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "W0228 21:10:32.073109 140700025685824 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 94s 2s/step - loss: 11.0266 - categorical_accuracy: 0.2770 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00001: saving model to pras_model_init_2020-02-2821_02_44.214821/model-00001-11.03161-0.27602-12.41093-0.23000.h5\n",
      "Epoch 2/30\n",
      "45/45 [==============================] - 75s 2s/step - loss: 10.8545 - categorical_accuracy: 0.3169 - val_loss: 13.0557 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00002: saving model to pras_model_init_2020-02-2821_02_44.214821/model-00002-10.95149-0.31071-13.05566-0.19000.h5\n",
      "Epoch 3/30\n",
      "45/45 [==============================] - 80s 2s/step - loss: 10.8740 - categorical_accuracy: 0.3155 - val_loss: 10.9604 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00003: saving model to pras_model_init_2020-02-2821_02_44.214821/model-00003-10.87627-0.31523-10.96036-0.32000.h5\n",
      "Epoch 4/30\n",
      "45/45 [==============================] - 76s 2s/step - loss: 10.6355 - categorical_accuracy: 0.3319 - val_loss: 9.5153 - val_categorical_accuracy: 0.4100\n",
      "\n",
      "Epoch 00004: saving model to pras_model_init_2020-02-2821_02_44.214821/model-00004-10.63335-0.33183-9.51535-0.41000.h5\n",
      "Epoch 5/30\n",
      "45/45 [==============================] - 78s 2s/step - loss: 10.7593 - categorical_accuracy: 0.3246 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00005: saving model to pras_model_init_2020-02-2821_02_44.214821/model-00005-10.66443-0.33032-12.41093-0.23000.h5\n",
      "Epoch 6/30\n",
      "45/45 [==============================] - 77s 2s/step - loss: 11.1398 - categorical_accuracy: 0.3068 - val_loss: 10.1544 - val_categorical_accuracy: 0.3700\n",
      "\n",
      "Epoch 00006: saving model to pras_model_init_2020-02-2821_02_44.214821/model-00006-11.05170-0.31222-10.15440-0.37000.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0006000000052154065.\n",
      "Epoch 7/30\n",
      "45/45 [==============================] - 70s 2s/step - loss: 11.1277 - categorical_accuracy: 0.3053 - val_loss: 10.1544 - val_categorical_accuracy: 0.3700\n",
      "\n",
      "Epoch 00007: saving model to pras_model_init_2020-02-2821_02_44.214821/model-00007-11.03933-0.31071-10.15441-0.37000.h5\n",
      "Epoch 8/30\n",
      "45/45 [==============================] - 81s 2s/step - loss: 10.9329 - categorical_accuracy: 0.3184 - val_loss: 9.9932 - val_categorical_accuracy: 0.3800\n",
      "\n",
      "Epoch 00008: saving model to pras_model_init_2020-02-2821_02_44.214821/model-00008-11.03126-0.31222-9.99322-0.38000.h5\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00012000000569969416.\n",
      "Epoch 9/30\n",
      "45/45 [==============================] - 72s 2s/step - loss: 10.9531 - categorical_accuracy: 0.3170 - val_loss: 10.9603 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00009: saving model to pras_model_init_2020-02-2821_02_44.214821/model-00009-10.95676-0.31674-10.96030-0.32000.h5\n",
      "Epoch 10/30\n",
      "45/45 [==============================] - 78s 2s/step - loss: 10.8495 - categorical_accuracy: 0.3230 - val_loss: 11.2827 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00010: saving model to pras_model_init_2020-02-2821_02_44.214821/model-00010-10.85134-0.32278-11.28267-0.30000.h5\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.4000000848900527e-05.\n",
      "Epoch 11/30\n",
      "45/45 [==============================] - 81s 2s/step - loss: 10.7626 - categorical_accuracy: 0.3304 - val_loss: 11.3728 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00011: saving model to pras_model_init_2020-02-2821_02_44.214821/model-00011-10.76295-0.33032-11.37278-0.29000.h5\n",
      "Epoch 12/30\n",
      "45/45 [==============================] - 78s 2s/step - loss: 10.8550 - categorical_accuracy: 0.3244 - val_loss: 10.9803 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00012: saving model to pras_model_init_2020-02-2821_02_44.214821/model-00012-10.85698-0.32428-10.98027-0.31000.h5\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 4.800000169780105e-06.\n",
      "Epoch 13/30\n",
      "45/45 [==============================] - 72s 2s/step - loss: 10.9006 - categorical_accuracy: 0.3215 - val_loss: 10.9603 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00013: saving model to pras_model_init_2020-02-2821_02_44.214821/model-00013-10.90335-0.32127-10.96032-0.32000.h5\n",
      "Epoch 14/30\n",
      "45/45 [==============================] - 76s 2s/step - loss: 11.1142 - categorical_accuracy: 0.3096 - val_loss: 10.9603 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00014: saving model to pras_model_init_2020-02-2821_02_44.214821/model-00014-11.12074-0.30920-10.96030-0.32000.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 9.60000033956021e-07.\n",
      "Epoch 15/30\n",
      "22/45 [=============>................] - ETA: 41s - loss: 11.3315 - categorical_accuracy: 0.2970"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f786e4d6e958>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m model7.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n\u001b[1;32m      3\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n\u001b[0m",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit or training process\n",
    "model7.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values again not improving. Will add few more images now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model8 Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model8\n",
    "model8 = Sequential()\n",
    "#help(Conv3D)\n",
    "# a keras convolutional layer is called Conv3D\n",
    "\n",
    "# note that the first layer needs to be told the input shape explicitly\n",
    "\n",
    "# first conv layer\n",
    "img_rows = 100\n",
    "img_cols = 100\n",
    "imnum = 18 # len(img_idx)\n",
    "num_classes = 5\n",
    "input_shape = (imnum, img_rows, img_cols, 3)\n",
    "model8.add(Conv3D(32, kernel_size=(3, 3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape)) # input shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# second conv layer\n",
    "model8.add(Conv3D(32, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model8.add(BatchNormalization())\n",
    "#model8.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model8.add(Dropout(0.25))\n",
    "\n",
    "# Third conv layer\n",
    "model8.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model8.add(BatchNormalization())\n",
    "model8.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model8.add(Dropout(0.25))\n",
    "\n",
    "# Fourth conv layer\n",
    "model8.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model8.add(BatchNormalization())\n",
    "model8.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model8.add(Dropout(0.25))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model8.add(Flatten())\n",
    "model8.add(Dense(128, activation='relu')) # fully connected\n",
    "model8.add(Dropout(0.5))\n",
    "\n",
    "# softmax layer\n",
    "model8.add(Dense(num_classes, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_9 (Conv3D)            (None, 16, 98, 98, 32)    2624      \n",
      "_________________________________________________________________\n",
      "conv3d_10 (Conv3D)           (None, 14, 96, 96, 32)    27680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 14, 96, 96, 32)    128       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 14, 96, 96, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_11 (Conv3D)           (None, 12, 94, 94, 64)    55360     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 12, 94, 94, 64)    256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 6, 47, 47, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 6, 47, 47, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_12 (Conv3D)           (None, 4, 45, 45, 64)     110656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 45, 45, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3 (None, 2, 22, 22, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 2, 22, 22, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 61952)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               7929984   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 8,127,589\n",
      "Trainable params: 8,127,269\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001 # Third model value\n",
    "optimiser = Adam(lr=learning_rate,clipvalue=1.0)\n",
    "model8.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model8.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator` for second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pras_model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0228 21:42:09.868390 140117426284352 deprecation.py:323] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0228 21:42:10.490084 140117426284352 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0228 21:42:10.706593 140117426284352 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "W0228 21:42:10.972047 140117426284352 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Source path = Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 15\n",
      " /mnt/disks/user/project/PROJECT/Project_data/val ; batch size = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0228 21:42:13.651912 140117426284352 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0228 21:42:13.654470 140117426284352 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0228 21:42:15.924528 140117426284352 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W0228 21:42:15.925768 140117426284352 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "W0228 21:42:17.207567 140117426284352 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 132s 3s/step - loss: 11.4206 - categorical_accuracy: 0.2696 - val_loss: 13.0557 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00001: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00001-11.43251-0.26848-13.05566-0.19000.h5\n",
      "Epoch 2/30\n",
      "45/45 [==============================] - 95s 2s/step - loss: 11.0304 - categorical_accuracy: 0.2992 - val_loss: 13.0557 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00002: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00002-11.03540-0.29864-13.05566-0.19000.h5\n",
      "Epoch 3/30\n",
      "45/45 [==============================] - 90s 2s/step - loss: 10.0612 - categorical_accuracy: 0.3630 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00003: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00003-10.04905-0.36350-12.41093-0.23000.h5\n",
      "Epoch 4/30\n",
      "45/45 [==============================] - 93s 2s/step - loss: 10.6786 - categorical_accuracy: 0.3261 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00004: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00004-10.58234-0.33183-12.41093-0.23000.h5\n",
      "Epoch 5/30\n",
      "45/45 [==============================] - 94s 2s/step - loss: 9.8384 - categorical_accuracy: 0.3733 - val_loss: 13.5392 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00005: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00005-9.82238-0.37406-13.53920-0.16000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 6/30\n",
      "45/45 [==============================] - 90s 2s/step - loss: 9.9451 - categorical_accuracy: 0.3732 - val_loss: 13.5392 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00006: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00006-10.02603-0.36802-13.53920-0.16000.h5\n",
      "Epoch 7/30\n",
      "45/45 [==============================] - 90s 2s/step - loss: 9.8524 - categorical_accuracy: 0.3867 - val_loss: 13.5392 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00007: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00007-9.83658-0.38763-13.53920-0.16000.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 8/30\n",
      "45/45 [==============================] - 99s 2s/step - loss: 9.7805 - categorical_accuracy: 0.3779 - val_loss: 13.2168 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00008: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00008-9.66837-0.38462-13.21684-0.18000.h5\n",
      "Epoch 9/30\n",
      "45/45 [==============================] - 92s 2s/step - loss: 9.7525 - categorical_accuracy: 0.3821 - val_loss: 12.4114 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00009: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00009-9.83000-0.37707-12.41142-0.23000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 10/30\n",
      "45/45 [==============================] - 100s 2s/step - loss: 9.1313 - categorical_accuracy: 0.4236 - val_loss: 11.6891 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00010: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00010-9.19787-0.41931-11.68905-0.27000.h5\n",
      "Epoch 11/30\n",
      "45/45 [==============================] - 102s 2s/step - loss: 9.4002 - categorical_accuracy: 0.4089 - val_loss: 11.2868 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00011: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00011-9.37638-0.41026-11.28680-0.30000.h5\n",
      "Epoch 12/30\n",
      "45/45 [==============================] - 100s 2s/step - loss: 9.6101 - categorical_accuracy: 0.3867 - val_loss: 10.9176 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00012: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00012-9.58997-0.38763-10.91759-0.31000.h5\n",
      "Epoch 13/30\n",
      "45/45 [==============================] - 94s 2s/step - loss: 9.3525 - categorical_accuracy: 0.4104 - val_loss: 10.5332 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00013: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00013-9.32789-0.41176-10.53317-0.32000.h5\n",
      "Epoch 14/30\n",
      "45/45 [==============================] - 93s 2s/step - loss: 9.6385 - categorical_accuracy: 0.3956 - val_loss: 9.9357 - val_categorical_accuracy: 0.3700\n",
      "\n",
      "Epoch 00014: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00014-9.61843-0.39668-9.93566-0.37000.h5\n",
      "Epoch 15/30\n",
      "45/45 [==============================] - 97s 2s/step - loss: 9.2823 - categorical_accuracy: 0.4193 - val_loss: 9.7437 - val_categorical_accuracy: 0.3800\n",
      "\n",
      "Epoch 00015: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00015-9.25642-0.42081-9.74368-0.38000.h5\n",
      "Epoch 16/30\n",
      "45/45 [==============================] - 89s 2s/step - loss: 9.4293 - categorical_accuracy: 0.4089 - val_loss: 9.5744 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00016: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00016-9.40598-0.41026-9.57443-0.39000.h5\n",
      "Epoch 17/30\n",
      "45/45 [==============================] - 88s 2s/step - loss: 9.6565 - categorical_accuracy: 0.3927 - val_loss: 9.5631 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00017: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00017-9.54212-0.39970-9.56314-0.40000.h5\n",
      "Epoch 18/30\n",
      "45/45 [==============================] - 99s 2s/step - loss: 9.3300 - categorical_accuracy: 0.4147 - val_loss: 9.5881 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00018: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00018-9.40000-0.41026-9.58811-0.40000.h5\n",
      "Epoch 19/30\n",
      "45/45 [==============================] - 98s 2s/step - loss: 9.1952 - categorical_accuracy: 0.4221 - val_loss: 9.5936 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00019: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00019-9.26283-0.41780-9.59357-0.40000.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 20/30\n",
      "45/45 [==============================] - 93s 2s/step - loss: 9.5026 - categorical_accuracy: 0.4014 - val_loss: 9.6609 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00020: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00020-9.59873-0.39668-9.66093-0.40000.h5\n",
      "Epoch 21/30\n",
      "45/45 [==============================] - 90s 2s/step - loss: 9.7011 - categorical_accuracy: 0.3913 - val_loss: 9.6709 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00021: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00021-9.58749-0.39819-9.67086-0.40000.h5\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 22/30\n",
      "45/45 [==============================] - 91s 2s/step - loss: 9.3543 - categorical_accuracy: 0.4132 - val_loss: 9.6709 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00022: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00022-9.42472-0.40875-9.67086-0.40000.h5\n",
      "Epoch 23/30\n",
      "45/45 [==============================] - 90s 2s/step - loss: 9.3807 - categorical_accuracy: 0.4089 - val_loss: 9.6709 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00023: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00023-9.35654-0.41026-9.67086-0.40000.h5\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "Epoch 24/30\n",
      "45/45 [==============================] - 97s 2s/step - loss: 9.2504 - categorical_accuracy: 0.4206 - val_loss: 9.6709 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00024: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00024-9.31905-0.41629-9.67086-0.40000.h5\n",
      "Epoch 25/30\n",
      "45/45 [==============================] - 96s 2s/step - loss: 9.5848 - categorical_accuracy: 0.4000 - val_loss: 9.6709 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00025: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00025-9.56423-0.40121-9.67086-0.40000.h5\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
      "Epoch 26/30\n",
      "45/45 [==============================] - 94s 2s/step - loss: 9.5521 - categorical_accuracy: 0.4000 - val_loss: 9.6709 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00026: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00026-9.53095-0.40121-9.67086-0.40000.h5\n",
      "Epoch 27/30\n",
      "45/45 [==============================] - 90s 2s/step - loss: 9.4288 - categorical_accuracy: 0.4043 - val_loss: 9.6709 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00027: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00027-9.50057-0.39970-9.67086-0.40000.h5\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 95s 2s/step - loss: 9.4522 - categorical_accuracy: 0.4043 - val_loss: 9.6709 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00028: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00028-9.52444-0.39970-9.67086-0.40000.h5\n",
      "Epoch 29/30\n",
      "45/45 [==============================] - 91s 2s/step - loss: 9.0908 - categorical_accuracy: 0.4266 - val_loss: 9.6709 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00029: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00029-9.15658-0.42232-9.67086-0.40000.h5\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
      "Epoch 30/30\n",
      "45/45 [==============================] - 92s 2s/step - loss: 9.6686 - categorical_accuracy: 0.3883 - val_loss: 9.6709 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00030: saving model to pras_model_init_2020-02-2821_40_59.412193/model-00030-9.55447-0.39517-9.67086-0.40000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6efd576630>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit or training process\n",
    "model8.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model did not perform well when compared to first model with the same additional layer and almost the same parameters as were considered in the first model. Will now try by adding more images and also cropping the image again. Will also keep the same architecture as the validation and train scores were quite similar. Will reduce the epoch to 20 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_path\n",
    "#folder_list\n",
    "#source_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29] #Selecting second half of images as the differentiating actions within video would most likely be represented by these images  \n",
    "    #img_idx = [x for x in range(12,30)] # Selecting three more images apart from second 15\n",
    "    #img_idx = [x for x in range(0,30,2)] #selected for second and third model and fifth model\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),100,100,3))# x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            #batch_data = np.zeros((batch_size,len(img_idx),64,64,3))\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    #image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = imgio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    # Cropping the image for second model analysis by 5%\n",
    "                    center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                    width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                    left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                    top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                    image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:] \n",
    "                    image = resize(image, (100,100,3)) # first model instance and fourth model\n",
    "                    # resizing image to 64x64 for second  and third iteration\n",
    "                    #image = resize(image, (64,64,3))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                #print(batch_labels.shape(0))\n",
    "                #print(image.shape(0))\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        diff = len(folder_list) - (num_batches*batch_size)\n",
    "        last_batch_size = diff\n",
    "        batch_data = np.zeros((last_batch_size,len(img_idx),100,100,3)) # first model instance and fourth model\n",
    "        #batch_data = np.zeros((last_batch_size,len(img_idx),64,64,3)) #second and third model instance\n",
    "        batch_labels = np.zeros((last_batch_size,5))\n",
    "        for fd in range(diff):\n",
    "            imgs = os.listdir(source_path+'/'+ t[fd + (num_batches*batch_size)].split(';')[0])\n",
    "            for idx,item in enumerate(img_idx):\n",
    "                image = imgio.imread(source_path+'/'+ t[fd + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                # Cropping the image for second model analysis by 5%\n",
    "                center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:]\n",
    "                image = resize(image, (100,100,3)) #first model instance and fourth model\n",
    "                #image = resize(image, (64,64,3)) # second and third model instance\n",
    "                batch_data[fd,idx,:,:,0] = image[:,:,0]/255 \n",
    "                batch_data[fd,idx,:,:,1] = image[:,:,1]/255 \n",
    "                batch_data[fd,idx,:,:,2] = image[:,:,2]/255\n",
    "            batch_labels[fd, int(t[fd + (num_batches*batch_size)].strip().split(';')[2])] = 1    \n",
    "        yield batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 20\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/mnt/disks/user/project/PROJECT/Project_data/train'\n",
    "#train_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "val_path = '/mnt/disks/user/project/PROJECT/Project_data/val'\n",
    "#val_path = 'C://Users/pchadha/Neural_case_study/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 20# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 9\n",
    "model9 = Sequential()\n",
    "#help(Conv3D)\n",
    "# a keras convolutional layer is called Conv3D\n",
    "\n",
    "# note that the first layer needs to be told the input shape explicitly\n",
    "\n",
    "# first conv layer\n",
    "img_rows = 100\n",
    "img_cols = 100\n",
    "imnum = 20 # len(img_idx)\n",
    "num_classes = 5\n",
    "input_shape = (imnum, img_rows, img_cols, 3)\n",
    "model9.add(Conv3D(32, kernel_size=(3, 3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape)) # input shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# second conv layer\n",
    "model9.add(Conv3D(32, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model9.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model9.add(Dropout(0.25))\n",
    "\n",
    "# Third conv layer\n",
    "model9.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model9.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model9.add(Dropout(0.25))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model9.add(Flatten())\n",
    "model9.add(Dense(128, activation='relu')) # fully connected\n",
    "model9.add(Dropout(0.5))\n",
    "\n",
    "# softmax layer\n",
    "model9.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model summary\n",
    "#model1.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_13 (Conv3D)           (None, 18, 98, 98, 32)    2624      \n",
      "_________________________________________________________________\n",
      "conv3d_14 (Conv3D)           (None, 16, 96, 96, 32)    27680     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_7 (MaxPooling3 (None, 8, 48, 48, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 8, 48, 48, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_15 (Conv3D)           (None, 6, 46, 46, 64)     55360     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_8 (MaxPooling3 (None, 3, 23, 23, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 3, 23, 23, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 101568)            0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               13000832  \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 13,087,141\n",
      "Trainable params: 13,087,141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "optimiser = Adam(lr=learning_rate,clipvalue=1.0)\n",
    "model9.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model9.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pras_model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/val ; batch size = 15\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 15\n",
      "45/45 [==============================] - 137s 3s/step - loss: 1.7511 - categorical_accuracy: 0.2370 - val_loss: 1.6091 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00001: saving model to pras_model_init_2020-02-2822_42_49.563538/model-00001-1.75358-0.23529-1.60914-0.21000.h5\n",
      "Epoch 2/20\n",
      "45/45 [==============================] - 91s 2s/step - loss: 1.6091 - categorical_accuracy: 0.2148 - val_loss: 1.6064 - val_categorical_accuracy: 0.3700\n",
      "\n",
      "Epoch 00002: saving model to pras_model_init_2020-02-2822_42_49.563538/model-00002-1.60943-0.21267-1.60641-0.37000.h5\n",
      "Epoch 3/20\n",
      "45/45 [==============================] - 99s 2s/step - loss: 1.6088 - categorical_accuracy: 0.1882 - val_loss: 1.4747 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00003: saving model to pras_model_init_2020-02-2822_42_49.563538/model-00003-1.60901-0.19155-1.47472-0.29000.h5\n",
      "Epoch 4/20\n",
      "45/45 [==============================] - 83s 2s/step - loss: 1.5560 - categorical_accuracy: 0.3068 - val_loss: 1.4422 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00004: saving model to pras_model_init_2020-02-2822_42_49.563538/model-00004-1.55482-0.31222-1.44223-0.45000.h5\n",
      "Epoch 5/20\n",
      "45/45 [==============================] - 112s 2s/step - loss: 1.4689 - categorical_accuracy: 0.3438 - val_loss: 1.4048 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00005: saving model to pras_model_init_2020-02-2822_42_49.563538/model-00005-1.46649-0.34992-1.40476-0.42000.h5\n",
      "Epoch 6/20\n",
      "45/45 [==============================] - 107s 2s/step - loss: 1.3745 - categorical_accuracy: 0.4058 - val_loss: 1.3499 - val_categorical_accuracy: 0.4800\n",
      "\n",
      "Epoch 00006: saving model to pras_model_init_2020-02-2822_42_49.563538/model-00006-1.37977-0.40121-1.34990-0.48000.h5\n",
      "Epoch 7/20\n",
      "45/45 [==============================] - 106s 2s/step - loss: 1.3547 - categorical_accuracy: 0.4105 - val_loss: 1.3312 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00007: saving model to pras_model_init_2020-02-2822_42_49.563538/model-00007-1.35013-0.41780-1.33122-0.42000.h5\n",
      "Epoch 8/20\n",
      "45/45 [==============================] - 92s 2s/step - loss: 1.2343 - categorical_accuracy: 0.4697 - val_loss: 1.2794 - val_categorical_accuracy: 0.4300\n",
      "\n",
      "Epoch 00008: saving model to pras_model_init_2020-02-2822_42_49.563538/model-00008-1.23623-0.47210-1.27936-0.43000.h5\n",
      "Epoch 9/20\n",
      "45/45 [==============================] - 111s 2s/step - loss: 1.1497 - categorical_accuracy: 0.5229 - val_loss: 1.3938 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00009: saving model to pras_model_init_2020-02-2822_42_49.563538/model-00009-1.15479-0.52036-1.39377-0.44000.h5\n",
      "Epoch 10/20\n",
      "45/45 [==============================] - 86s 2s/step - loss: 1.0618 - categorical_accuracy: 0.5763 - val_loss: 1.2805 - val_categorical_accuracy: 0.4300\n",
      "\n",
      "Epoch 00010: saving model to pras_model_init_2020-02-2822_42_49.563538/model-00010-1.06637-0.57466-1.28049-0.43000.h5\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 11/20\n",
      "45/45 [==============================] - 109s 2s/step - loss: 0.9611 - categorical_accuracy: 0.6074 - val_loss: 1.3604 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00011: saving model to pras_model_init_2020-02-2822_42_49.563538/model-00011-0.95454-0.60633-1.36044-0.47000.h5\n",
      "Epoch 12/20\n",
      "45/45 [==============================] - 100s 2s/step - loss: 0.8767 - categorical_accuracy: 0.6370 - val_loss: 1.4147 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00012: saving model to pras_model_init_2020-02-2822_42_49.563538/model-00012-0.88300-0.63650-1.41469-0.47000.h5\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 13/20\n",
      "45/45 [==============================] - 84s 2s/step - loss: 0.8262 - categorical_accuracy: 0.6845 - val_loss: 1.4687 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00013: saving model to pras_model_init_2020-02-2822_42_49.563538/model-00013-0.82908-0.68477-1.46873-0.49000.h5\n",
      "Epoch 14/20\n",
      "45/45 [==============================] - 91s 2s/step - loss: 0.8102 - categorical_accuracy: 0.6904 - val_loss: 1.4967 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00014: saving model to pras_model_init_2020-02-2822_42_49.563538/model-00014-0.81774-0.69080-1.49665-0.49000.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 15/20\n",
      "45/45 [==============================] - 103s 2s/step - loss: 0.8223 - categorical_accuracy: 0.6683 - val_loss: 1.4977 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00015: saving model to pras_model_init_2020-02-2822_42_49.563538/model-00015-0.81886-0.67421-1.49770-0.49000.h5\n",
      "Epoch 16/20\n",
      "45/45 [==============================] - 111s 2s/step - loss: 0.8334 - categorical_accuracy: 0.6726 - val_loss: 1.5002 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00016: saving model to pras_model_init_2020-02-2822_42_49.563538/model-00016-0.81221-0.67270-1.50019-0.49000.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 17/20\n",
      "45/45 [==============================] - 84s 2s/step - loss: 0.8208 - categorical_accuracy: 0.6712 - val_loss: 1.4991 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00017: saving model to pras_model_init_2020-02-2822_42_49.563538/model-00017-0.81850-0.67722-1.49908-0.49000.h5\n",
      "Epoch 18/20\n",
      "45/45 [==============================] - 101s 2s/step - loss: 0.8168 - categorical_accuracy: 0.6947 - val_loss: 1.4994 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00018: saving model to pras_model_init_2020-02-2822_42_49.563538/model-00018-0.82069-0.68929-1.49939-0.49000.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 19/20\n",
      "45/45 [==============================] - 102s 2s/step - loss: 0.8287 - categorical_accuracy: 0.6665 - val_loss: 1.4991 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00019: saving model to pras_model_init_2020-02-2822_42_49.563538/model-00019-0.83854-0.66063-1.49912-0.49000.h5\n",
      "Epoch 20/20\n",
      "45/45 [==============================] - 90s 2s/step - loss: 0.8136 - categorical_accuracy: 0.6593 - val_loss: 1.4993 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00020: saving model to pras_model_init_2020-02-2822_42_49.563538/model-00020-0.81310-0.65913-1.49933-0.49000.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6efc0be7f0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit or training process\n",
    "model9.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model has performed better. However, needs to be optimized further. will add couple of more images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_path\n",
    "#folder_list\n",
    "#source_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29] #Selecting second half of images as the differentiating actions within video would most likely be represented by these images  \n",
    "    #img_idx = [x for x in range(12,30)] # Selecting three more images apart from second 15\n",
    "    #img_idx = [x for x in range(0,30,2)] #selected for second and third model and fifth model\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),100,100,3))# x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            #batch_data = np.zeros((batch_size,len(img_idx),64,64,3))\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    #image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = imgio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    # Cropping the image for second model analysis by 5%\n",
    "                    center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                    width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                    left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                    top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                    image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:] \n",
    "                    image = resize(image, (100,100,3)) # first model instance and fourth model\n",
    "                    # resizing image to 64x64 for second  and third iteration\n",
    "                    #image = resize(image, (64,64,3))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                #print(batch_labels.shape(0))\n",
    "                #print(image.shape(0))\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        diff = len(folder_list) - (num_batches*batch_size)\n",
    "        last_batch_size = diff\n",
    "        batch_data = np.zeros((last_batch_size,len(img_idx),100,100,3)) # first model instance and fourth model\n",
    "        #batch_data = np.zeros((last_batch_size,len(img_idx),64,64,3)) #second and third model instance\n",
    "        batch_labels = np.zeros((last_batch_size,5))\n",
    "        for fd in range(diff):\n",
    "            imgs = os.listdir(source_path+'/'+ t[fd + (num_batches*batch_size)].split(';')[0])\n",
    "            for idx,item in enumerate(img_idx):\n",
    "                image = imgio.imread(source_path+'/'+ t[fd + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                # Cropping the image for second model analysis by 5%\n",
    "                center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:]\n",
    "                image = resize(image, (100,100,3)) #first model instance and fourth model\n",
    "                #image = resize(image, (64,64,3)) # second and third model instance\n",
    "                batch_data[fd,idx,:,:,0] = image[:,:,0]/255 \n",
    "                batch_data[fd,idx,:,:,1] = image[:,:,1]/255 \n",
    "                batch_data[fd,idx,:,:,2] = image[:,:,2]/255\n",
    "            batch_labels[fd, int(t[fd + (num_batches*batch_size)].strip().split(';')[2])] = 1    \n",
    "        yield batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 20\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/mnt/disks/user/project/PROJECT/Project_data/train'\n",
    "#train_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "val_path = '/mnt/disks/user/project/PROJECT/Project_data/val'\n",
    "#val_path = 'C://Users/pchadha/Neural_case_study/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 20# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 10\n",
    "model10 = Sequential()\n",
    "#help(Conv3D)\n",
    "# a keras convolutional layer is called Conv3D\n",
    "\n",
    "# note that the first layer needs to be told the input shape explicitly\n",
    "\n",
    "# first conv layer\n",
    "img_rows = 100\n",
    "img_cols = 100\n",
    "imnum = 22 # len(img_idx)\n",
    "num_classes = 5\n",
    "input_shape = (imnum, img_rows, img_cols, 3)\n",
    "model10.add(Conv3D(32, kernel_size=(3, 3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape)) # input shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# second conv layer\n",
    "model10.add(Conv3D(32, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model10.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model10.add(Dropout(0.25))\n",
    "\n",
    "# Third conv layer\n",
    "model10.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model10.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model10.add(Dropout(0.25))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model10.add(Flatten())\n",
    "model10.add(Dense(128, activation='relu')) # fully connected\n",
    "model10.add(Dropout(0.5))\n",
    "\n",
    "# softmax layer\n",
    "model10.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model summary\n",
    "#model1.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_9 (Conv3D)            (None, 20, 98, 98, 32)    2624      \n",
      "_________________________________________________________________\n",
      "conv3d_10 (Conv3D)           (None, 18, 96, 96, 32)    27680     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 9, 48, 48, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 9, 48, 48, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_11 (Conv3D)           (None, 7, 46, 46, 64)     55360     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3 (None, 3, 23, 23, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 3, 23, 23, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 101568)            0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               13000832  \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 13,087,141\n",
      "Trainable params: 13,087,141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "optimiser = Adam(lr=learning_rate,clipvalue=1.0)\n",
    "model10.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model10.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pras_model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Source path = Source path =  /mnt/disks/user/project/PROJECT/Project_data/val ; batch size = 15\n",
      " /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 15\n",
      "45/45 [==============================] - 126s 3s/step - loss: 1.9927 - categorical_accuracy: 0.2043 - val_loss: 1.5216 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00001: saving model to pras_model_init_2020-02-2900_19_10.982650/model-00001-2.01209-0.19608-1.52160-0.23000.h5\n",
      "Epoch 2/20\n",
      "45/45 [==============================] - 112s 2s/step - loss: 1.5143 - categorical_accuracy: 0.2978 - val_loss: 1.4658 - val_categorical_accuracy: 0.4100\n",
      "\n",
      "Epoch 00002: saving model to pras_model_init_2020-02-2900_19_10.982650/model-00002-1.51432-0.29713-1.46578-0.41000.h5\n",
      "Epoch 3/20\n",
      "45/45 [==============================] - 101s 2s/step - loss: 1.4322 - categorical_accuracy: 0.3764 - val_loss: 1.3663 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00003: saving model to pras_model_init_2020-02-2900_19_10.982650/model-00003-1.42334-0.38311-1.36627-0.44000.h5\n",
      "Epoch 4/20\n",
      "45/45 [==============================] - 92s 2s/step - loss: 1.3173 - categorical_accuracy: 0.4371 - val_loss: 1.3668 - val_categorical_accuracy: 0.4100\n",
      "\n",
      "Epoch 00004: saving model to pras_model_init_2020-02-2900_19_10.982650/model-00004-1.31968-0.43891-1.36682-0.41000.h5\n",
      "Epoch 5/20\n",
      "45/45 [==============================] - 100s 2s/step - loss: 1.2552 - categorical_accuracy: 0.4817 - val_loss: 1.2972 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00005: saving model to pras_model_init_2020-02-2900_19_10.982650/model-00005-1.24792-0.49020-1.29722-0.45000.h5\n",
      "Epoch 6/20\n",
      "45/45 [==============================] - 90s 2s/step - loss: 1.1564 - categorical_accuracy: 0.5097 - val_loss: 1.4071 - val_categorical_accuracy: 0.4300\n",
      "\n",
      "Epoch 00006: saving model to pras_model_init_2020-02-2900_19_10.982650/model-00006-1.15923-0.51282-1.40715-0.43000.h5\n",
      "Epoch 7/20\n",
      "45/45 [==============================] - 113s 3s/step - loss: 0.9825 - categorical_accuracy: 0.6265 - val_loss: 1.1533 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00007: saving model to pras_model_init_2020-02-2900_19_10.982650/model-00007-0.99155-0.61991-1.15334-0.54000.h5\n",
      "Epoch 8/20\n",
      "45/45 [==============================] - 116s 3s/step - loss: 0.7957 - categorical_accuracy: 0.6756 - val_loss: 1.2897 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00008: saving model to pras_model_init_2020-02-2900_19_10.982650/model-00008-0.79509-0.67572-1.28966-0.53000.h5\n",
      "Epoch 9/20\n",
      "45/45 [==============================] - 107s 2s/step - loss: 0.6717 - categorical_accuracy: 0.7658 - val_loss: 1.2088 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00009: saving model to pras_model_init_2020-02-2900_19_10.982650/model-00009-0.68077-0.76169-1.20878-0.61000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 10/20\n",
      "45/45 [==============================] - 120s 3s/step - loss: 0.4604 - categorical_accuracy: 0.8340 - val_loss: 1.1692 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00010: saving model to pras_model_init_2020-02-2900_19_10.982650/model-00010-0.46716-0.83107-1.16916-0.56000.h5\n",
      "Epoch 11/20\n",
      "45/45 [==============================] - 119s 3s/step - loss: 0.3678 - categorical_accuracy: 0.8638 - val_loss: 1.2000 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00011: saving model to pras_model_init_2020-02-2900_19_10.982650/model-00011-0.36731-0.86727-1.19997-0.57000.h5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 12/20\n",
      "45/45 [==============================] - 110s 2s/step - loss: 0.2852 - categorical_accuracy: 0.9111 - val_loss: 1.2366 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00012: saving model to pras_model_init_2020-02-2900_19_10.982650/model-00012-0.29019-0.90950-1.23656-0.58000.h5\n",
      "Epoch 13/20\n",
      "45/45 [==============================] - 96s 2s/step - loss: 0.3025 - categorical_accuracy: 0.8934 - val_loss: 1.2387 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00013: saving model to pras_model_init_2020-02-2900_19_10.982650/model-00013-0.29094-0.89744-1.23871-0.57000.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 14/20\n",
      "45/45 [==============================] - 88s 2s/step - loss: 0.2831 - categorical_accuracy: 0.9037 - val_loss: 1.2418 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00014: saving model to pras_model_init_2020-02-2900_19_10.982650/model-00014-0.28500-0.90196-1.24183-0.57000.h5\n",
      "Epoch 15/20\n",
      "45/45 [==============================] - 119s 3s/step - loss: 0.2554 - categorical_accuracy: 0.9126 - val_loss: 1.2495 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00015: saving model to pras_model_init_2020-02-2900_19_10.982650/model-00015-0.25951-0.91101-1.24947-0.58000.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 16/20\n",
      "45/45 [==============================] - 112s 2s/step - loss: 0.2476 - categorical_accuracy: 0.9229 - val_loss: 1.2502 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00016: saving model to pras_model_init_2020-02-2900_19_10.982650/model-00016-0.25087-0.92157-1.25022-0.58000.h5\n",
      "Epoch 17/20\n",
      "45/45 [==============================] - 92s 2s/step - loss: 0.2839 - categorical_accuracy: 0.8979 - val_loss: 1.2516 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00017: saving model to pras_model_init_2020-02-2900_19_10.982650/model-00017-0.28061-0.90196-1.25158-0.58000.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 18/20\n",
      "45/45 [==============================] - 118s 3s/step - loss: 0.2788 - categorical_accuracy: 0.9022 - val_loss: 1.2518 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00018: saving model to pras_model_init_2020-02-2900_19_10.982650/model-00018-0.27975-0.90045-1.25179-0.58000.h5\n",
      "Epoch 19/20\n",
      "45/45 [==============================] - 91s 2s/step - loss: 0.2768 - categorical_accuracy: 0.8875 - val_loss: 1.2521 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00019: saving model to pras_model_init_2020-02-2900_19_10.982650/model-00019-0.27677-0.89140-1.25207-0.58000.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "Epoch 20/20\n",
      "45/45 [==============================] - 109s 2s/step - loss: 0.2608 - categorical_accuracy: 0.9022 - val_loss: 1.2521 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00020: saving model to pras_model_init_2020-02-2900_19_10.982650/model-00020-0.26508-0.90045-1.25210-0.58000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f10c8f1b748>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit or training process\n",
    "model10.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being similar to model-1, model is performing great in training set but validation scores are quite less. Will keep the number of images the same but will scale the image to 64x64. Will also, make the last layer more dense "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_path\n",
    "#folder_list\n",
    "#source_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29] #Selecting second half of images as the differentiating actions within video would most likely be represented by these images  \n",
    "    #img_idx = [x for x in range(12,30)] # Selecting three more images apart from second 15\n",
    "    #img_idx = [x for x in range(0,30,2)] #selected for second and third model and fifth model\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            #batch_data = np.zeros((batch_size,len(img_idx),100,100,3))# x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),64,64,3))\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    #image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = imgio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    # Cropping the image for second model analysis by 5%\n",
    "                    center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                    width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                    left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                    top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                    image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:] \n",
    "                    #image = resize(image, (100,100,3)) # first model instance and fourth model\n",
    "                    # resizing image to 64x64 for second  and third iteration. Also for model-11\n",
    "                    image = resize(image, (64,64,3))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                #print(batch_labels.shape(0))\n",
    "                #print(image.shape(0))\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        diff = len(folder_list) - (num_batches*batch_size)\n",
    "        last_batch_size = diff\n",
    "        #batch_data = np.zeros((last_batch_size,len(img_idx),100,100,3)) # first model instance and fourth model\n",
    "        batch_data = np.zeros((last_batch_size,len(img_idx),64,64,3)) #second and third model instance, model 11 as well\n",
    "        batch_labels = np.zeros((last_batch_size,5))\n",
    "        for fd in range(diff):\n",
    "            imgs = os.listdir(source_path+'/'+ t[fd + (num_batches*batch_size)].split(';')[0])\n",
    "            for idx,item in enumerate(img_idx):\n",
    "                image = imgio.imread(source_path+'/'+ t[fd + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                # Cropping the image for second model analysis by 5%\n",
    "                center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:]\n",
    "                #image = resize(image, (100,100,3)) #first model instance and fourth model\n",
    "                image = resize(image, (64,64,3)) # second and third model instance\n",
    "                batch_data[fd,idx,:,:,0] = image[:,:,0]/255 \n",
    "                batch_data[fd,idx,:,:,1] = image[:,:,1]/255 \n",
    "                batch_data[fd,idx,:,:,2] = image[:,:,2]/255\n",
    "            batch_labels[fd, int(t[fd + (num_batches*batch_size)].strip().split(';')[2])] = 1    \n",
    "        yield batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 20\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/mnt/disks/user/project/PROJECT/Project_data/train'\n",
    "#train_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "val_path = '/mnt/disks/user/project/PROJECT/Project_data/val'\n",
    "#val_path = 'C://Users/pchadha/Neural_case_study/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 20# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0229 01:44:59.478920 140215828494144 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0229 01:44:59.482001 140215828494144 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0229 01:44:59.523870 140215828494144 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0229 01:44:59.563205 140215828494144 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0229 01:44:59.570627 140215828494144 deprecation.py:506] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# model 11\n",
    "model11 = Sequential()\n",
    "#help(Conv3D)\n",
    "# a keras convolutional layer is called Conv3D\n",
    "\n",
    "# note that the first layer needs to be told the input shape explicitly\n",
    "\n",
    "# first conv layer\n",
    "img_rows = 64\n",
    "img_cols = 64\n",
    "imnum = 22 # len(img_idx)\n",
    "num_classes = 5\n",
    "input_shape = (imnum, img_rows, img_cols, 3)\n",
    "model11.add(Conv3D(32, kernel_size=(3, 3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape)) # input shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# second conv layer\n",
    "model11.add(Conv3D(32, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model11.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model11.add(Dropout(0.25))\n",
    "\n",
    "# Third conv layer\n",
    "model11.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model11.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model11.add(Dropout(0.25))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model11.add(Flatten())\n",
    "model11.add(Dense(256, activation='relu')) # fully connected\n",
    "model11.add(Dropout(0.5))\n",
    "\n",
    "# softmax layer\n",
    "model11.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model summary\n",
    "#model1.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0229 01:45:07.634571 140215828494144 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0229 01:45:07.642872 140215828494144 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 20, 62, 62, 32)    2624      \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 18, 60, 60, 32)    27680     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 9, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 9, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 7, 28, 28, 64)     55360     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 3, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 37632)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               9634048   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 9,720,997\n",
      "Trainable params: 9,720,997\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "optimiser = Adam(lr=learning_rate,clipvalue=1.0)\n",
    "model11.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model11.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pras_model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0229 01:45:24.639739 140215828494144 deprecation.py:323] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0229 01:45:24.972927 140215828494144 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0229 01:45:25.117365 140215828494144 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "W0229 01:45:25.251022 140215828494144 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/val ; batch size = 15\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0229 01:45:31.139109 140215828494144 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0229 01:45:31.140705 140215828494144 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0229 01:45:37.888105 140215828494144 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W0229 01:45:37.889829 140215828494144 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "W0229 01:45:39.274640 140215828494144 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 224s 5s/step - loss: 1.7180 - categorical_accuracy: 0.2028 - val_loss: 1.4360 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00001: saving model to pras_model_init_2020-02-2901_44_53.123820/model-00001-1.72903-0.19457-1.43603-0.23000.h5\n",
      "Epoch 2/20\n",
      "45/45 [==============================] - 115s 3s/step - loss: 1.4773 - categorical_accuracy: 0.3141 - val_loss: 1.3749 - val_categorical_accuracy: 0.4300\n",
      "\n",
      "Epoch 00002: saving model to pras_model_init_2020-02-2901_44_53.123820/model-00002-1.47796-0.31373-1.37486-0.43000.h5\n",
      "Epoch 3/20\n",
      "45/45 [==============================] - 109s 2s/step - loss: 1.3263 - categorical_accuracy: 0.4134 - val_loss: 1.1103 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00003: saving model to pras_model_init_2020-02-2901_44_53.123820/model-00003-1.32965-0.41478-1.11029-0.54000.h5\n",
      "Epoch 4/20\n",
      "45/45 [==============================] - 114s 3s/step - loss: 1.1778 - categorical_accuracy: 0.4919 - val_loss: 0.9484 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00004: saving model to pras_model_init_2020-02-2901_44_53.123820/model-00004-1.17814-0.49472-0.94843-0.61000.h5\n",
      "Epoch 5/20\n",
      "45/45 [==============================] - 147s 3s/step - loss: 0.9851 - categorical_accuracy: 0.5897 - val_loss: 0.8957 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00005: saving model to pras_model_init_2020-02-2901_44_53.123820/model-00005-0.98527-0.59427-0.89565-0.66000.h5\n",
      "Epoch 6/20\n",
      "45/45 [==============================] - 83s 2s/step - loss: 0.8628 - categorical_accuracy: 0.6578 - val_loss: 1.0423 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00006: saving model to pras_model_init_2020-02-2901_44_53.123820/model-00006-0.86139-0.65762-1.04227-0.67000.h5\n",
      "Epoch 7/20\n",
      "45/45 [==============================] - 95s 2s/step - loss: 0.6918 - categorical_accuracy: 0.7347 - val_loss: 0.7310 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00007: saving model to pras_model_init_2020-02-2901_44_53.123820/model-00007-0.70240-0.73002-0.73095-0.72000.h5\n",
      "Epoch 8/20\n",
      "45/45 [==============================] - 90s 2s/step - loss: 0.5631 - categorical_accuracy: 0.7793 - val_loss: 0.7414 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00008: saving model to pras_model_init_2020-02-2901_44_53.123820/model-00008-0.55949-0.78130-0.74145-0.70000.h5\n",
      "Epoch 9/20\n",
      "45/45 [==============================] - 76s 2s/step - loss: 0.4680 - categorical_accuracy: 0.8208 - val_loss: 0.8025 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00009: saving model to pras_model_init_2020-02-2901_44_53.123820/model-00009-0.46028-0.82353-0.80247-0.67000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 10/20\n",
      "45/45 [==============================] - 72s 2s/step - loss: 0.2899 - categorical_accuracy: 0.9082 - val_loss: 0.7616 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00010: saving model to pras_model_init_2020-02-2901_44_53.123820/model-00010-0.28401-0.91252-0.76160-0.69000.h5\n",
      "Epoch 11/20\n",
      "45/45 [==============================] - 74s 2s/step - loss: 0.2234 - categorical_accuracy: 0.9364 - val_loss: 0.7193 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00011: saving model to pras_model_init_2020-02-2901_44_53.123820/model-00011-0.21724-0.94118-0.71932-0.75000.h5\n",
      "Epoch 12/20\n",
      "45/45 [==============================] - 71s 2s/step - loss: 0.1753 - categorical_accuracy: 0.9437 - val_loss: 0.7234 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00012: saving model to pras_model_init_2020-02-2901_44_53.123820/model-00012-0.17835-0.94268-0.72336-0.75000.h5\n",
      "Epoch 13/20\n",
      "45/45 [==============================] - 98s 2s/step - loss: 0.1382 - categorical_accuracy: 0.9468 - val_loss: 0.7893 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00013: saving model to pras_model_init_2020-02-2901_44_53.123820/model-00013-0.13498-0.95173-0.78926-0.74000.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 14/20\n",
      "45/45 [==============================] - 73s 2s/step - loss: 0.1092 - categorical_accuracy: 0.9763 - val_loss: 0.7498 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00014: saving model to pras_model_init_2020-02-2901_44_53.123820/model-00014-0.10481-0.97587-0.74975-0.75000.h5\n",
      "Epoch 15/20\n",
      "45/45 [==============================] - 75s 2s/step - loss: 0.1290 - categorical_accuracy: 0.9585 - val_loss: 0.7670 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00015: saving model to pras_model_init_2020-02-2901_44_53.123820/model-00015-0.12838-0.95777-0.76699-0.75000.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 16/20\n",
      "45/45 [==============================] - 77s 2s/step - loss: 0.0912 - categorical_accuracy: 0.9778 - val_loss: 0.7549 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00016: saving model to pras_model_init_2020-02-2901_44_53.123820/model-00016-0.09219-0.97738-0.75492-0.75000.h5\n",
      "Epoch 17/20\n",
      "45/45 [==============================] - 88s 2s/step - loss: 0.0984 - categorical_accuracy: 0.9704 - val_loss: 0.7459 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00017: saving model to pras_model_init_2020-02-2901_44_53.123820/model-00017-0.10010-0.96983-0.74589-0.74000.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 18/20\n",
      "45/45 [==============================] - 78s 2s/step - loss: 0.0914 - categorical_accuracy: 0.9689 - val_loss: 0.7462 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00018: saving model to pras_model_init_2020-02-2901_44_53.123820/model-00018-0.09297-0.96833-0.74623-0.74000.h5\n",
      "Epoch 19/20\n",
      "45/45 [==============================] - 97s 2s/step - loss: 0.0951 - categorical_accuracy: 0.9733 - val_loss: 0.7462 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00019: saving model to pras_model_init_2020-02-2901_44_53.123820/model-00019-0.09377-0.97285-0.74623-0.74000.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 20/20\n",
      "45/45 [==============================] - 93s 2s/step - loss: 0.1022 - categorical_accuracy: 0.9674 - val_loss: 0.7466 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00020: saving model to pras_model_init_2020-02-2901_44_53.123820/model-00020-0.10188-0.96682-0.74656-0.74000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f85f98216d8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit or training process\n",
    "model11.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model above has performed much better with validate data as well and training score is exceptionally high. Still, difference between train and validation data is much more than desired. So will need to be optimized further. Will crop the image by another 5% for this test. Also, will add two dense layer network rather than one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_path\n",
    "#folder_list\n",
    "#source_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29] #Selecting second half of images as the differentiating actions within video would most likely be represented by these images  \n",
    "    #img_idx = [x for x in range(12,30)] # Selecting three more images apart from second 15\n",
    "    #img_idx = [x for x in range(0,30,2)] #selected for second and third model and fifth model\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            #batch_data = np.zeros((batch_size,len(img_idx),100,100,3))# x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),64,64,3))\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    #image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = imgio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    # Cropping the image for second model analysis by 10%\n",
    "                    center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                    width_scaled, height_scaled = image.shape[1] * 0.90, image.shape[0] * 0.90 # Corrected the scaling factor for model 5 iteration\n",
    "                    left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                    top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                    image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:] \n",
    "                    #image = resize(image, (100,100,3)) # first model instance and fourth model\n",
    "                    # resizing image to 64x64 for second  and third iteration. Also for model-11\n",
    "                    image = resize(image, (64,64,3))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                #print(batch_labels.shape(0))\n",
    "                #print(image.shape(0))\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        diff = len(folder_list) - (num_batches*batch_size)\n",
    "        last_batch_size = diff\n",
    "        #batch_data = np.zeros((last_batch_size,len(img_idx),100,100,3)) # first model instance and fourth model\n",
    "        batch_data = np.zeros((last_batch_size,len(img_idx),64,64,3)) #second and third model instance, model 11 as well\n",
    "        batch_labels = np.zeros((last_batch_size,5))\n",
    "        for fd in range(diff):\n",
    "            imgs = os.listdir(source_path+'/'+ t[fd + (num_batches*batch_size)].split(';')[0])\n",
    "            for idx,item in enumerate(img_idx):\n",
    "                image = imgio.imread(source_path+'/'+ t[fd + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                # Cropping the image for second model analysis by 10%\n",
    "                center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                width_scaled, height_scaled = image.shape[1] * 0.90, image.shape[0] * 0.90 # Corrected the scaling factor for model 5 iteration\n",
    "                left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:]\n",
    "                #image = resize(image, (100,100,3)) #first model instance and fourth model\n",
    "                image = resize(image, (64,64,3)) # second and third model instance\n",
    "                batch_data[fd,idx,:,:,0] = image[:,:,0]/255 \n",
    "                batch_data[fd,idx,:,:,1] = image[:,:,1]/255 \n",
    "                batch_data[fd,idx,:,:,2] = image[:,:,2]/255\n",
    "            batch_labels[fd, int(t[fd + (num_batches*batch_size)].strip().split(';')[2])] = 1    \n",
    "        yield batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 20\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/mnt/disks/user/project/PROJECT/Project_data/train'\n",
    "#train_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "val_path = '/mnt/disks/user/project/PROJECT/Project_data/val'\n",
    "#val_path = 'C://Users/pchadha/Neural_case_study/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 20# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 12\n",
    "model12 = Sequential()\n",
    "#help(Conv3D)\n",
    "# a keras convolutional layer is called Conv3D\n",
    "\n",
    "# note that the first layer needs to be told the input shape explicitly\n",
    "\n",
    "# first conv layer\n",
    "img_rows = 64\n",
    "img_cols = 64\n",
    "imnum = 22 # len(img_idx)\n",
    "num_classes = 5\n",
    "input_shape = (imnum, img_rows, img_cols, 3)\n",
    "model12.add(Conv3D(32, kernel_size=(3, 3, 3),\n",
    "                 activation='relu', \n",
    "                 input_shape=input_shape)) # input shape = (img_rows, img_cols, 1)\n",
    "model12.add(BatchNormalization())\n",
    "# second conv layer\n",
    "model12.add(Conv3D(32, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model12.add(BatchNormalization())\n",
    "model12.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model12.add(Dropout(0.25))\n",
    "\n",
    "# Third conv layer\n",
    "model12.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model12.add(BatchNormalization())\n",
    "model12.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model12.add(Dropout(0.25))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model12.add(Flatten())\n",
    "model12.add(Dense(512, activation='relu')) # fully connected\n",
    "model12.add(Dropout(0.5))\n",
    "model12.add(Dense(256, activation='relu')) # fully connected\n",
    "model12.add(Dropout(0.5))\n",
    "\n",
    "# softmax layer\n",
    "model12.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model summary\n",
    "#model1.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_10 (Conv3D)           (None, 20, 62, 62, 32)    2624      \n",
      "_________________________________________________________________\n",
      "conv3d_11 (Conv3D)           (None, 18, 60, 60, 32)    27680     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_7 (MaxPooling3 (None, 9, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 9, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_12 (Conv3D)           (None, 7, 28, 28, 64)     55360     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_8 (MaxPooling3 (None, 3, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 3, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 37632)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               19268096  \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 19,486,373\n",
      "Trainable params: 19,486,373\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "optimiser = Adam(lr=learning_rate,clipvalue=1.0)\n",
    "model12.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model12.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pras_model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 15\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/val ; batch size = 15\n",
      "45/45 [==============================] - 92s 2s/step - loss: 1.7009 - categorical_accuracy: 0.1867 - val_loss: 1.6095 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00001: saving model to pras_model_init_2020-02-2913_22_01.835205/model-00001-1.70247-0.19005-1.60951-0.18000.h5\n",
      "Epoch 2/20\n",
      "45/45 [==============================] - 104s 2s/step - loss: 1.6110 - categorical_accuracy: 0.2075 - val_loss: 1.6069 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00002: saving model to pras_model_init_2020-02-2913_22_01.835205/model-00002-1.61090-0.21116-1.60695-0.18000.h5\n",
      "Epoch 3/20\n",
      "45/45 [==============================] - 85s 2s/step - loss: 1.6114 - categorical_accuracy: 0.2074 - val_loss: 1.6077 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00003: saving model to pras_model_init_2020-02-2913_22_01.835205/model-00003-1.61131-0.20513-1.60771-0.21000.h5\n",
      "Epoch 4/20\n",
      "45/45 [==============================] - 96s 2s/step - loss: 1.6092 - categorical_accuracy: 0.2327 - val_loss: 1.6077 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00004: saving model to pras_model_init_2020-02-2913_22_01.835205/model-00004-1.60913-0.23680-1.60773-0.21000.h5\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 5/20\n",
      "45/45 [==============================] - 102s 2s/step - loss: 1.6087 - categorical_accuracy: 0.1882 - val_loss: 1.6078 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00005: saving model to pras_model_init_2020-02-2913_22_01.835205/model-00005-1.60857-0.19155-1.60782-0.21000.h5\n",
      "Epoch 6/20\n",
      "45/45 [==============================] - 107s 2s/step - loss: 1.6095 - categorical_accuracy: 0.2044 - val_loss: 1.6077 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00006: saving model to pras_model_init_2020-02-2913_22_01.835205/model-00006-1.60940-0.20211-1.60770-0.21000.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 7/20\n",
      "45/45 [==============================] - 88s 2s/step - loss: 1.6077 - categorical_accuracy: 0.1927 - val_loss: 1.6077 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00007: saving model to pras_model_init_2020-02-2913_22_01.835205/model-00007-1.60783-0.19608-1.60766-0.21000.h5\n",
      "Epoch 8/20\n",
      "45/45 [==============================] - 111s 2s/step - loss: 1.6089 - categorical_accuracy: 0.1985 - val_loss: 1.6076 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00008: saving model to pras_model_init_2020-02-2913_22_01.835205/model-00008-1.60905-0.19608-1.60761-0.21000.h5\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 9/20\n",
      "45/45 [==============================] - 107s 2s/step - loss: 1.6083 - categorical_accuracy: 0.2118 - val_loss: 1.6076 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00009: saving model to pras_model_init_2020-02-2913_22_01.835205/model-00009-1.60830-0.20965-1.60761-0.21000.h5\n",
      "Epoch 10/20\n",
      "45/45 [==============================] - 74s 2s/step - loss: 1.6081 - categorical_accuracy: 0.2371 - val_loss: 1.6076 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00010: saving model to pras_model_init_2020-02-2913_22_01.835205/model-00010-1.60787-0.24133-1.60760-0.21000.h5\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 11/20\n",
      "45/45 [==============================] - 94s 2s/step - loss: 1.6081 - categorical_accuracy: 0.2075 - val_loss: 1.6076 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00011: saving model to pras_model_init_2020-02-2913_22_01.835205/model-00011-1.60834-0.21116-1.60760-0.21000.h5\n",
      "Epoch 12/20\n",
      "45/45 [==============================] - 81s 2s/step - loss: 1.6078 - categorical_accuracy: 0.2235 - val_loss: 1.6076 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00012: saving model to pras_model_init_2020-02-2913_22_01.835205/model-00012-1.60822-0.21569-1.60759-0.21000.h5\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 13/20\n",
      "45/45 [==============================] - 108s 2s/step - loss: 1.6091 - categorical_accuracy: 0.2059 - val_loss: 1.6076 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00013: saving model to pras_model_init_2020-02-2913_22_01.835205/model-00013-1.60920-0.20362-1.60759-0.21000.h5\n",
      "Epoch 14/20\n",
      "45/45 [==============================] - 76s 2s/step - loss: 1.6097 - categorical_accuracy: 0.2149 - val_loss: 1.6076 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00014: saving model to pras_model_init_2020-02-2913_22_01.835205/model-00014-1.60975-0.21870-1.60759-0.21000.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "Epoch 15/20\n",
      "45/45 [==============================] - 98s 2s/step - loss: 1.6097 - categorical_accuracy: 0.1999 - val_loss: 1.6076 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00015: saving model to pras_model_init_2020-02-2913_22_01.835205/model-00015-1.60962-0.19759-1.60759-0.21000.h5\n",
      "Epoch 16/20\n",
      "45/45 [==============================] - 75s 2s/step - loss: 1.6079 - categorical_accuracy: 0.2353 - val_loss: 1.6076 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00016: saving model to pras_model_init_2020-02-2913_22_01.835205/model-00016-1.60841-0.22172-1.60759-0.21000.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
      "Epoch 17/20\n",
      "45/45 [==============================] - 104s 2s/step - loss: 1.6085 - categorical_accuracy: 0.1986 - val_loss: 1.6076 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00017: saving model to pras_model_init_2020-02-2913_22_01.835205/model-00017-1.60856-0.20211-1.60759-0.21000.h5\n",
      "Epoch 18/20\n",
      "45/45 [==============================] - 86s 2s/step - loss: 1.6093 - categorical_accuracy: 0.2016 - val_loss: 1.6076 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00018: saving model to pras_model_init_2020-02-2913_22_01.835205/model-00018-1.60900-0.20513-1.60759-0.21000.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
      "Epoch 19/20\n",
      "45/45 [==============================] - 89s 2s/step - loss: 1.6090 - categorical_accuracy: 0.1985 - val_loss: 1.6076 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00019: saving model to pras_model_init_2020-02-2913_22_01.835205/model-00019-1.60899-0.19608-1.60759-0.21000.h5\n",
      "Epoch 20/20\n",
      "45/45 [==============================] - 87s 2s/step - loss: 1.6089 - categorical_accuracy: 0.1897 - val_loss: 1.6076 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00020: saving model to pras_model_init_2020-02-2913_22_01.835205/model-00020-1.60872-0.19306-1.60759-0.21000.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc12b276080>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit or training process\n",
    "model12.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model ended up performing poorly. Will also, not use Batch Normalization as it has proven to be counterproductive in all models where I used it so far. Will now keep the cropping size as 5% only. Will have one Dense layer but that of 512, i.e., one denser layer and increase the number of filters  of second conv3D layer to 64. Will use 'l2' kernel regularizer to see if this helps in reducing overfitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_path\n",
    "#folder_list\n",
    "#source_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29] #Selecting second half of images as the differentiating actions within video would most likely be represented by these images  \n",
    "    #img_idx = [x for x in range(12,30)] # Selecting three more images apart from second 15\n",
    "    #img_idx = [x for x in range(0,30,2)] #selected for second and third model and fifth model\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            #batch_data = np.zeros((batch_size,len(img_idx),100,100,3))# x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),64,64,3))\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    #image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = imgio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    # Cropping the image for second model analysis by 10%\n",
    "                    center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                    width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                    left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                    top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                    image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:] \n",
    "                    #image = resize(image, (100,100,3)) # first model instance and fourth model\n",
    "                    # resizing image to 64x64 for second  and third iteration. Also for model-11\n",
    "                    image = resize(image, (64,64,3))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                #print(batch_labels.shape(0))\n",
    "                #print(image.shape(0))\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        diff = len(folder_list) - (num_batches*batch_size)\n",
    "        last_batch_size = diff\n",
    "        #batch_data = np.zeros((last_batch_size,len(img_idx),100,100,3)) # first model instance and fourth model\n",
    "        batch_data = np.zeros((last_batch_size,len(img_idx),64,64,3)) #second and third model instance, model 11 as well\n",
    "        batch_labels = np.zeros((last_batch_size,5))\n",
    "        for fd in range(diff):\n",
    "            imgs = os.listdir(source_path+'/'+ t[fd + (num_batches*batch_size)].split(';')[0])\n",
    "            for idx,item in enumerate(img_idx):\n",
    "                image = imgio.imread(source_path+'/'+ t[fd + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                # Cropping the image for second model analysis by 10%\n",
    "                center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:]\n",
    "                #image = resize(image, (100,100,3)) #first model instance and fourth model\n",
    "                image = resize(image, (64,64,3)) # second and third model instance\n",
    "                batch_data[fd,idx,:,:,0] = image[:,:,0]/255 \n",
    "                batch_data[fd,idx,:,:,1] = image[:,:,1]/255 \n",
    "                batch_data[fd,idx,:,:,2] = image[:,:,2]/255\n",
    "            batch_labels[fd, int(t[fd + (num_batches*batch_size)].strip().split(';')[2])] = 1    \n",
    "        yield batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 20\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/mnt/disks/user/project/PROJECT/Project_data/train'\n",
    "#train_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "val_path = '/mnt/disks/user/project/PROJECT/Project_data/val'\n",
    "#val_path = 'C://Users/pchadha/Neural_case_study/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 20# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 13\n",
    "model13 = Sequential()\n",
    "#help(Conv3D)\n",
    "# a keras convolutional layer is called Conv3D\n",
    "\n",
    "# note that the first layer needs to be told the input shape explicitly\n",
    "\n",
    "# first conv layer\n",
    "img_rows = 64\n",
    "img_cols = 64\n",
    "imnum = 22 # len(img_idx)\n",
    "num_classes = 5\n",
    "input_shape = (imnum, img_rows, img_cols, 3)\n",
    "model13.add(Conv3D(32, kernel_size=(3, 3, 3),\n",
    "                 activation='relu', \n",
    "                 input_shape=input_shape)) # input shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# second conv layer\n",
    "model13.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model13.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model13.add(Dropout(0.25))\n",
    "\n",
    "# Third conv layer\n",
    "model13.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model13.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model13.add(Dropout(0.25))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model13.add(Flatten())\n",
    "model13.add(Dense(512, kernel_regularizer = regularizers.l2(0.01), activation='relu')) # fully connected\n",
    "model13.add(Dropout(0.5))\n",
    "\n",
    "# softmax layer\n",
    "model13.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model summary\n",
    "#model1.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_22 (Conv3D)           (None, 20, 62, 62, 32)    2624      \n",
      "_________________________________________________________________\n",
      "conv3d_23 (Conv3D)           (None, 18, 60, 60, 64)    55360     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_15 (MaxPooling (None, 9, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 9, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_24 (Conv3D)           (None, 7, 28, 28, 64)     110656    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_16 (MaxPooling (None, 3, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 3, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 37632)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               19268096  \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 19,439,301\n",
      "Trainable params: 19,439,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "optimiser = Adam(lr=learning_rate,clipvalue=1.0)\n",
    "model13.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model13.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pras_model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 15\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/val ; batch size = 15\n",
      "45/45 [==============================] - 103s 2s/step - loss: 5.8505 - categorical_accuracy: 0.2133 - val_loss: 2.8211 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00001: saving model to pras_model_init_2020-02-2914_04_03.771783/model-00001-5.90320-0.21116-2.82106-0.21000.h5\n",
      "Epoch 2/20\n",
      "45/45 [==============================] - 76s 2s/step - loss: 2.1902 - categorical_accuracy: 0.2088 - val_loss: 1.8539 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00002: saving model to pras_model_init_2020-02-2914_04_03.771783/model-00002-2.19605-0.20664-1.85387-0.21000.h5\n",
      "Epoch 3/20\n",
      "45/45 [==============================] - 105s 2s/step - loss: 1.7471 - categorical_accuracy: 0.2030 - val_loss: 1.6776 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00003: saving model to pras_model_init_2020-02-2914_04_03.771783/model-00003-1.74777-0.20664-1.67755-0.21000.h5\n",
      "Epoch 4/20\n",
      "45/45 [==============================] - 108s 2s/step - loss: 1.6522 - categorical_accuracy: 0.2030 - val_loss: 1.6323 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00004: saving model to pras_model_init_2020-02-2914_04_03.771783/model-00004-1.65249-0.20664-1.63234-0.21000.h5\n",
      "Epoch 5/20\n",
      "45/45 [==============================] - 90s 2s/step - loss: 1.6241 - categorical_accuracy: 0.2074 - val_loss: 1.6174 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00005: saving model to pras_model_init_2020-02-2914_04_03.771783/model-00005-1.62428-0.20513-1.61743-0.21000.h5\n",
      "Epoch 6/20\n",
      "45/45 [==============================] - 85s 2s/step - loss: 1.6150 - categorical_accuracy: 0.1970 - val_loss: 1.6118 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00006: saving model to pras_model_init_2020-02-2914_04_03.771783/model-00006-1.61540-0.19457-1.61183-0.21000.h5\n",
      "Epoch 7/20\n",
      "45/45 [==============================] - 87s 2s/step - loss: 1.6115 - categorical_accuracy: 0.2044 - val_loss: 1.6094 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00007: saving model to pras_model_init_2020-02-2914_04_03.771783/model-00007-1.61174-0.20211-1.60938-0.21000.h5\n",
      "Epoch 8/20\n",
      "45/45 [==============================] - 84s 2s/step - loss: 1.6102 - categorical_accuracy: 0.2016 - val_loss: 1.6081 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00008: saving model to pras_model_init_2020-02-2914_04_03.771783/model-00008-1.61007-0.20513-1.60814-0.21000.h5\n",
      "Epoch 9/20\n",
      "45/45 [==============================] - 78s 2s/step - loss: 1.6095 - categorical_accuracy: 0.1836 - val_loss: 1.6080 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00009: saving model to pras_model_init_2020-02-2914_04_03.771783/model-00009-1.60942-0.18100-1.60804-0.21000.h5\n",
      "Epoch 10/20\n",
      "45/45 [==============================] - 81s 2s/step - loss: 1.6088 - categorical_accuracy: 0.2074 - val_loss: 1.6080 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00010: saving model to pras_model_init_2020-02-2914_04_03.771783/model-00010-1.60929-0.20513-1.60798-0.21000.h5\n",
      "Epoch 11/20\n",
      "45/45 [==============================] - 104s 2s/step - loss: 1.6087 - categorical_accuracy: 0.1822 - val_loss: 1.6077 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00011: saving model to pras_model_init_2020-02-2914_04_03.771783/model-00011-1.60906-0.17949-1.60773-0.21000.h5\n",
      "Epoch 12/20\n",
      "45/45 [==============================] - 88s 2s/step - loss: 1.6081 - categorical_accuracy: 0.2118 - val_loss: 1.6077 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00012: saving model to pras_model_init_2020-02-2914_04_03.771783/model-00012-1.60881-0.20965-1.60769-0.21000.h5\n",
      "Epoch 13/20\n",
      "45/45 [==============================] - 89s 2s/step - loss: 1.6093 - categorical_accuracy: 0.2016 - val_loss: 1.6074 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00013: saving model to pras_model_init_2020-02-2914_04_03.771783/model-00013-1.60926-0.20513-1.60737-0.21000.h5\n",
      "Epoch 14/20\n",
      "45/45 [==============================] - 91s 2s/step - loss: 1.6098 - categorical_accuracy: 0.1808 - val_loss: 1.6072 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00014: saving model to pras_model_init_2020-02-2914_04_03.771783/model-00014-1.60939-0.18401-1.60720-0.21000.h5\n",
      "Epoch 15/20\n",
      "45/45 [==============================] - 88s 2s/step - loss: 1.6093 - categorical_accuracy: 0.1927 - val_loss: 1.6077 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00015: saving model to pras_model_init_2020-02-2914_04_03.771783/model-00015-1.60957-0.19608-1.60765-0.18000.h5\n",
      "Epoch 16/20\n",
      "45/45 [==============================] - 89s 2s/step - loss: 1.6101 - categorical_accuracy: 0.1956 - val_loss: 1.6075 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00016: saving model to pras_model_init_2020-02-2914_04_03.771783/model-00016-1.60951-0.19910-1.60751-0.21000.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 17/20\n",
      " 2/45 [>.............................] - ETA: 1:34 - loss: 1.6108 - categorical_accuracy: 0.3000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-e338b2e291ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m model13.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n\u001b[1;32m      3\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n\u001b[0m",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit or training process\n",
    "model13.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopped the model execution as again, model performance was detoriating. Will remove the l2-regularization and then run the model now. Will also increase epoch to 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_path\n",
    "#folder_list\n",
    "#source_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(8,30)] #Selecting second half of images as the differentiating actions within video would most likely be represented by these images  \n",
    "    #img_idx = [x for x in range(12,30)] # Selecting three more images apart from second 15\n",
    "    #img_idx = [x for x in range(0,30,2)] #selected for second and third model and fifth model\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            #batch_data = np.zeros((batch_size,len(img_idx),100,100,3))# x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),64,64,3))\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    #image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = imgio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    # Cropping the image for second model analysis by 10%\n",
    "                    center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                    width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                    left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                    top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                    image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:] \n",
    "                    #image = resize(image, (100,100,3)) # first model instance and fourth model\n",
    "                    # resizing image to 64x64 for second  and third iteration. Also for model-11\n",
    "                    image = resize(image, (64,64,3))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                #print(batch_labels.shape(0))\n",
    "                #print(image.shape(0))\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        diff = len(folder_list) - (num_batches*batch_size)\n",
    "        last_batch_size = diff\n",
    "        #batch_data = np.zeros((last_batch_size,len(img_idx),100,100,3)) # first model instance and fourth model\n",
    "        batch_data = np.zeros((last_batch_size,len(img_idx),64,64,3)) #second and third model instance, model 11 as well\n",
    "        batch_labels = np.zeros((last_batch_size,5))\n",
    "        for fd in range(diff):\n",
    "            imgs = os.listdir(source_path+'/'+ t[fd + (num_batches*batch_size)].split(';')[0])\n",
    "            for idx,item in enumerate(img_idx):\n",
    "                image = imgio.imread(source_path+'/'+ t[fd + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                # Cropping the image for second model analysis by 10%\n",
    "                center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:]\n",
    "                #image = resize(image, (100,100,3)) #first model instance and fourth model\n",
    "                image = resize(image, (64,64,3)) # second and third model instance\n",
    "                batch_data[fd,idx,:,:,0] = image[:,:,0]/255 \n",
    "                batch_data[fd,idx,:,:,1] = image[:,:,1]/255 \n",
    "                batch_data[fd,idx,:,:,2] = image[:,:,2]/255\n",
    "            batch_labels[fd, int(t[fd + (num_batches*batch_size)].strip().split(';')[2])] = 1    \n",
    "        yield batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 30\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/mnt/disks/user/project/PROJECT/Project_data/train'\n",
    "#train_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "val_path = '/mnt/disks/user/project/PROJECT/Project_data/val'\n",
    "#val_path = 'C://Users/pchadha/Neural_case_study/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 30# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 14\n",
    "model14 = Sequential()\n",
    "#help(Conv3D)\n",
    "# a keras convolutional layer is called Conv3D\n",
    "\n",
    "# note that the first layer needs to be told the input shape explicitly\n",
    "\n",
    "# first conv layer\n",
    "img_rows = 64\n",
    "img_cols = 64\n",
    "imnum = 22 # len(img_idx)\n",
    "num_classes = 5\n",
    "input_shape = (imnum, img_rows, img_cols, 3)\n",
    "model14.add(Conv3D(32, kernel_size=(3, 3, 3),\n",
    "                 activation='relu', \n",
    "                 input_shape=input_shape)) # input shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# second conv layer\n",
    "model14.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model14.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model14.add(Dropout(0.25))\n",
    "\n",
    "# Third conv layer\n",
    "model14.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model14.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model14.add(Dropout(0.30))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model14.add(Flatten())\n",
    "model14.add(Dense(512, activation='relu')) # fully connected\n",
    "model14.add(Dropout(0.5))\n",
    "\n",
    "# softmax layer\n",
    "model14.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model summary\n",
    "#model1.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_7 (Conv3D)            (None, 20, 62, 62, 32)    2624      \n",
      "_________________________________________________________________\n",
      "conv3d_8 (Conv3D)            (None, 18, 60, 60, 64)    55360     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 9, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 9, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_9 (Conv3D)            (None, 7, 28, 28, 64)     110656    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3 (None, 3, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 3, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37632)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               19268096  \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 19,439,301\n",
      "Trainable params: 19,439,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "optimiser = Adam(lr=learning_rate,clipvalue=1.0)\n",
    "model14.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model14.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pras_model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/train Source path = ; batch size =  /mnt/disks/user/project/PROJECT/Project_data/val ; batch size = 15\n",
      "15\n",
      "45/45 [==============================] - 94s 2s/step - loss: 1.8329 - categorical_accuracy: 0.2429 - val_loss: 1.5796 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00001: saving model to pras_model_init_2020-02-2918_40_44.975965/model-00001-1.83897-0.24133-1.57961-0.23000.h5\n",
      "Epoch 2/30\n",
      "45/45 [==============================] - 102s 2s/step - loss: 1.5713 - categorical_accuracy: 0.2459 - val_loss: 1.4474 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00002: saving model to pras_model_init_2020-02-2918_40_44.975965/model-00002-1.57439-0.24434-1.44740-0.39000.h5\n",
      "Epoch 3/30\n",
      "45/45 [==============================] - 79s 2s/step - loss: 1.4510 - categorical_accuracy: 0.3333 - val_loss: 1.3121 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00003: saving model to pras_model_init_2020-02-2918_40_44.975965/model-00003-1.44745-0.33333-1.31212-0.44000.h5\n",
      "Epoch 4/30\n",
      "45/45 [==============================] - 89s 2s/step - loss: 1.3141 - categorical_accuracy: 0.4460 - val_loss: 1.1881 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00004: saving model to pras_model_init_2020-02-2918_40_44.975965/model-00004-1.31324-0.44796-1.18807-0.50000.h5\n",
      "Epoch 5/30\n",
      "45/45 [==============================] - 80s 2s/step - loss: 1.1870 - categorical_accuracy: 0.4875 - val_loss: 1.1608 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00005: saving model to pras_model_init_2020-02-2918_40_44.975965/model-00005-1.17942-0.49020-1.16076-0.53000.h5\n",
      "Epoch 6/30\n",
      "45/45 [==============================] - 96s 2s/step - loss: 1.1960 - categorical_accuracy: 0.5112 - val_loss: 1.1747 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00006: saving model to pras_model_init_2020-02-2918_40_44.975965/model-00006-1.15192-0.51433-1.17469-0.51000.h5\n",
      "Epoch 7/30\n",
      "45/45 [==============================] - 79s 2s/step - loss: 1.0654 - categorical_accuracy: 0.5392 - val_loss: 1.0336 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00007: saving model to pras_model_init_2020-02-2918_40_44.975965/model-00007-1.06901-0.53695-1.03356-0.57000.h5\n",
      "Epoch 8/30\n",
      "45/45 [==============================] - 88s 2s/step - loss: 0.9641 - categorical_accuracy: 0.6148 - val_loss: 1.0414 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00008: saving model to pras_model_init_2020-02-2918_40_44.975965/model-00008-0.96159-0.61388-1.04144-0.60000.h5\n",
      "Epoch 9/30\n",
      "45/45 [==============================] - 101s 2s/step - loss: 0.7769 - categorical_accuracy: 0.6905 - val_loss: 1.0763 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00009: saving model to pras_model_init_2020-02-2918_40_44.975965/model-00009-0.77190-0.69683-1.07635-0.58000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 10/30\n",
      "45/45 [==============================] - 103s 2s/step - loss: 0.5476 - categorical_accuracy: 0.8104 - val_loss: 1.0232 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00010: saving model to pras_model_init_2020-02-2918_40_44.975965/model-00010-0.55009-0.81297-1.02323-0.68000.h5\n",
      "Epoch 11/30\n",
      "45/45 [==============================] - 108s 2s/step - loss: 0.4098 - categorical_accuracy: 0.8401 - val_loss: 1.0196 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00011: saving model to pras_model_init_2020-02-2918_40_44.975965/model-00011-0.40874-0.84314-1.01962-0.64000.h5\n",
      "Epoch 12/30\n",
      "45/45 [==============================] - 109s 2s/step - loss: 0.3269 - categorical_accuracy: 0.8888 - val_loss: 1.0004 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00012: saving model to pras_model_init_2020-02-2918_40_44.975965/model-00012-0.33231-0.88688-1.00044-0.71000.h5\n",
      "Epoch 13/30\n",
      "45/45 [==============================] - 92s 2s/step - loss: 0.2724 - categorical_accuracy: 0.8977 - val_loss: 0.9878 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00013: saving model to pras_model_init_2020-02-2918_40_44.975965/model-00013-0.27230-0.89593-0.98783-0.68000.h5\n",
      "Epoch 14/30\n",
      "45/45 [==============================] - 84s 2s/step - loss: 0.2228 - categorical_accuracy: 0.9244 - val_loss: 0.9737 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00014: saving model to pras_model_init_2020-02-2918_40_44.975965/model-00014-0.22121-0.92308-0.97366-0.74000.h5\n",
      "Epoch 15/30\n",
      "45/45 [==============================] - 83s 2s/step - loss: 0.1828 - categorical_accuracy: 0.9407 - val_loss: 1.0682 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00015: saving model to pras_model_init_2020-02-2918_40_44.975965/model-00015-0.18595-0.93967-1.06816-0.74000.h5\n",
      "Epoch 16/30\n",
      "45/45 [==============================] - 85s 2s/step - loss: 0.1667 - categorical_accuracy: 0.9407 - val_loss: 1.0936 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00016: saving model to pras_model_init_2020-02-2918_40_44.975965/model-00016-0.16758-0.93967-1.09356-0.72000.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 17/30\n",
      "45/45 [==============================] - 99s 2s/step - loss: 0.1076 - categorical_accuracy: 0.9659 - val_loss: 1.1126 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00017: saving model to pras_model_init_2020-02-2918_40_44.975965/model-00017-0.10866-0.96531-1.11264-0.71000.h5\n",
      "Epoch 18/30\n",
      "45/45 [==============================] - 99s 2s/step - loss: 0.1158 - categorical_accuracy: 0.9689 - val_loss: 1.1341 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00018: saving model to pras_model_init_2020-02-2918_40_44.975965/model-00018-0.11596-0.96833-1.13406-0.74000.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 19/30\n",
      "45/45 [==============================] - 108s 2s/step - loss: 0.0975 - categorical_accuracy: 0.9733 - val_loss: 1.1227 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00019: saving model to pras_model_init_2020-02-2918_40_44.975965/model-00019-0.09860-0.97285-1.12272-0.73000.h5\n",
      "Epoch 20/30\n",
      "45/45 [==============================] - 94s 2s/step - loss: 0.1317 - categorical_accuracy: 0.9615 - val_loss: 1.1117 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00020: saving model to pras_model_init_2020-02-2918_40_44.975965/model-00020-0.13214-0.96078-1.11166-0.72000.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 21/30\n",
      "45/45 [==============================] - 110s 2s/step - loss: 0.0995 - categorical_accuracy: 0.9644 - val_loss: 1.1116 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00021: saving model to pras_model_init_2020-02-2918_40_44.975965/model-00021-0.10068-0.96380-1.11162-0.72000.h5\n",
      "Epoch 22/30\n",
      "45/45 [==============================] - 107s 2s/step - loss: 0.1067 - categorical_accuracy: 0.9704 - val_loss: 1.1103 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00022: saving model to pras_model_init_2020-02-2918_40_44.975965/model-00022-0.10852-0.96983-1.11029-0.72000.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 23/30\n",
      "45/45 [==============================] - 92s 2s/step - loss: 0.1043 - categorical_accuracy: 0.9733 - val_loss: 1.1103 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00023: saving model to pras_model_init_2020-02-2918_40_44.975965/model-00023-0.10393-0.97285-1.11032-0.72000.h5\n",
      "Epoch 24/30\n",
      "45/45 [==============================] - 86s 2s/step - loss: 0.0974 - categorical_accuracy: 0.9763 - val_loss: 1.1105 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00024: saving model to pras_model_init_2020-02-2918_40_44.975965/model-00024-0.09852-0.97587-1.11051-0.72000.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'h5py._objects.ObjectID.__dealloc__'\n",
      "Traceback (most recent call last):\n",
      "  File \"h5py/_objects.pyx\", line 193, in h5py._objects.ObjectID.__dealloc__\n",
      "RuntimeError: Problems closing file (file write failed: time = Sat Feb 29 19:19:40 2020\n",
      ", filename = 'pras_model_init_2020-02-2918_30_51.546803/model-00003-1.55190-0.25189-1.37962-0.37000.h5', file descriptor = 69, errno = 28, error message = 'No space left on device', buf = 0x55d334d037a0, total write size = 2048, bytes this sub-write = 2048, bytes actually written = 18446744073709551615, offset = 4096)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unable to flush file's cached information (file write failed: time = Sat Feb 29 19:19:40 2020\n, filename = 'pras_model_init_2020-02-2918_40_44.975965/model-00024-0.09852-0.97587-1.11051-0.72000.h5', file descriptor = 70, errno = 28, error message = 'No space left on device', buf = 0x55d3368af190, total write size = 2048, bytes this sub-write = 2048, bytes actually written = 18446744073709551615, offset = 4096)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0m_serialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36m_serialize_model\u001b[0;34m(model, f, include_optimizer)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mlayer_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minclude_optimizer\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/utils/io_utils.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, attr, val)\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, args, val)\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfspace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdxpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dxpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5d.pyx\u001b[0m in \u001b[0;36mh5py.h5d.DatasetID.write\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_proxy.pyx\u001b[0m in \u001b[0;36mh5py._proxy.dset_rw\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_proxy.pyx\u001b[0m in \u001b[0;36mh5py._proxy.H5PY_H5Dwrite\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't write data (file write failed: time = Sat Feb 29 19:19:40 2020\n, filename = 'pras_model_init_2020-02-2918_40_44.975965/model-00024-0.09852-0.97587-1.11051-0.72000.h5', file descriptor = 70, errno = 28, error message = 'No space left on device', buf = 0x217a92820, total write size = 2807776, bytes this sub-write = 2807776, bytes actually written = 18446744073709551615, offset = 74960896)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-598aeb9e22fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m model14.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n\u001b[1;32m      3\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n\u001b[0m",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    249\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    455\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/utils/io_utils.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \"\"\"\n\u001b[1;32m    432\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m             \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwith_phil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.flush\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to flush file's cached information (file write failed: time = Sat Feb 29 19:19:40 2020\n, filename = 'pras_model_init_2020-02-2918_40_44.975965/model-00024-0.09852-0.97587-1.11051-0.72000.h5', file descriptor = 70, errno = 28, error message = 'No space left on device', buf = 0x55d3368af190, total write size = 2048, bytes this sub-write = 2048, bytes actually written = 18446744073709551615, offset = 4096)"
     ]
    }
   ],
   "source": [
    "# Fit or training process\n",
    "model14.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model stopped due to insufficient memory error. Again, like the best model achieved so far, training score was well above 90 but validation score was little over 70. Will now scale the image size to 48x48 and see if this helps in reduction of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_path\n",
    "#folder_list\n",
    "#source_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(8,30)] #Selecting second half of images as the differentiating actions within video would most likely be represented by these images  \n",
    "    #img_idx = [x for x in range(12,30)] # Selecting three more images apart from second 15\n",
    "    #img_idx = [x for x in range(0,30,2)] #selected for second and third model and fifth model\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            #batch_data = np.zeros((batch_size,len(img_idx),100,100,3))# x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),48,48,3))\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    #image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = imgio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    # Cropping the image for second model analysis by 10%\n",
    "                    center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                    width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                    left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                    top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                    image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:] \n",
    "                    #image = resize(image, (100,100,3)) # first model instance and fourth model\n",
    "                    # resizing image to 64x64 for second  and third iteration. Also for model-11\n",
    "                    image = resize(image, (48,48,3))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                #print(batch_labels.shape(0))\n",
    "                #print(image.shape(0))\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        diff = len(folder_list) - (num_batches*batch_size)\n",
    "        last_batch_size = diff\n",
    "        #batch_data = np.zeros((last_batch_size,len(img_idx),100,100,3)) # first model instance and fourth model\n",
    "        batch_data = np.zeros((last_batch_size,len(img_idx),48,48,3)) #second and third model instance, model 11 as well\n",
    "        batch_labels = np.zeros((last_batch_size,5))\n",
    "        for fd in range(diff):\n",
    "            imgs = os.listdir(source_path+'/'+ t[fd + (num_batches*batch_size)].split(';')[0])\n",
    "            for idx,item in enumerate(img_idx):\n",
    "                image = imgio.imread(source_path+'/'+ t[fd + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                # Cropping the image for second model analysis by 10%\n",
    "                center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:]\n",
    "                #image = resize(image, (100,100,3)) #first model instance and fourth model\n",
    "                image = resize(image, (48,48,3)) # second and third model instance\n",
    "                batch_data[fd,idx,:,:,0] = image[:,:,0]/255 \n",
    "                batch_data[fd,idx,:,:,1] = image[:,:,1]/255 \n",
    "                batch_data[fd,idx,:,:,2] = image[:,:,2]/255\n",
    "            batch_labels[fd, int(t[fd + (num_batches*batch_size)].strip().split(';')[2])] = 1    \n",
    "        yield batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 30\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/mnt/disks/user/project/PROJECT/Project_data/train'\n",
    "#train_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "val_path = '/mnt/disks/user/project/PROJECT/Project_data/val'\n",
    "#val_path = 'C://Users/pchadha/Neural_case_study/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 30# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0229 20:30:26.400179 140647504074560 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0229 20:30:26.403157 140647504074560 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0229 20:30:26.468965 140647504074560 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0229 20:30:26.520292 140647504074560 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0229 20:30:26.528281 140647504074560 deprecation.py:506] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# model 15\n",
    "model15 = Sequential()\n",
    "#help(Conv3D)\n",
    "# a keras convolutional layer is called Conv3D\n",
    "\n",
    "# note that the first layer needs to be told the input shape explicitly\n",
    "\n",
    "# first conv layer\n",
    "img_rows = 48\n",
    "img_cols = 48\n",
    "imnum = 22 # len(img_idx)\n",
    "num_classes = 5\n",
    "input_shape = (imnum, img_rows, img_cols, 3)\n",
    "model15.add(Conv3D(32, kernel_size=(3, 3, 3),\n",
    "                 activation='relu', \n",
    "                 input_shape=input_shape)) # input shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# second conv layer\n",
    "model15.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model15.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model15.add(Dropout(0.30))\n",
    "\n",
    "# Third conv layer\n",
    "model15.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model15.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model15.add(Dropout(0.30))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model15.add(Flatten())\n",
    "model15.add(Dense(512, activation='relu')) # fully connected\n",
    "model15.add(Dropout(0.5))\n",
    "\n",
    "# softmax layer\n",
    "model15.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model summary\n",
    "#model1.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0229 20:31:01.495034 140647504074560 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0229 20:31:01.502064 140647504074560 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 20, 46, 46, 32)    2624      \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 18, 44, 44, 64)    55360     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 9, 22, 22, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 9, 22, 22, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 7, 20, 20, 64)     110656    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 3, 10, 10, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3, 10, 10, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 19200)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               9830912   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 10,002,117\n",
      "Trainable params: 10,002,117\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "optimiser = Adam(lr=learning_rate,clipvalue=1.0)\n",
    "model15.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model15.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pras_model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0229 20:32:34.953036 140647504074560 deprecation.py:323] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0229 20:32:35.369562 140647504074560 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0229 20:32:35.504024 140647504074560 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "W0229 20:32:35.636791 140647504074560 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Source path = Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 15\n",
      " /mnt/disks/user/project/PROJECT/Project_data/val ; batch size = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0229 20:32:40.980192 140647504074560 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0229 20:32:40.987170 140647504074560 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0229 20:32:47.392564 140647504074560 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W0229 20:32:47.393864 140647504074560 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "W0229 20:32:48.923183 140647504074560 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 246s 5s/step - loss: 1.6665 - categorical_accuracy: 0.2090 - val_loss: 1.4464 - val_categorical_accuracy: 0.4100\n",
      "\n",
      "Epoch 00001: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00001-1.66794-0.21267-1.44639-0.41000.h5\n",
      "Epoch 2/30\n",
      "45/45 [==============================] - 96s 2s/step - loss: 1.4302 - categorical_accuracy: 0.3674 - val_loss: 1.3374 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00002: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00002-1.43066-0.36802-1.33737-0.44000.h5\n",
      "Epoch 3/30\n",
      "45/45 [==============================] - 71s 2s/step - loss: 1.2922 - categorical_accuracy: 0.4518 - val_loss: 1.1195 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00003: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00003-1.29248-0.44796-1.11947-0.52000.h5\n",
      "Epoch 4/30\n",
      "45/45 [==============================] - 95s 2s/step - loss: 1.1303 - categorical_accuracy: 0.5036 - val_loss: 0.9662 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00004: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00004-1.13229-0.50075-0.96623-0.61000.h5\n",
      "Epoch 5/30\n",
      "45/45 [==============================] - 82s 2s/step - loss: 0.9776 - categorical_accuracy: 0.5988 - val_loss: 0.7727 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00005: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00005-0.97141-0.60935-0.77266-0.71000.h5\n",
      "Epoch 6/30\n",
      "45/45 [==============================] - 97s 2s/step - loss: 0.7626 - categorical_accuracy: 0.6828 - val_loss: 0.8960 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00006: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00006-0.76826-0.67722-0.89602-0.64000.h5\n",
      "Epoch 7/30\n",
      "45/45 [==============================] - 95s 2s/step - loss: 0.6653 - categorical_accuracy: 0.7317 - val_loss: 0.6741 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00007: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00007-0.67468-0.72700-0.67406-0.75000.h5\n",
      "Epoch 8/30\n",
      "45/45 [==============================] - 98s 2s/step - loss: 0.4549 - categorical_accuracy: 0.8473 - val_loss: 0.7130 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00008: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00008-0.45657-0.84465-0.71301-0.70000.h5\n",
      "Epoch 9/30\n",
      "45/45 [==============================] - 73s 2s/step - loss: 0.3513 - categorical_accuracy: 0.8814 - val_loss: 0.6972 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00009: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00009-0.35294-0.87934-0.69715-0.75000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 10/30\n",
      "45/45 [==============================] - 97s 2s/step - loss: 0.1786 - categorical_accuracy: 0.9392 - val_loss: 0.5719 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00010: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00010-0.18087-0.93816-0.57194-0.76000.h5\n",
      "Epoch 11/30\n",
      "45/45 [==============================] - 70s 2s/step - loss: 0.1688 - categorical_accuracy: 0.9378 - val_loss: 0.6193 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00011: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00011-0.16934-0.93665-0.61934-0.77000.h5\n",
      "Epoch 12/30\n",
      "45/45 [==============================] - 96s 2s/step - loss: 0.1198 - categorical_accuracy: 0.9704 - val_loss: 0.6800 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00012: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00012-0.12180-0.96983-0.68003-0.78000.h5\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 13/30\n",
      "45/45 [==============================] - 68s 2s/step - loss: 0.0999 - categorical_accuracy: 0.9733 - val_loss: 0.6545 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00013: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00013-0.09944-0.97285-0.65450-0.76000.h5\n",
      "Epoch 14/30\n",
      "45/45 [==============================] - 92s 2s/step - loss: 0.0812 - categorical_accuracy: 0.9807 - val_loss: 0.6598 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00014: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00014-0.08112-0.98039-0.65979-0.76000.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 15/30\n",
      "45/45 [==============================] - 72s 2s/step - loss: 0.0847 - categorical_accuracy: 0.9778 - val_loss: 0.6596 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00015: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00015-0.08482-0.97738-0.65965-0.76000.h5\n",
      "Epoch 16/30\n",
      "45/45 [==============================] - 75s 2s/step - loss: 0.0850 - categorical_accuracy: 0.9822 - val_loss: 0.6563 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00016: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00016-0.08335-0.98190-0.65630-0.76000.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 17/30\n",
      "45/45 [==============================] - 82s 2s/step - loss: 0.0904 - categorical_accuracy: 0.9763 - val_loss: 0.6552 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00017: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00017-0.09160-0.97587-0.65523-0.77000.h5\n",
      "Epoch 18/30\n",
      "45/45 [==============================] - 73s 2s/step - loss: 0.0922 - categorical_accuracy: 0.9763 - val_loss: 0.6550 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00018: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00018-0.09365-0.97587-0.65498-0.77000.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 19/30\n",
      "45/45 [==============================] - 96s 2s/step - loss: 0.0889 - categorical_accuracy: 0.9704 - val_loss: 0.6547 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00019: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00019-0.08833-0.96983-0.65473-0.77000.h5\n",
      "Epoch 20/30\n",
      "45/45 [==============================] - 98s 2s/step - loss: 0.0769 - categorical_accuracy: 0.9837 - val_loss: 0.6548 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00020: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00020-0.07781-0.98341-0.65483-0.77000.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "Epoch 21/30\n",
      "45/45 [==============================] - 67s 1s/step - loss: 0.0820 - categorical_accuracy: 0.9733 - val_loss: 0.6548 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00021: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00021-0.08267-0.97285-0.65481-0.77000.h5\n",
      "Epoch 22/30\n",
      "45/45 [==============================] - 69s 2s/step - loss: 0.0774 - categorical_accuracy: 0.9763 - val_loss: 0.6548 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00022: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00022-0.07878-0.97587-0.65482-0.77000.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
      "Epoch 23/30\n",
      "45/45 [==============================] - 73s 2s/step - loss: 0.0749 - categorical_accuracy: 0.9867 - val_loss: 0.6548 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00023: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00023-0.07445-0.98643-0.65483-0.77000.h5\n",
      "Epoch 24/30\n",
      "45/45 [==============================] - 90s 2s/step - loss: 0.0887 - categorical_accuracy: 0.9748 - val_loss: 0.6548 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00024: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00024-0.08858-0.97436-0.65482-0.77000.h5\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
      "Epoch 25/30\n",
      "45/45 [==============================] - 99s 2s/step - loss: 0.0845 - categorical_accuracy: 0.9807 - val_loss: 0.6548 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00025: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00025-0.08514-0.98039-0.65482-0.77000.h5\n",
      "Epoch 26/30\n",
      "45/45 [==============================] - 73s 2s/step - loss: 0.0808 - categorical_accuracy: 0.9822 - val_loss: 0.6548 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00026: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00026-0.08195-0.98190-0.65482-0.77000.h5\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
      "Epoch 27/30\n",
      "45/45 [==============================] - 72s 2s/step - loss: 0.0846 - categorical_accuracy: 0.9733 - val_loss: 0.6548 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00027: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00027-0.08473-0.97285-0.65482-0.77000.h5\n",
      "Epoch 28/30\n",
      "45/45 [==============================] - 78s 2s/step - loss: 0.0848 - categorical_accuracy: 0.9778 - val_loss: 0.6548 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00028: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00028-0.08517-0.97738-0.65482-0.77000.h5\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
      "Epoch 29/30\n",
      "45/45 [==============================] - 71s 2s/step - loss: 0.0783 - categorical_accuracy: 0.9822 - val_loss: 0.6548 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00029: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00029-0.07960-0.98190-0.65482-0.77000.h5\n",
      "Epoch 30/30\n",
      "45/45 [==============================] - 69s 2s/step - loss: 0.0877 - categorical_accuracy: 0.9718 - val_loss: 0.6548 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00030: saving model to pras_model_init_2020-02-2920_27_59.611703/model-00030-0.08563-0.97134-0.65482-0.77000.h5\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea7b6036a0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit or training process\n",
    "model15.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the image scale to 48x48 is giving slightly better performance than the best model so far. Now will keep the image size as 48x48 and will deploy two dense layers of size 256 and 128 neurons respectively, instead of single 512 layer. Will also reduce the number of filters of second conv layer to 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_path\n",
    "#folder_list\n",
    "#source_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(8,30)] #Selecting second half of images as the differentiating actions within video would most likely be represented by these images  \n",
    "    #img_idx = [x for x in range(12,30)] # Selecting three more images apart from second 15\n",
    "    #img_idx = [x for x in range(0,30,2)] #selected for second and third model and fifth model\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            #batch_data = np.zeros((batch_size,len(img_idx),100,100,3))# x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),48,48,3))\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    #image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = imgio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    # Cropping the image for second model analysis by 10%\n",
    "                    center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                    width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                    left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                    top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                    image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:] \n",
    "                    #image = resize(image, (100,100,3)) # first model instance and fourth model\n",
    "                    # resizing image to 64x64 for second  and third iteration. Also for model-11\n",
    "                    image = resize(image, (48,48,3))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                #print(batch_labels.shape(0))\n",
    "                #print(image.shape(0))\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        diff = len(folder_list) - (num_batches*batch_size)\n",
    "        last_batch_size = diff\n",
    "        #batch_data = np.zeros((last_batch_size,len(img_idx),100,100,3)) # first model instance and fourth model\n",
    "        batch_data = np.zeros((last_batch_size,len(img_idx),48,48,3)) #second and third model instance, model 11 as well\n",
    "        batch_labels = np.zeros((last_batch_size,5))\n",
    "        for fd in range(diff):\n",
    "            imgs = os.listdir(source_path+'/'+ t[fd + (num_batches*batch_size)].split(';')[0])\n",
    "            for idx,item in enumerate(img_idx):\n",
    "                image = imgio.imread(source_path+'/'+ t[fd + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                # Cropping the image for second model analysis by 10%\n",
    "                center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:]\n",
    "                #image = resize(image, (100,100,3)) #first model instance and fourth model\n",
    "                image = resize(image, (48,48,3)) # second and third model instance\n",
    "                batch_data[fd,idx,:,:,0] = image[:,:,0]/255 \n",
    "                batch_data[fd,idx,:,:,1] = image[:,:,1]/255 \n",
    "                batch_data[fd,idx,:,:,2] = image[:,:,2]/255\n",
    "            batch_labels[fd, int(t[fd + (num_batches*batch_size)].strip().split(';')[2])] = 1    \n",
    "        yield batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 30\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/mnt/disks/user/project/PROJECT/Project_data/train'\n",
    "#train_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "val_path = '/mnt/disks/user/project/PROJECT/Project_data/val'\n",
    "#val_path = 'C://Users/pchadha/Neural_case_study/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 30# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 16\n",
    "model16 = Sequential()\n",
    "#help(Conv3D)\n",
    "# a keras convolutional layer is called Conv3D\n",
    "\n",
    "# note that the first layer needs to be told the input shape explicitly\n",
    "\n",
    "# first conv layer\n",
    "img_rows = 48\n",
    "img_cols = 48\n",
    "imnum = 22 # len(img_idx)\n",
    "num_classes = 5\n",
    "input_shape = (imnum, img_rows, img_cols, 3)\n",
    "model16.add(Conv3D(32, kernel_size=(3, 3, 3),\n",
    "                 activation='relu', \n",
    "                 input_shape=input_shape)) # input shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# second conv layer\n",
    "model16.add(Conv3D(32, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model16.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model16.add(Dropout(0.30))\n",
    "\n",
    "# Third conv layer\n",
    "model16.add(Conv3D(32, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model16.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model16.add(Dropout(0.30))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model16.add(Flatten())\n",
    "model16.add(Dense(256, activation='relu')) # fully connected\n",
    "model16.add(Dropout(0.5))\n",
    "model16.add(Dense(128, activation='relu')) # fully connected\n",
    "model16.add(Dropout(0.5))\n",
    "\n",
    "# softmax layer\n",
    "model16.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model summary\n",
    "#model1.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_7 (Conv3D)            (None, 20, 46, 46, 32)    2624      \n",
      "_________________________________________________________________\n",
      "conv3d_8 (Conv3D)            (None, 18, 44, 44, 32)    27680     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 9, 22, 22, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 9, 22, 22, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_9 (Conv3D)            (None, 7, 20, 20, 32)     27680     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3 (None, 3, 10, 10, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 3, 10, 10, 32)     0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 9600)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               2457856   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 2,549,381\n",
      "Trainable params: 2,549,381\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "optimiser = Adam(lr=learning_rate,clipvalue=1.0)\n",
    "model16.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model16.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pras_model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/val ; batch size = 15\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 15\n",
      "45/45 [==============================] - 102s 2s/step - loss: 1.6250 - categorical_accuracy: 0.2072 - val_loss: 1.5469 - val_categorical_accuracy: 0.2600\n",
      "\n",
      "Epoch 00001: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00001-1.62908-0.19910-1.54692-0.26000.h5\n",
      "Epoch 2/30\n",
      "45/45 [==============================] - 74s 2s/step - loss: 1.5670 - categorical_accuracy: 0.2621 - val_loss: 1.4515 - val_categorical_accuracy: 0.3600\n",
      "\n",
      "Epoch 00002: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00002-1.56709-0.25490-1.45149-0.36000.h5\n",
      "Epoch 3/30\n",
      "45/45 [==============================] - 96s 2s/step - loss: 1.5124 - categorical_accuracy: 0.3421 - val_loss: 1.3487 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00003: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00003-1.49927-0.33635-1.34875-0.40000.h5\n",
      "Epoch 4/30\n",
      "45/45 [==============================] - 100s 2s/step - loss: 1.4467 - categorical_accuracy: 0.3570 - val_loss: 1.3185 - val_categorical_accuracy: 0.4800\n",
      "\n",
      "Epoch 00004: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00004-1.44058-0.35747-1.31853-0.48000.h5\n",
      "Epoch 5/30\n",
      "45/45 [==============================] - 73s 2s/step - loss: 1.3477 - categorical_accuracy: 0.4120 - val_loss: 1.2312 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00005: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00005-1.34439-0.41931-1.23118-0.40000.h5\n",
      "Epoch 6/30\n",
      "45/45 [==============================] - 71s 2s/step - loss: 1.2951 - categorical_accuracy: 0.3942 - val_loss: 1.2445 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00006: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00006-1.29081-0.40121-1.24454-0.47000.h5\n",
      "Epoch 7/30\n",
      "45/45 [==============================] - 92s 2s/step - loss: 1.2436 - categorical_accuracy: 0.4134 - val_loss: 1.2752 - val_categorical_accuracy: 0.3700\n",
      "\n",
      "Epoch 00007: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00007-1.24537-0.41478-1.27524-0.37000.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 8/30\n",
      "45/45 [==============================] - 66s 1s/step - loss: 1.1420 - categorical_accuracy: 0.4860 - val_loss: 1.1634 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00008: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00008-1.13742-0.48869-1.16342-0.52000.h5\n",
      "Epoch 9/30\n",
      "45/45 [==============================] - 73s 2s/step - loss: 1.0670 - categorical_accuracy: 0.5259 - val_loss: 1.1749 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00009: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00009-1.07080-0.52338-1.17495-0.54000.h5\n",
      "Epoch 10/30\n",
      "45/45 [==============================] - 81s 2s/step - loss: 1.0425 - categorical_accuracy: 0.5438 - val_loss: 1.1424 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00010: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00010-1.03971-0.54751-1.14237-0.50000.h5\n",
      "Epoch 11/30\n",
      "45/45 [==============================] - 84s 2s/step - loss: 1.0304 - categorical_accuracy: 0.5335 - val_loss: 1.0728 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00011: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00011-1.02672-0.54299-1.07276-0.54000.h5\n",
      "Epoch 12/30\n",
      "45/45 [==============================] - 66s 1s/step - loss: 0.9245 - categorical_accuracy: 0.5659 - val_loss: 1.0426 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00012: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00012-0.93189-0.56410-1.04263-0.52000.h5\n",
      "Epoch 13/30\n",
      "45/45 [==============================] - 65s 1s/step - loss: 0.9119 - categorical_accuracy: 0.5942 - val_loss: 1.0588 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00013: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00013-0.91431-0.59879-1.05883-0.52000.h5\n",
      "Epoch 14/30\n",
      "45/45 [==============================] - 65s 1s/step - loss: 0.8774 - categorical_accuracy: 0.6237 - val_loss: 1.0196 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00014: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00014-0.87701-0.62293-1.01963-0.57000.h5\n",
      "Epoch 15/30\n",
      "45/45 [==============================] - 93s 2s/step - loss: 0.8054 - categorical_accuracy: 0.6444 - val_loss: 0.9484 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00015: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00015-0.80889-0.64404-0.94837-0.59000.h5\n",
      "Epoch 16/30\n",
      "45/45 [==============================] - 75s 2s/step - loss: 0.7732 - categorical_accuracy: 0.6681 - val_loss: 0.9587 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00016: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00016-0.77377-0.66817-0.95868-0.61000.h5\n",
      "Epoch 17/30\n",
      "45/45 [==============================] - 64s 1s/step - loss: 0.7332 - categorical_accuracy: 0.6932 - val_loss: 0.9321 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00017: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00017-0.73673-0.68778-0.93209-0.59000.h5\n",
      "Epoch 18/30\n",
      "45/45 [==============================] - 85s 2s/step - loss: 0.6376 - categorical_accuracy: 0.7362 - val_loss: 0.9324 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00018: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00018-0.63960-0.73152-0.93243-0.65000.h5\n",
      "Epoch 19/30\n",
      "45/45 [==============================] - 100s 2s/step - loss: 0.5959 - categorical_accuracy: 0.7691 - val_loss: 0.9375 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00019: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00019-0.59219-0.77677-0.93747-0.60000.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 20/30\n",
      "45/45 [==============================] - 76s 2s/step - loss: 0.5673 - categorical_accuracy: 0.7749 - val_loss: 0.8850 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00020: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00020-0.56560-0.77677-0.88497-0.60000.h5\n",
      "Epoch 21/30\n",
      "45/45 [==============================] - 67s 1s/step - loss: 0.5420 - categorical_accuracy: 0.7999 - val_loss: 0.9065 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00021: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00021-0.54234-0.79638-0.90653-0.63000.h5\n",
      "Epoch 22/30\n",
      "45/45 [==============================] - 77s 2s/step - loss: 0.5110 - categorical_accuracy: 0.7955 - val_loss: 0.9096 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00022: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00022-0.51517-0.79186-0.90958-0.61000.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 23/30\n",
      "45/45 [==============================] - 81s 2s/step - loss: 0.4550 - categorical_accuracy: 0.8430 - val_loss: 0.8934 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00023: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00023-0.45555-0.84615-0.89336-0.63000.h5\n",
      "Epoch 24/30\n",
      "45/45 [==============================] - 81s 2s/step - loss: 0.4917 - categorical_accuracy: 0.8118 - val_loss: 0.9035 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00024: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00024-0.49066-0.80845-0.90345-0.63000.h5\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 25/30\n",
      "45/45 [==============================] - 66s 1s/step - loss: 0.4959 - categorical_accuracy: 0.7971 - val_loss: 0.9082 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00025: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00025-0.49485-0.79940-0.90820-0.63000.h5\n",
      "Epoch 26/30\n",
      "45/45 [==============================] - 100s 2s/step - loss: 0.4956 - categorical_accuracy: 0.8031 - val_loss: 0.9108 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00026: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00026-0.48587-0.81146-0.91078-0.63000.h5\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 27/30\n",
      "45/45 [==============================] - 68s 2s/step - loss: 0.4829 - categorical_accuracy: 0.8267 - val_loss: 0.9106 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00027: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00027-0.47721-0.82956-0.91059-0.63000.h5\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 95s 2s/step - loss: 0.4860 - categorical_accuracy: 0.8044 - val_loss: 0.9103 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00028: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00028-0.48884-0.80090-0.91029-0.63000.h5\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "Epoch 29/30\n",
      "45/45 [==============================] - 65s 1s/step - loss: 0.5049 - categorical_accuracy: 0.7986 - val_loss: 0.9103 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00029: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00029-0.50568-0.80090-0.91030-0.63000.h5\n",
      "Epoch 30/30\n",
      "45/45 [==============================] - 66s 1s/step - loss: 0.4883 - categorical_accuracy: 0.8120 - val_loss: 0.9103 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00030: saving model to pras_model_init_2020-02-2921_27_27.589979/model-00030-0.48240-0.82051-0.91027-0.63000.h5\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea446e2cc0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit or training process\n",
    "model16.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making model simpler has not helped as both training and validation scores have reduced. Will keep the number of filters of second Conv layer to be 64. Will use the early stopping to stop iteration in case no progress on accuracy is being achieved for 5 epochs   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_path\n",
    "#folder_list\n",
    "#source_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(8,30)] #Selecting second half of images as the differentiating actions within video would most likely be represented by these images  \n",
    "    #img_idx = [x for x in range(12,30)] # Selecting three more images apart from second 15\n",
    "    #img_idx = [x for x in range(0,30,2)] #selected for second and third model and fifth model\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            #batch_data = np.zeros((batch_size,len(img_idx),100,100,3))# x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),48,48,3))\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    #image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = imgio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    # Cropping the image for second model analysis by 10%\n",
    "                    center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                    width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                    left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                    top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                    image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:] \n",
    "                    #image = resize(image, (100,100,3)) # first model instance and fourth model\n",
    "                    # resizing image to 64x64 for second  and third iteration. Also for model-11\n",
    "                    image = resize(image, (48,48,3))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                #print(batch_labels.shape(0))\n",
    "                #print(image.shape(0))\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        diff = len(folder_list) - (num_batches*batch_size)\n",
    "        last_batch_size = diff\n",
    "        #batch_data = np.zeros((last_batch_size,len(img_idx),100,100,3)) # first model instance and fourth model\n",
    "        batch_data = np.zeros((last_batch_size,len(img_idx),48,48,3)) #second and third model instance, model 11 as well\n",
    "        batch_labels = np.zeros((last_batch_size,5))\n",
    "        for fd in range(diff):\n",
    "            imgs = os.listdir(source_path+'/'+ t[fd + (num_batches*batch_size)].split(';')[0])\n",
    "            for idx,item in enumerate(img_idx):\n",
    "                image = imgio.imread(source_path+'/'+ t[fd + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                # Cropping the image for second model analysis by 10%\n",
    "                center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:]\n",
    "                #image = resize(image, (100,100,3)) #first model instance and fourth model\n",
    "                image = resize(image, (48,48,3)) # second and third model instance\n",
    "                batch_data[fd,idx,:,:,0] = image[:,:,0]/255 \n",
    "                batch_data[fd,idx,:,:,1] = image[:,:,1]/255 \n",
    "                batch_data[fd,idx,:,:,2] = image[:,:,2]/255\n",
    "            batch_labels[fd, int(t[fd + (num_batches*batch_size)].strip().split(';')[2])] = 1    \n",
    "        yield batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 30\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/mnt/disks/user/project/PROJECT/Project_data/train'\n",
    "#train_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "val_path = '/mnt/disks/user/project/PROJECT/Project_data/val'\n",
    "#val_path = 'C://Users/pchadha/Neural_case_study/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 30# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0301 07:34:02.719241 140649066219328 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0301 07:34:02.722075 140649066219328 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0301 07:34:02.781112 140649066219328 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0301 07:34:02.826275 140649066219328 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0301 07:34:02.834435 140649066219328 deprecation.py:506] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# model 17\n",
    "model17 = Sequential()\n",
    "#help(Conv3D)\n",
    "# a keras convolutional layer is called Conv3D\n",
    "\n",
    "# note that the first layer needs to be told the input shape explicitly\n",
    "\n",
    "# first conv layer\n",
    "img_rows = 48\n",
    "img_cols = 48\n",
    "imnum = 22 # len(img_idx)\n",
    "num_classes = 5\n",
    "input_shape = (imnum, img_rows, img_cols, 3)\n",
    "model17.add(Conv3D(32, kernel_size=(3, 3, 3),\n",
    "                 activation='relu', \n",
    "                 input_shape=input_shape)) # input shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# second conv layer\n",
    "model17.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model17.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model17.add(Dropout(0.30))\n",
    "\n",
    "# Third conv layer\n",
    "model17.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model17.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model17.add(Dropout(0.30))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model17.add(Flatten())\n",
    "model17.add(Dense(256, activation='relu')) # fully connected\n",
    "model17.add(Dropout(0.5))\n",
    "model17.add(Dense(128, activation='relu')) # fully connected\n",
    "model17.add(Dropout(0.5))\n",
    "\n",
    "# softmax layer\n",
    "model17.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model summary\n",
    "#model1.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0301 07:34:22.952633 140649066219328 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0301 07:34:22.960447 140649066219328 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 20, 46, 46, 32)    2624      \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 18, 44, 44, 64)    55360     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 9, 22, 22, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 9, 22, 22, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 7, 20, 20, 64)     110656    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 3, 10, 10, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3, 10, 10, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 19200)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               4915456   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 5,117,637\n",
      "Trainable params: 5,117,637\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "optimiser = Adam(lr=learning_rate,clipvalue=1.0)\n",
    "model17.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model17.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pras_model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "earlystop = EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=1, patience=5)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, LR, earlystop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0301 07:35:25.430218 140649066219328 deprecation.py:323] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0301 07:35:25.786520 140649066219328 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0301 07:35:25.945378 140649066219328 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "W0301 07:35:26.110173 140649066219328 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/val ; batch size = Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 15\n",
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0301 07:35:32.042979 140649066219328 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0301 07:35:32.044333 140649066219328 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0301 07:35:38.854555 140649066219328 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W0301 07:35:38.856267 140649066219328 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "W0301 07:35:40.408146 140649066219328 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 214s 5s/step - loss: 1.6619 - categorical_accuracy: 0.1925 - val_loss: 1.5694 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00001: saving model to pras_model_init_2020-03-0107_33_59.031243/model-00001-1.66443-0.19005-1.56942-0.23000.h5\n",
      "Epoch 2/30\n",
      "45/45 [==============================] - 102s 2s/step - loss: 1.5861 - categorical_accuracy: 0.2059 - val_loss: 1.4492 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00002: saving model to pras_model_init_2020-03-0107_33_59.031243/model-00002-1.58848-0.20362-1.44920-0.24000.h5\n",
      "Epoch 3/30\n",
      "45/45 [==============================] - 105s 2s/step - loss: 1.5221 - categorical_accuracy: 0.2831 - val_loss: 1.3490 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00003: saving model to pras_model_init_2020-03-0107_33_59.031243/model-00003-1.51615-0.28808-1.34899-0.40000.h5\n",
      "Epoch 4/30\n",
      "45/45 [==============================] - 106s 2s/step - loss: 1.4603 - categorical_accuracy: 0.3438 - val_loss: 1.3145 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00004: saving model to pras_model_init_2020-03-0107_33_59.031243/model-00004-1.45504-0.34992-1.31450-0.44000.h5\n",
      "Epoch 5/30\n",
      "45/45 [==============================] - 108s 2s/step - loss: 1.4051 - categorical_accuracy: 0.3809 - val_loss: 1.4524 - val_categorical_accuracy: 0.3700\n",
      "\n",
      "Epoch 00005: saving model to pras_model_init_2020-03-0107_33_59.031243/model-00005-1.39925-0.38763-1.45241-0.37000.h5\n",
      "Epoch 6/30\n",
      "45/45 [==============================] - 143s 3s/step - loss: 1.3692 - categorical_accuracy: 0.3927 - val_loss: 1.2653 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00006: saving model to pras_model_init_2020-03-0107_33_59.031243/model-00006-1.36649-0.39970-1.26534-0.42000.h5\n",
      "Epoch 7/30\n",
      "45/45 [==============================] - 82s 2s/step - loss: 1.3048 - categorical_accuracy: 0.4237 - val_loss: 1.2154 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00007: saving model to pras_model_init_2020-03-0107_33_59.031243/model-00007-1.30162-0.42534-1.21540-0.45000.h5\n",
      "Epoch 8/30\n",
      "45/45 [==============================] - 69s 2s/step - loss: 1.1994 - categorical_accuracy: 0.4756 - val_loss: 1.1827 - val_categorical_accuracy: 0.4300\n",
      "\n",
      "Epoch 00008: saving model to pras_model_init_2020-03-0107_33_59.031243/model-00008-1.19762-0.47813-1.18272-0.43000.h5\n",
      "Epoch 9/30\n",
      "45/45 [==============================] - 83s 2s/step - loss: 1.1187 - categorical_accuracy: 0.5125 - val_loss: 1.1016 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00009: saving model to pras_model_init_2020-03-0107_33_59.031243/model-00009-1.12521-0.50980-1.10161-0.50000.h5\n",
      "Epoch 10/30\n",
      "45/45 [==============================] - 100s 2s/step - loss: 0.9788 - categorical_accuracy: 0.6001 - val_loss: 1.3204 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00010: saving model to pras_model_init_2020-03-0107_33_59.031243/model-00010-0.97278-0.60483-1.32042-0.52000.h5\n",
      "Epoch 11/30\n",
      "45/45 [==============================] - 79s 2s/step - loss: 0.9040 - categorical_accuracy: 0.6342 - val_loss: 1.1033 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00011: saving model to pras_model_init_2020-03-0107_33_59.031243/model-00011-0.90430-0.63952-1.10333-0.59000.h5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 12/30\n",
      "45/45 [==============================] - 91s 2s/step - loss: 0.6493 - categorical_accuracy: 0.7510 - val_loss: 1.1388 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00012: saving model to pras_model_init_2020-03-0107_33_59.031243/model-00012-0.65568-0.74661-1.13877-0.58000.h5\n",
      "Epoch 13/30\n",
      "45/45 [==============================] - 85s 2s/step - loss: 0.6124 - categorical_accuracy: 0.7480 - val_loss: 1.2085 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00013: saving model to pras_model_init_2020-03-0107_33_59.031243/model-00013-0.61745-0.74359-1.20847-0.58000.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 14/30\n",
      "45/45 [==============================] - 99s 2s/step - loss: 0.5167 - categorical_accuracy: 0.7852 - val_loss: 1.1406 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00014: saving model to pras_model_init_2020-03-0107_33_59.031243/model-00014-0.51911-0.78733-1.14059-0.58000.h5\n",
      "Epoch 15/30\n",
      "45/45 [==============================] - 90s 2s/step - loss: 0.4803 - categorical_accuracy: 0.8044 - val_loss: 1.1462 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00015: saving model to pras_model_init_2020-03-0107_33_59.031243/model-00015-0.48097-0.80090-1.14623-0.56000.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 16/30\n",
      "45/45 [==============================] - 71s 2s/step - loss: 0.4839 - categorical_accuracy: 0.8162 - val_loss: 1.1494 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00016: saving model to pras_model_init_2020-03-0107_33_59.031243/model-00016-0.48563-0.81297-1.14939-0.57000.h5\n",
      "Epoch 00016: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fead877c208>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit or training process\n",
    "model17.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model has not improved the validation score. Will start adding more images, starting from one more, and again, see if it makes difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_path\n",
    "#folder_list\n",
    "#source_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(7,30)] #Selecting second half of images as the differentiating actions within video would most likely be represented by these images  \n",
    "    #img_idx = [x for x in range(12,30)] # Selecting three more images apart from second 15\n",
    "    #img_idx = [x for x in range(0,30,2)] #selected for second and third model and fifth model\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            #batch_data = np.zeros((batch_size,len(img_idx),100,100,3))# x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),48,48,3))\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    #image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = imgio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    # Cropping the image for second model analysis by 10%\n",
    "                    center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                    width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                    left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                    top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                    image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:] \n",
    "                    #image = resize(image, (100,100,3)) # first model instance and fourth model\n",
    "                    # resizing image to 64x64 for second  and third iteration. Also for model-11\n",
    "                    image = resize(image, (48,48,3))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                #print(batch_labels.shape(0))\n",
    "                #print(image.shape(0))\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        diff = len(folder_list) - (num_batches*batch_size)\n",
    "        last_batch_size = diff\n",
    "        #batch_data = np.zeros((last_batch_size,len(img_idx),100,100,3)) # first model instance and fourth model\n",
    "        batch_data = np.zeros((last_batch_size,len(img_idx),48,48,3)) #second and third model instance, model 11 as well\n",
    "        batch_labels = np.zeros((last_batch_size,5))\n",
    "        for fd in range(diff):\n",
    "            imgs = os.listdir(source_path+'/'+ t[fd + (num_batches*batch_size)].split(';')[0])\n",
    "            for idx,item in enumerate(img_idx):\n",
    "                image = imgio.imread(source_path+'/'+ t[fd + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                # Cropping the image for second model analysis by 10%\n",
    "                center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:]\n",
    "                #image = resize(image, (100,100,3)) #first model instance and fourth model\n",
    "                image = resize(image, (48,48,3)) # second and third model instance\n",
    "                batch_data[fd,idx,:,:,0] = image[:,:,0]/255 \n",
    "                batch_data[fd,idx,:,:,1] = image[:,:,1]/255 \n",
    "                batch_data[fd,idx,:,:,2] = image[:,:,2]/255\n",
    "            batch_labels[fd, int(t[fd + (num_batches*batch_size)].strip().split(';')[2])] = 1    \n",
    "        yield batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 30\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/mnt/disks/user/project/PROJECT/Project_data/train'\n",
    "#train_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "val_path = '/mnt/disks/user/project/PROJECT/Project_data/val'\n",
    "#val_path = 'C://Users/pchadha/Neural_case_study/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 30# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 18\n",
    "model18 = Sequential()\n",
    "#help(Conv3D)\n",
    "# a keras convolutional layer is called Conv3D\n",
    "\n",
    "# note that the first layer needs to be told the input shape explicitly\n",
    "\n",
    "# first conv layer\n",
    "img_rows = 48\n",
    "img_cols = 48\n",
    "imnum = 23 # len(img_idx)\n",
    "num_classes = 5\n",
    "input_shape = (imnum, img_rows, img_cols, 3)\n",
    "model18.add(Conv3D(32, kernel_size=(3, 3, 3),\n",
    "                 activation='relu', \n",
    "                 input_shape=input_shape)) # input shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# second conv layer\n",
    "model18.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model18.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model18.add(Dropout(0.30))\n",
    "\n",
    "# Third conv layer\n",
    "model18.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model18.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model18.add(Dropout(0.30))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model18.add(Flatten())\n",
    "model18.add(Dense(256, activation='relu')) # fully connected\n",
    "model18.add(Dropout(0.50))\n",
    "model18.add(Dense(256, activation='relu')) # fully connected\n",
    "model18.add(Dropout(0.50))\n",
    "\n",
    "# softmax layer\n",
    "model18.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model summary\n",
    "#model1.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_4 (Conv3D)            (None, 21, 46, 46, 32)    2624      \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 19, 44, 44, 64)    55360     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 9, 22, 22, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 9, 22, 22, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 7, 20, 20, 64)     110656    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 3, 10, 10, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 3, 10, 10, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 19200)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               4915456   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 5,151,173\n",
      "Trainable params: 5,151,173\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "optimiser = Adam(lr=learning_rate,clipvalue=1.0)\n",
    "model18.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model18.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pras_model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "earlystop = EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=1, patience=5)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, LR, earlystop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30Source path =  /mnt/disks/user/project/PROJECT/Project_data/val ; batch size = 15\n",
      "\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 15\n",
      "45/45 [==============================] - 201s 4s/step - loss: 1.6654 - categorical_accuracy: 0.1985 - val_loss: 1.6004 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00001: saving model to pras_model_init_2020-03-0108_53_41.874780/model-00001-1.66735-0.19608-1.60044-0.18000.h5\n",
      "Epoch 2/30\n",
      "45/45 [==============================] - 76s 2s/step - loss: 1.5803 - categorical_accuracy: 0.2429 - val_loss: 1.4525 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00002: saving model to pras_model_init_2020-03-0108_53_41.874780/model-00002-1.58110-0.24133-1.45255-0.31000.h5\n",
      "Epoch 3/30\n",
      "45/45 [==============================] - 83s 2s/step - loss: 1.5354 - categorical_accuracy: 0.2771 - val_loss: 1.5266 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00003: saving model to pras_model_init_2020-03-0108_53_41.874780/model-00003-1.53424-0.28205-1.52657-0.35000.h5\n",
      "Epoch 4/30\n",
      "45/45 [==============================] - 72s 2s/step - loss: 1.4543 - categorical_accuracy: 0.3496 - val_loss: 1.2978 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00004: saving model to pras_model_init_2020-03-0108_53_41.874780/model-00004-1.45299-0.34992-1.29782-0.47000.h5\n",
      "Epoch 5/30\n",
      "45/45 [==============================] - 73s 2s/step - loss: 1.3821 - categorical_accuracy: 0.4016 - val_loss: 1.2271 - val_categorical_accuracy: 0.4800\n",
      "\n",
      "Epoch 00005: saving model to pras_model_init_2020-03-0108_53_41.874780/model-00005-1.37332-0.40875-1.22710-0.48000.h5\n",
      "Epoch 6/30\n",
      "45/45 [==============================] - 72s 2s/step - loss: 1.3311 - categorical_accuracy: 0.4060 - val_loss: 1.2023 - val_categorical_accuracy: 0.4100\n",
      "\n",
      "Epoch 00006: saving model to pras_model_init_2020-03-0108_53_41.874780/model-00006-1.33091-0.40724-1.20226-0.41000.h5\n",
      "Epoch 7/30\n",
      "45/45 [==============================] - 72s 2s/step - loss: 1.3433 - categorical_accuracy: 0.4119 - val_loss: 1.2146 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00007: saving model to pras_model_init_2020-03-0108_53_41.874780/model-00007-1.34514-0.41327-1.21456-0.42000.h5\n",
      "Epoch 8/30\n",
      "45/45 [==============================] - 73s 2s/step - loss: 1.1956 - categorical_accuracy: 0.4504 - val_loss: 1.1497 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00008: saving model to pras_model_init_2020-03-0108_53_41.874780/model-00008-1.19335-0.45249-1.14969-0.44000.h5\n",
      "Epoch 9/30\n",
      "45/45 [==============================] - 75s 2s/step - loss: 1.1027 - categorical_accuracy: 0.5183 - val_loss: 1.0730 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00009: saving model to pras_model_init_2020-03-0108_53_41.874780/model-00009-1.10820-0.50980-1.07297-0.51000.h5\n",
      "Epoch 10/30\n",
      "45/45 [==============================] - 80s 2s/step - loss: 1.0173 - categorical_accuracy: 0.5392 - val_loss: 1.0806 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00010: saving model to pras_model_init_2020-03-0108_53_41.874780/model-00010-1.01526-0.53695-1.08064-0.53000.h5\n",
      "Epoch 11/30\n",
      "45/45 [==============================] - 105s 2s/step - loss: 0.8759 - categorical_accuracy: 0.6237 - val_loss: 1.0653 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00011: saving model to pras_model_init_2020-03-0108_53_41.874780/model-00011-0.87659-0.62293-1.06527-0.51000.h5\n",
      "Epoch 12/30\n",
      "45/45 [==============================] - 90s 2s/step - loss: 0.8147 - categorical_accuracy: 0.6861 - val_loss: 1.1450 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00012: saving model to pras_model_init_2020-03-0108_53_41.874780/model-00012-0.81452-0.69231-1.14499-0.55000.h5\n",
      "Epoch 13/30\n",
      "45/45 [==============================] - 100s 2s/step - loss: 0.7400 - categorical_accuracy: 0.6861 - val_loss: 1.0931 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00013: saving model to pras_model_init_2020-03-0108_53_41.874780/model-00013-0.73469-0.69231-1.09308-0.59000.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 14/30\n",
      "45/45 [==============================] - 76s 2s/step - loss: 0.5760 - categorical_accuracy: 0.7722 - val_loss: 1.0227 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00014: saving model to pras_model_init_2020-03-0108_53_41.874780/model-00014-0.56559-0.78582-1.02274-0.63000.h5\n",
      "Epoch 15/30\n",
      "45/45 [==============================] - 74s 2s/step - loss: 0.4668 - categorical_accuracy: 0.8399 - val_loss: 0.9706 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00015: saving model to pras_model_init_2020-03-0108_53_41.874780/model-00015-0.46618-0.83710-0.97055-0.64000.h5\n",
      "Epoch 16/30\n",
      "45/45 [==============================] - 101s 2s/step - loss: 0.3967 - categorical_accuracy: 0.8533 - val_loss: 1.0511 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00016: saving model to pras_model_init_2020-03-0108_53_41.874780/model-00016-0.40301-0.85068-1.05109-0.71000.h5\n",
      "Epoch 17/30\n",
      "45/45 [==============================] - 105s 2s/step - loss: 0.3589 - categorical_accuracy: 0.8592 - val_loss: 0.9970 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00017: saving model to pras_model_init_2020-03-0108_53_41.874780/model-00017-0.36174-0.85671-0.99697-0.67000.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 18/30\n",
      "45/45 [==============================] - 73s 2s/step - loss: 0.3407 - categorical_accuracy: 0.8874 - val_loss: 1.0060 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00018: saving model to pras_model_init_2020-03-0108_53_41.874780/model-00018-0.34442-0.88537-1.00597-0.68000.h5\n",
      "Epoch 19/30\n",
      "45/45 [==============================] - 74s 2s/step - loss: 0.3102 - categorical_accuracy: 0.8903 - val_loss: 0.9991 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00019: saving model to pras_model_init_2020-03-0108_53_41.874780/model-00019-0.31282-0.88839-0.99908-0.68000.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 20/30\n",
      "45/45 [==============================] - 93s 2s/step - loss: 0.2921 - categorical_accuracy: 0.9007 - val_loss: 1.0038 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00020: saving model to pras_model_init_2020-03-0108_53_41.874780/model-00020-0.29404-0.89894-1.00380-0.69000.h5\n",
      "Epoch 21/30\n",
      "45/45 [==============================] - 95s 2s/step - loss: 0.2955 - categorical_accuracy: 0.8964 - val_loss: 1.0024 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00021: saving model to pras_model_init_2020-03-0108_53_41.874780/model-00021-0.27630-0.90045-1.00235-0.69000.h5\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 00021: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7ae8047128>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit or training process\n",
    "model18.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model has performed better. Will therefore, increase number of image by one more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_path\n",
    "#folder_list\n",
    "#source_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(6,30)] #Selecting second half of images as the differentiating actions within video would most likely be represented by these images  \n",
    "    #img_idx = [x for x in range(12,30)] # Selecting three more images apart from second 15\n",
    "    #img_idx = [x for x in range(0,30,2)] #selected for second and third model and fifth model\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            #batch_data = np.zeros((batch_size,len(img_idx),100,100,3))# x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),48,48,3))\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    #image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = imgio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    # Cropping the image for second model analysis by 10%\n",
    "                    center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                    width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                    left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                    top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                    image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:] \n",
    "                    #image = resize(image, (100,100,3)) # first model instance and fourth model\n",
    "                    # resizing image to 64x64 for second  and third iteration. Also for model-11\n",
    "                    image = resize(image, (48,48,3))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                #print(batch_labels.shape(0))\n",
    "                #print(image.shape(0))\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        diff = len(folder_list) - (num_batches*batch_size)\n",
    "        last_batch_size = diff\n",
    "        #batch_data = np.zeros((last_batch_size,len(img_idx),100,100,3)) # first model instance and fourth model\n",
    "        batch_data = np.zeros((last_batch_size,len(img_idx),48,48,3)) #second and third model instance, model 11 as well\n",
    "        batch_labels = np.zeros((last_batch_size,5))\n",
    "        for fd in range(diff):\n",
    "            imgs = os.listdir(source_path+'/'+ t[fd + (num_batches*batch_size)].split(';')[0])\n",
    "            for idx,item in enumerate(img_idx):\n",
    "                image = imgio.imread(source_path+'/'+ t[fd + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                # Cropping the image for second model analysis by 10%\n",
    "                center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:]\n",
    "                #image = resize(image, (100,100,3)) #first model instance and fourth model\n",
    "                image = resize(image, (48,48,3)) # second and third model instance\n",
    "                batch_data[fd,idx,:,:,0] = image[:,:,0]/255 \n",
    "                batch_data[fd,idx,:,:,1] = image[:,:,1]/255 \n",
    "                batch_data[fd,idx,:,:,2] = image[:,:,2]/255\n",
    "            batch_labels[fd, int(t[fd + (num_batches*batch_size)].strip().split(';')[2])] = 1    \n",
    "        yield batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 30\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/mnt/disks/user/project/PROJECT/Project_data/train'\n",
    "#train_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "val_path = '/mnt/disks/user/project/PROJECT/Project_data/val'\n",
    "#val_path = 'C://Users/pchadha/Neural_case_study/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 30# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 19\n",
    "model19 = Sequential()\n",
    "#help(Conv3D)\n",
    "# a keras convolutional layer is called Conv3D\n",
    "\n",
    "# note that the first layer needs to be told the input shape explicitly\n",
    "\n",
    "# first conv layer\n",
    "img_rows = 48\n",
    "img_cols = 48\n",
    "imnum = 24 # len(img_idx)\n",
    "num_classes = 5\n",
    "input_shape = (imnum, img_rows, img_cols, 3)\n",
    "model19.add(Conv3D(32, kernel_size=(3, 3, 3),\n",
    "                 activation='relu', \n",
    "                 input_shape=input_shape)) # input shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# second conv layer\n",
    "model19.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model19.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model19.add(Dropout(0.30))\n",
    "\n",
    "# Third conv layer\n",
    "model19.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model19.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model19.add(Dropout(0.30))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model19.add(Flatten())\n",
    "model19.add(Dense(256, activation='relu')) # fully connected\n",
    "model19.add(Dropout(0.50))\n",
    "model19.add(Dense(256, activation='relu')) # fully connected\n",
    "model19.add(Dropout(0.50))\n",
    "\n",
    "# softmax layer\n",
    "model19.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model summary\n",
    "#model1.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_7 (Conv3D)            (None, 22, 46, 46, 32)    2624      \n",
      "_________________________________________________________________\n",
      "conv3d_8 (Conv3D)            (None, 20, 44, 44, 64)    55360     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 10, 22, 22, 64)    0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 10, 22, 22, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_9 (Conv3D)            (None, 8, 20, 20, 64)     110656    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3 (None, 4, 10, 10, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 4, 10, 10, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 25600)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               6553856   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 6,789,573\n",
      "Trainable params: 6,789,573\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "optimiser = Adam(lr=learning_rate,clipvalue=1.0)\n",
    "model19.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model19.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pras_model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "earlystop = EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=1, patience=5)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, LR, earlystop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Source path = Source path =  /mnt/disks/user/project/PROJECT/Project_data/val ; batch size = 15\n",
      " /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 15\n",
      "45/45 [==============================] - 135s 3s/step - loss: 1.6753 - categorical_accuracy: 0.1986 - val_loss: 1.6081 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00001: saving model to pras_model_init_2020-03-0109_28_44.914071/model-00001-1.67633-0.20211-1.60809-0.21000.h5\n",
      "Epoch 2/30\n",
      "45/45 [==============================] - 108s 2s/step - loss: 1.5990 - categorical_accuracy: 0.1971 - val_loss: 1.4960 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00002: saving model to pras_model_init_2020-03-0109_28_44.914071/model-00002-1.59881-0.20060-1.49600-0.23000.h5\n",
      "Epoch 3/30\n",
      "45/45 [==============================] - 110s 2s/step - loss: 1.5276 - categorical_accuracy: 0.3213 - val_loss: 1.3868 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00003: saving model to pras_model_init_2020-03-0109_28_44.914071/model-00003-1.53209-0.31523-1.38681-0.42000.h5\n",
      "Epoch 4/30\n",
      "45/45 [==============================] - 75s 2s/step - loss: 1.4282 - categorical_accuracy: 0.3572 - val_loss: 1.3100 - val_categorical_accuracy: 0.3800\n",
      "\n",
      "Epoch 00004: saving model to pras_model_init_2020-03-0109_28_44.914071/model-00004-1.42802-0.36350-1.30999-0.38000.h5\n",
      "Epoch 5/30\n",
      "45/45 [==============================] - 76s 2s/step - loss: 1.3497 - categorical_accuracy: 0.3984 - val_loss: 1.2799 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00005: saving model to pras_model_init_2020-03-0109_28_44.914071/model-00005-1.34908-0.39367-1.27994-0.42000.h5\n",
      "Epoch 6/30\n",
      "45/45 [==============================] - 96s 2s/step - loss: 1.2847 - categorical_accuracy: 0.4297 - val_loss: 1.2393 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00006: saving model to pras_model_init_2020-03-0109_28_44.914071/model-00006-1.28591-0.43137-1.23934-0.51000.h5\n",
      "Epoch 7/30\n",
      "45/45 [==============================] - 75s 2s/step - loss: 1.1314 - categorical_accuracy: 0.4891 - val_loss: 1.2295 - val_categorical_accuracy: 0.4600\n",
      "\n",
      "Epoch 00007: saving model to pras_model_init_2020-03-0109_28_44.914071/model-00007-1.12296-0.49774-1.22949-0.46000.h5\n",
      "Epoch 8/30\n",
      "45/45 [==============================] - 102s 2s/step - loss: 1.0884 - categorical_accuracy: 0.5763 - val_loss: 1.1632 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00008: saving model to pras_model_init_2020-03-0109_28_44.914071/model-00008-1.08537-0.57466-1.16319-0.53000.h5\n",
      "Epoch 9/30\n",
      "45/45 [==============================] - 111s 2s/step - loss: 0.9386 - categorical_accuracy: 0.6163 - val_loss: 1.1050 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00009: saving model to pras_model_init_2020-03-0109_28_44.914071/model-00009-0.93583-0.61538-1.10498-0.54000.h5\n",
      "Epoch 10/30\n",
      "45/45 [==============================] - 77s 2s/step - loss: 0.7800 - categorical_accuracy: 0.7006 - val_loss: 1.0962 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00010: saving model to pras_model_init_2020-03-0109_28_44.914071/model-00010-0.79233-0.69532-1.09622-0.54000.h5\n",
      "Epoch 11/30\n",
      "45/45 [==============================] - 75s 2s/step - loss: 0.6003 - categorical_accuracy: 0.7600 - val_loss: 1.1156 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00011: saving model to pras_model_init_2020-03-0109_28_44.914071/model-00011-0.59404-0.76169-1.11557-0.57000.h5\n",
      "Epoch 12/30\n",
      "45/45 [==============================] - 76s 2s/step - loss: 0.4958 - categorical_accuracy: 0.8177 - val_loss: 1.0611 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00012: saving model to pras_model_init_2020-03-0109_28_44.914071/model-00012-0.49886-0.81448-1.06108-0.57000.h5\n",
      "Epoch 13/30\n",
      "45/45 [==============================] - 83s 2s/step - loss: 0.3660 - categorical_accuracy: 0.8592 - val_loss: 1.2218 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00013: saving model to pras_model_init_2020-03-0109_28_44.914071/model-00013-0.36881-0.85671-1.22178-0.58000.h5\n",
      "Epoch 14/30\n",
      "45/45 [==============================] - 101s 2s/step - loss: 0.2961 - categorical_accuracy: 0.9007 - val_loss: 1.3384 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00014: saving model to pras_model_init_2020-03-0109_28_44.914071/model-00014-0.30126-0.89894-1.33838-0.60000.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 15/30\n",
      "45/45 [==============================] - 84s 2s/step - loss: 0.1793 - categorical_accuracy: 0.9466 - val_loss: 1.2216 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00015: saving model to pras_model_init_2020-03-0109_28_44.914071/model-00015-0.18122-0.94570-1.22156-0.63000.h5\n",
      "Epoch 16/30\n",
      "45/45 [==============================] - 109s 2s/step - loss: 0.1859 - categorical_accuracy: 0.9452 - val_loss: 1.2928 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00016: saving model to pras_model_init_2020-03-0109_28_44.914071/model-00016-0.18727-0.94419-1.29279-0.59000.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 17/30\n",
      "45/45 [==============================] - 108s 2s/step - loss: 0.1330 - categorical_accuracy: 0.9615 - val_loss: 1.2847 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00017: saving model to pras_model_init_2020-03-0109_28_44.914071/model-00017-0.13448-0.96078-1.28470-0.62000.h5\n",
      "Epoch 18/30\n",
      "45/45 [==============================] - 110s 2s/step - loss: 0.1328 - categorical_accuracy: 0.9629 - val_loss: 1.2894 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00018: saving model to pras_model_init_2020-03-0109_28_44.914071/model-00018-0.13508-0.96229-1.28937-0.62000.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 19/30\n",
      "45/45 [==============================] - 109s 2s/step - loss: 0.1201 - categorical_accuracy: 0.9600 - val_loss: 1.2886 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00019: saving model to pras_model_init_2020-03-0109_28_44.914071/model-00019-0.12042-0.95928-1.28855-0.63000.h5\n",
      "Epoch 20/30\n",
      "45/45 [==============================] - 75s 2s/step - loss: 0.1275 - categorical_accuracy: 0.9600 - val_loss: 1.2878 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00020: saving model to pras_model_init_2020-03-0109_28_44.914071/model-00020-0.12947-0.95928-1.28783-0.62000.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 21/30\n",
      "45/45 [==============================] - 101s 2s/step - loss: 0.1448 - categorical_accuracy: 0.9511 - val_loss: 1.2879 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00021: saving model to pras_model_init_2020-03-0109_28_44.914071/model-00021-0.14652-0.95023-1.28790-0.62000.h5\n",
      "Epoch 22/30\n",
      "45/45 [==============================] - 77s 2s/step - loss: 0.1184 - categorical_accuracy: 0.9659 - val_loss: 1.2877 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00022: saving model to pras_model_init_2020-03-0109_28_44.914071/model-00022-0.12024-0.96531-1.28773-0.62000.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 23/30\n",
      "45/45 [==============================] - 75s 2s/step - loss: 0.1201 - categorical_accuracy: 0.9570 - val_loss: 1.2878 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00023: saving model to pras_model_init_2020-03-0109_28_44.914071/model-00023-0.12219-0.95626-1.28776-0.62000.h5\n",
      "Epoch 24/30\n",
      "45/45 [==============================] - 104s 2s/step - loss: 0.1201 - categorical_accuracy: 0.9629 - val_loss: 1.2878 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00024: saving model to pras_model_init_2020-03-0109_28_44.914071/model-00024-0.12033-0.96229-1.28785-0.62000.h5\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "Epoch 00024: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7aa7244550>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit or training process\n",
    "model19.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model performance did not improve. Will now return to best performing model architecture so far and try that with one more image and slightly higher dropout parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_path\n",
    "#folder_list\n",
    "#source_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(7,30)] #Selecting second half of images as the differentiating actions within video would most likely be represented by these images  \n",
    "    #img_idx = [x for x in range(12,30)] # Selecting three more images apart from second 15\n",
    "    #img_idx = [x for x in range(0,30,2)] #selected for second and third model and fifth model\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            #batch_data = np.zeros((batch_size,len(img_idx),100,100,3))# x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),48,48,3))\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    #image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = imgio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    # Cropping the image for second model analysis by 10%\n",
    "                    center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                    width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                    left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                    top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                    image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:] \n",
    "                    #image = resize(image, (100,100,3)) # first model instance and fourth model\n",
    "                    # resizing image to 64x64 for second  and third iteration. Also for model-11\n",
    "                    image = resize(image, (48,48,3))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                #print(batch_labels.shape(0))\n",
    "                #print(image.shape(0))\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        diff = len(folder_list) - (num_batches*batch_size)\n",
    "        last_batch_size = diff\n",
    "        #batch_data = np.zeros((last_batch_size,len(img_idx),100,100,3)) # first model instance and fourth model\n",
    "        batch_data = np.zeros((last_batch_size,len(img_idx),48,48,3)) #second and third model instance, model 11 as well\n",
    "        batch_labels = np.zeros((last_batch_size,5))\n",
    "        for fd in range(diff):\n",
    "            imgs = os.listdir(source_path+'/'+ t[fd + (num_batches*batch_size)].split(';')[0])\n",
    "            for idx,item in enumerate(img_idx):\n",
    "                image = imgio.imread(source_path+'/'+ t[fd + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                # Cropping the image for second model analysis by 10%\n",
    "                center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:]\n",
    "                #image = resize(image, (100,100,3)) #first model instance and fourth model\n",
    "                image = resize(image, (48,48,3)) # second and third model instance\n",
    "                batch_data[fd,idx,:,:,0] = image[:,:,0]/255 \n",
    "                batch_data[fd,idx,:,:,1] = image[:,:,1]/255 \n",
    "                batch_data[fd,idx,:,:,2] = image[:,:,2]/255\n",
    "            batch_labels[fd, int(t[fd + (num_batches*batch_size)].strip().split(';')[2])] = 1    \n",
    "        yield batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 30\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/mnt/disks/user/project/PROJECT/Project_data/train'\n",
    "#train_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "val_path = '/mnt/disks/user/project/PROJECT/Project_data/val'\n",
    "#val_path = 'C://Users/pchadha/Neural_case_study/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 30# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0301 19:39:42.487816 140606473676608 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0301 19:39:42.490810 140606473676608 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0301 19:39:42.614988 140606473676608 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0301 19:39:42.663658 140606473676608 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0301 19:39:42.671030 140606473676608 deprecation.py:506] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# model 20\n",
    "model20 = Sequential()\n",
    "#help(Conv3D)\n",
    "# a keras convolutional layer is called Conv3D\n",
    "\n",
    "# note that the first layer needs to be told the input shape explicitly\n",
    "\n",
    "# first conv layer\n",
    "img_rows = 48\n",
    "img_cols = 48\n",
    "imnum = 23 # len(img_idx)\n",
    "num_classes = 5\n",
    "input_shape = (imnum, img_rows, img_cols, 3)\n",
    "model20.add(Conv3D(32, kernel_size=(3, 3, 3),\n",
    "                 activation='relu', \n",
    "                 input_shape=input_shape)) # input shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# second conv layer\n",
    "model20.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model20.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model20.add(Dropout(0.35))\n",
    "\n",
    "# Third conv layer\n",
    "model20.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model20.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model20.add(Dropout(0.35))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model20.add(Flatten())\n",
    "model20.add(Dense(512, activation='relu')) # fully connected\n",
    "model20.add(Dropout(0.5))\n",
    "\n",
    "# softmax layer\n",
    "model20.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0301 19:40:02.061957 140606473676608 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0301 19:40:02.069444 140606473676608 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 21, 46, 46, 32)    2624      \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 19, 44, 44, 64)    55360     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 9, 22, 22, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 9, 22, 22, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 7, 20, 20, 64)     110656    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 3, 10, 10, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3, 10, 10, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 19200)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               9830912   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 10,002,117\n",
      "Trainable params: 10,002,117\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "optimiser = Adam(lr=learning_rate,clipvalue=1.0)\n",
    "model20.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model20.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pras_model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "earlystop = EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=1, patience=6)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, LR, earlystop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0301 19:40:35.148517 140606473676608 deprecation.py:323] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0301 19:40:35.505953 140606473676608 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0301 19:40:35.641750 140606473676608 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "W0301 19:40:35.774736 140606473676608 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Source path = Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 15\n",
      " /mnt/disks/user/project/PROJECT/Project_data/val ; batch size = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0301 19:40:41.950914 140606473676608 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0301 19:40:41.952640 140606473676608 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0301 19:40:48.486605 140606473676608 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W0301 19:40:48.488322 140606473676608 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "W0301 19:40:50.047668 140606473676608 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 233s 5s/step - loss: 1.6587 - categorical_accuracy: 0.2148 - val_loss: 1.4763 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00001: saving model to pras_model_init_2020-03-0119_39_31.700400/model-00001-1.66438-0.21267-1.47631-0.24000.h5\n",
      "Epoch 2/30\n",
      "45/45 [==============================] - 111s 2s/step - loss: 1.4773 - categorical_accuracy: 0.3157 - val_loss: 1.3562 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00002: saving model to pras_model_init_2020-03-0119_39_31.700400/model-00002-1.47584-0.32127-1.35619-0.39000.h5\n",
      "Epoch 3/30\n",
      "45/45 [==============================] - 108s 2s/step - loss: 1.3078 - categorical_accuracy: 0.4488 - val_loss: 1.1563 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00003: saving model to pras_model_init_2020-03-0119_39_31.700400/model-00003-1.31186-0.44495-1.15628-0.51000.h5\n",
      "Epoch 4/30\n",
      "45/45 [==============================] - 123s 3s/step - loss: 1.1363 - categorical_accuracy: 0.5304 - val_loss: 1.1612 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00004: saving model to pras_model_init_2020-03-0119_39_31.700400/model-00004-1.13888-0.53394-1.16116-0.47000.h5\n",
      "Epoch 5/30\n",
      "45/45 [==============================] - 129s 3s/step - loss: 1.0041 - categorical_accuracy: 0.6060 - val_loss: 1.0061 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00005: saving model to pras_model_init_2020-03-0119_39_31.700400/model-00005-0.99329-0.61086-1.00610-0.57000.h5\n",
      "Epoch 6/30\n",
      "45/45 [==============================] - 74s 2s/step - loss: 0.8235 - categorical_accuracy: 0.6726 - val_loss: 0.9353 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00006: saving model to pras_model_init_2020-03-0119_39_31.700400/model-00006-0.82726-0.67270-0.93527-0.65000.h5\n",
      "Epoch 7/30\n",
      "45/45 [==============================] - 79s 2s/step - loss: 0.6968 - categorical_accuracy: 0.7332 - val_loss: 0.9944 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00007: saving model to pras_model_init_2020-03-0119_39_31.700400/model-00007-0.70691-0.72851-0.99438-0.72000.h5\n",
      "Epoch 8/30\n",
      "45/45 [==============================] - 90s 2s/step - loss: 0.5420 - categorical_accuracy: 0.7882 - val_loss: 0.9836 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00008: saving model to pras_model_init_2020-03-0119_39_31.700400/model-00008-0.53990-0.79035-0.98357-0.71000.h5\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 9/30\n",
      "45/45 [==============================] - 96s 2s/step - loss: 0.3693 - categorical_accuracy: 0.8696 - val_loss: 0.8391 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00009: saving model to pras_model_init_2020-03-0119_39_31.700400/model-00009-0.37144-0.86727-0.83908-0.77000.h5\n",
      "Epoch 10/30\n",
      "45/45 [==============================] - 86s 2s/step - loss: 0.2920 - categorical_accuracy: 0.8802 - val_loss: 0.8685 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00010: saving model to pras_model_init_2020-03-0119_39_31.700400/model-00010-0.28713-0.88989-0.86847-0.79000.h5\n",
      "Epoch 11/30\n",
      "45/45 [==============================] - 79s 2s/step - loss: 0.2554 - categorical_accuracy: 0.9081 - val_loss: 0.8642 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00011: saving model to pras_model_init_2020-03-0119_39_31.700400/model-00011-0.25843-0.90649-0.86423-0.76000.h5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 12/30\n",
      "45/45 [==============================] - 82s 2s/step - loss: 0.2074 - categorical_accuracy: 0.9274 - val_loss: 0.8686 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00012: saving model to pras_model_init_2020-03-0119_39_31.700400/model-00012-0.21067-0.92609-0.86862-0.78000.h5\n",
      "Epoch 13/30\n",
      "45/45 [==============================] - 72s 2s/step - loss: 0.2182 - categorical_accuracy: 0.9185 - val_loss: 0.8833 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00013: saving model to pras_model_init_2020-03-0119_39_31.700400/model-00013-0.22011-0.91704-0.88328-0.78000.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 14/30\n",
      "45/45 [==============================] - 76s 2s/step - loss: 0.2038 - categorical_accuracy: 0.9305 - val_loss: 0.8750 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00014: saving model to pras_model_init_2020-03-0119_39_31.700400/model-00014-0.19822-0.93514-0.87500-0.78000.h5\n",
      "Epoch 15/30\n",
      "45/45 [==============================] - 73s 2s/step - loss: 0.2114 - categorical_accuracy: 0.9303 - val_loss: 0.8689 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00015: saving model to pras_model_init_2020-03-0119_39_31.700400/model-00015-0.21475-0.92911-0.86891-0.78000.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 16/30\n",
      "45/45 [==============================] - 72s 2s/step - loss: 0.1868 - categorical_accuracy: 0.9303 - val_loss: 0.8687 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00016: saving model to pras_model_init_2020-03-0119_39_31.700400/model-00016-0.18947-0.92911-0.86869-0.78000.h5\n",
      "Epoch 00016: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe0edc687b8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit or training process\n",
    "model20.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model performance did improve, infact reached higher the score than the best model so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 21: Increased images by one, i,e, considered images (6-29) and increased dropout value of dense layer to 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_path\n",
    "#folder_list\n",
    "#source_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(6,30)] #Selecting second half of images as the differentiating actions within video would most likely be represented by these images  \n",
    "    #img_idx = [x for x in range(12,30)] # Selecting three more images apart from second 15\n",
    "    #img_idx = [x for x in range(0,30,2)] #selected for second and third model and fifth model\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            #batch_data = np.zeros((batch_size,len(img_idx),100,100,3))# x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),48,48,3))\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    #image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = imgio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    # Cropping the image for second model analysis by 10%\n",
    "                    center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                    width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                    left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                    top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                    image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:] \n",
    "                    #image = resize(image, (100,100,3)) # first model instance and fourth model\n",
    "                    # resizing image to 64x64 for second  and third iteration. Also for model-11\n",
    "                    image = resize(image, (48,48,3))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                #print(batch_labels.shape(0))\n",
    "                #print(image.shape(0))\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        diff = len(folder_list) - (num_batches*batch_size)\n",
    "        last_batch_size = diff\n",
    "        #batch_data = np.zeros((last_batch_size,len(img_idx),100,100,3)) # first model instance and fourth model\n",
    "        batch_data = np.zeros((last_batch_size,len(img_idx),48,48,3)) #second and third model instance, model 11 as well\n",
    "        batch_labels = np.zeros((last_batch_size,5))\n",
    "        for fd in range(diff):\n",
    "            imgs = os.listdir(source_path+'/'+ t[fd + (num_batches*batch_size)].split(';')[0])\n",
    "            for idx,item in enumerate(img_idx):\n",
    "                image = imgio.imread(source_path+'/'+ t[fd + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                # Cropping the image for second model analysis by 10%\n",
    "                center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:]\n",
    "                #image = resize(image, (100,100,3)) #first model instance and fourth model\n",
    "                image = resize(image, (48,48,3)) # second and third model instance\n",
    "                batch_data[fd,idx,:,:,0] = image[:,:,0]/255 \n",
    "                batch_data[fd,idx,:,:,1] = image[:,:,1]/255 \n",
    "                batch_data[fd,idx,:,:,2] = image[:,:,2]/255\n",
    "            batch_labels[fd, int(t[fd + (num_batches*batch_size)].strip().split(';')[2])] = 1    \n",
    "        yield batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 30\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/mnt/disks/user/project/PROJECT/Project_data/train'\n",
    "#train_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "val_path = '/mnt/disks/user/project/PROJECT/Project_data/val'\n",
    "#val_path = 'C://Users/pchadha/Neural_case_study/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 30# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0301 21:15:17.879889 139673403180864 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0301 21:15:17.882623 139673403180864 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0301 21:15:17.956442 139673403180864 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0301 21:15:18.008820 139673403180864 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0301 21:15:18.017389 139673403180864 deprecation.py:506] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0301 21:15:18.125350 139673403180864 nn_ops.py:4283] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "# model 21\n",
    "model21 = Sequential()\n",
    "#help(Conv3D)\n",
    "# a keras convolutional layer is called Conv3D\n",
    "\n",
    "# note that the first layer needs to be told the input shape explicitly\n",
    "\n",
    "# first conv layer\n",
    "img_rows = 48\n",
    "img_cols = 48\n",
    "imnum = 24 # len(img_idx)\n",
    "num_classes = 5\n",
    "input_shape = (imnum, img_rows, img_cols, 3)\n",
    "model21.add(Conv3D(32, kernel_size=(3, 3, 3),\n",
    "                 activation='relu', \n",
    "                 input_shape=input_shape)) # input shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# second conv layer\n",
    "model21.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model21.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model21.add(Dropout(0.35))\n",
    "\n",
    "# Third conv layer\n",
    "model21.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model21.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model21.add(Dropout(0.35))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model21.add(Flatten())\n",
    "model21.add(Dense(512, activation='relu')) # fully connected\n",
    "model21.add(Dropout(0.6))\n",
    "\n",
    "# softmax layer\n",
    "model21.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0301 21:15:23.037102 139673403180864 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0301 21:15:23.044458 139673403180864 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 22, 46, 46, 32)    2624      \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 20, 44, 44, 64)    55360     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 10, 22, 22, 64)    0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 22, 22, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 8, 20, 20, 64)     110656    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 4, 10, 10, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 10, 10, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25600)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               13107712  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 13,278,917\n",
      "Trainable params: 13,278,917\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "optimiser = Adam(lr=learning_rate,clipvalue=1.0)\n",
    "model21.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model21.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pras_model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "earlystop = EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=1, patience=7)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, LR, earlystop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0301 21:15:33.510824 139673403180864 deprecation.py:323] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0301 21:15:33.825703 139673403180864 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0301 21:15:33.958622 139673403180864 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "W0301 21:15:34.082069 139673403180864 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/val ; batch size = 15\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0301 21:15:40.541683 139673403180864 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0301 21:15:40.544059 139673403180864 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0301 21:15:46.978258 139673403180864 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W0301 21:15:46.982310 139673403180864 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "W0301 21:15:48.357179 139673403180864 module_wrapper.py:139] From /mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 236s 5s/step - loss: 1.6720 - categorical_accuracy: 0.2250 - val_loss: 1.4049 - val_categorical_accuracy: 0.2600\n",
      "\n",
      "Epoch 00001: saving model to pras_model_init_2020-03-0121_15_14.502505/model-00001-1.68550-0.21719-1.40486-0.26000.h5\n",
      "Epoch 2/30\n",
      "45/45 [==============================] - 116s 3s/step - loss: 1.4494 - categorical_accuracy: 0.3453 - val_loss: 1.3005 - val_categorical_accuracy: 0.4800\n",
      "\n",
      "Epoch 00002: saving model to pras_model_init_2020-03-0121_15_14.502505/model-00002-1.44624-0.35143-1.30051-0.48000.h5\n",
      "Epoch 3/30\n",
      "45/45 [==============================] - 113s 3s/step - loss: 1.3466 - categorical_accuracy: 0.4043 - val_loss: 1.2005 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00003: saving model to pras_model_init_2020-03-0121_15_14.502505/model-00003-1.35110-0.39970-1.20048-0.47000.h5\n",
      "Epoch 4/30\n",
      "45/45 [==============================] - 116s 3s/step - loss: 1.2104 - categorical_accuracy: 0.4400 - val_loss: 1.1251 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00004: saving model to pras_model_init_2020-03-0121_15_14.502505/model-00004-1.21357-0.44193-1.12512-0.53000.h5\n",
      "Epoch 5/30\n",
      "45/45 [==============================] - 117s 3s/step - loss: 1.0696 - categorical_accuracy: 0.5349 - val_loss: 1.0668 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00005: saving model to pras_model_init_2020-03-0121_15_14.502505/model-00005-1.06379-0.53846-1.06680-0.59000.h5\n",
      "Epoch 6/30\n",
      "45/45 [==============================] - 122s 3s/step - loss: 0.9566 - categorical_accuracy: 0.5881 - val_loss: 0.9499 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00006: saving model to pras_model_init_2020-03-0121_15_14.502505/model-00006-0.96292-0.58673-0.94990-0.61000.h5\n",
      "Epoch 7/30\n",
      "45/45 [==============================] - 137s 3s/step - loss: 0.7408 - categorical_accuracy: 0.6843 - val_loss: 0.8248 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00007: saving model to pras_model_init_2020-03-0121_15_14.502505/model-00007-0.75083-0.67873-0.82484-0.66000.h5\n",
      "Epoch 8/30\n",
      "45/45 [==============================] - 78s 2s/step - loss: 0.6246 - categorical_accuracy: 0.7661 - val_loss: 1.0262 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00008: saving model to pras_model_init_2020-03-0121_15_14.502505/model-00008-0.59974-0.77376-1.02619-0.62000.h5\n",
      "Epoch 9/30\n",
      "45/45 [==============================] - 76s 2s/step - loss: 0.5119 - categorical_accuracy: 0.8073 - val_loss: 0.8085 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00009: saving model to pras_model_init_2020-03-0121_15_14.502505/model-00009-0.51802-0.80392-0.80850-0.64000.h5\n",
      "Epoch 10/30\n",
      "45/45 [==============================] - 108s 2s/step - loss: 0.4015 - categorical_accuracy: 0.8444 - val_loss: 0.9184 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00010: saving model to pras_model_init_2020-03-0121_15_14.502505/model-00010-0.40368-0.84163-0.91836-0.70000.h5\n",
      "Epoch 11/30\n",
      "45/45 [==============================] - 111s 2s/step - loss: 0.2759 - categorical_accuracy: 0.8992 - val_loss: 1.0392 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00011: saving model to pras_model_init_2020-03-0121_15_14.502505/model-00011-0.27958-0.89744-1.03924-0.70000.h5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 12/30\n",
      "45/45 [==============================] - 93s 2s/step - loss: 0.1730 - categorical_accuracy: 0.9392 - val_loss: 0.9090 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00012: saving model to pras_model_init_2020-03-0121_15_14.502505/model-00012-0.17304-0.93816-0.90903-0.73000.h5\n",
      "Epoch 13/30\n",
      "45/45 [==============================] - 75s 2s/step - loss: 0.1495 - categorical_accuracy: 0.9466 - val_loss: 1.0181 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00013: saving model to pras_model_init_2020-03-0121_15_14.502505/model-00013-0.15082-0.94570-1.01811-0.71000.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 14/30\n",
      "45/45 [==============================] - 85s 2s/step - loss: 0.1136 - categorical_accuracy: 0.9644 - val_loss: 0.9940 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00014: saving model to pras_model_init_2020-03-0121_15_14.502505/model-00014-0.11115-0.96380-0.99404-0.71000.h5\n",
      "Epoch 15/30\n",
      "45/45 [==============================] - 110s 2s/step - loss: 0.1194 - categorical_accuracy: 0.9659 - val_loss: 0.9999 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00015: saving model to pras_model_init_2020-03-0121_15_14.502505/model-00015-0.12142-0.96531-0.99995-0.71000.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 16/30\n",
      "45/45 [==============================] - 83s 2s/step - loss: 0.0909 - categorical_accuracy: 0.9748 - val_loss: 1.0002 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00016: saving model to pras_model_init_2020-03-0121_15_14.502505/model-00016-0.09218-0.97436-1.00017-0.71000.h5\n",
      "Epoch 17/30\n",
      "45/45 [==============================] - 104s 2s/step - loss: 0.1011 - categorical_accuracy: 0.9763 - val_loss: 1.0015 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00017: saving model to pras_model_init_2020-03-0121_15_14.502505/model-00017-0.10285-0.97587-1.00154-0.71000.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 18/30\n",
      "45/45 [==============================] - 102s 2s/step - loss: 0.0843 - categorical_accuracy: 0.9763 - val_loss: 1.0023 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00018: saving model to pras_model_init_2020-03-0121_15_14.502505/model-00018-0.08564-0.97587-1.00234-0.71000.h5\n",
      "Epoch 19/30\n",
      "45/45 [==============================] - 104s 2s/step - loss: 0.0804 - categorical_accuracy: 0.9778 - val_loss: 1.0038 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00019: saving model to pras_model_init_2020-03-0121_15_14.502505/model-00019-0.08130-0.97738-1.00376-0.71000.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 00019: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f07ae71a748>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit or training process\n",
    "model21.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model performance did not improve, infact detoriated. Model performance did not improve, it detoriated. Will now try some changes to architecture to make it simpler, keeping the number of images same as the best model-20. Will replace single 512 neuron dense layer with two dense layers, one with 256 neurons while the other with 128. Dropout values considered for these layers will be 0.35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_path\n",
    "#folder_list\n",
    "#source_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(7,30)] #Selecting second half of images as the differentiating actions within video would most likely be represented by these images  \n",
    "    #img_idx = [x for x in range(12,30)] # Selecting three more images apart from second 15\n",
    "    #img_idx = [x for x in range(0,30,2)] #selected for second and third model and fifth model\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            #batch_data = np.zeros((batch_size,len(img_idx),100,100,3))# x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),48,48,3))\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    #image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = imgio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    # Cropping the image for second model analysis by 10%\n",
    "                    center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                    width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                    left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                    top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                    image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:] \n",
    "                    #image = resize(image, (100,100,3)) # first model instance and fourth model\n",
    "                    # resizing image to 64x64 for second  and third iteration. Also for model-11\n",
    "                    image = resize(image, (48,48,3))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                #print(batch_labels.shape(0))\n",
    "                #print(image.shape(0))\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        diff = len(folder_list) - (num_batches*batch_size)\n",
    "        last_batch_size = diff\n",
    "        #batch_data = np.zeros((last_batch_size,len(img_idx),100,100,3)) # first model instance and fourth model\n",
    "        batch_data = np.zeros((last_batch_size,len(img_idx),48,48,3)) #second and third model instance, model 11 as well\n",
    "        batch_labels = np.zeros((last_batch_size,5))\n",
    "        for fd in range(diff):\n",
    "            imgs = os.listdir(source_path+'/'+ t[fd + (num_batches*batch_size)].split(';')[0])\n",
    "            for idx,item in enumerate(img_idx):\n",
    "                image = imgio.imread(source_path+'/'+ t[fd + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                # Cropping the image for second model analysis by 10%\n",
    "                center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:]\n",
    "                #image = resize(image, (100,100,3)) #first model instance and fourth model\n",
    "                image = resize(image, (48,48,3)) # second and third model instance\n",
    "                batch_data[fd,idx,:,:,0] = image[:,:,0]/255 \n",
    "                batch_data[fd,idx,:,:,1] = image[:,:,1]/255 \n",
    "                batch_data[fd,idx,:,:,2] = image[:,:,2]/255\n",
    "            batch_labels[fd, int(t[fd + (num_batches*batch_size)].strip().split(';')[2])] = 1    \n",
    "        yield batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 30\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/mnt/disks/user/project/PROJECT/Project_data/train'\n",
    "#train_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "val_path = '/mnt/disks/user/project/PROJECT/Project_data/val'\n",
    "#val_path = 'C://Users/pchadha/Neural_case_study/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 30# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 22\n",
    "model22 = Sequential()\n",
    "#help(Conv3D)\n",
    "# a keras convolutional layer is called Conv3D\n",
    "\n",
    "# note that the first layer needs to be told the input shape explicitly\n",
    "\n",
    "# first conv layer\n",
    "img_rows = 48\n",
    "img_cols = 48\n",
    "imnum = 23 # len(img_idx)\n",
    "num_classes = 5\n",
    "input_shape = (imnum, img_rows, img_cols, 3)\n",
    "model22.add(Conv3D(32, kernel_size=(3, 3, 3),\n",
    "                 activation='relu', \n",
    "                 input_shape=input_shape)) # input shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# second conv layer\n",
    "model22.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model22.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model22.add(Dropout(0.35))\n",
    "\n",
    "# Third conv layer\n",
    "model22.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model22.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model22.add(Dropout(0.35))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model22.add(Flatten())\n",
    "model22.add(Dense(256, activation='relu')) # fully connected\n",
    "model22.add(Dropout(0.35))\n",
    "model22.add(Dense(128, activation='relu')) # fully connected\n",
    "model22.add(Dropout(0.35))\n",
    "\n",
    "# softmax layer\n",
    "model22.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_4 (Conv3D)            (None, 21, 46, 46, 32)    2624      \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 19, 44, 44, 64)    55360     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 9, 22, 22, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 9, 22, 22, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 7, 20, 20, 64)     110656    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 3, 10, 10, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 3, 10, 10, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 19200)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               4915456   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 5,117,637\n",
      "Trainable params: 5,117,637\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "optimiser = Adam(lr=learning_rate,clipvalue=1.0)\n",
    "model22.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model22.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pras_model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "#earlystop = EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=1, patience=9)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "#callbacks_list = [checkpoint, LR, earlystop]\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/val ; batch size = 15\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size = 15\n",
      "45/45 [==============================] - 96s 2s/step - loss: 1.6508 - categorical_accuracy: 0.1998 - val_loss: 1.5698 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00001: saving model to pras_model_init_2020-03-0214_28_53.600044/model-00001-1.65390-0.19155-1.56976-0.23000.h5\n",
      "Epoch 2/30\n",
      "45/45 [==============================] - 70s 2s/step - loss: 1.5972 - categorical_accuracy: 0.2281 - val_loss: 1.5851 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00002: saving model to pras_model_init_2020-03-0214_28_53.600044/model-00002-1.59728-0.22624-1.58507-0.27000.h5\n",
      "Epoch 3/30\n",
      "45/45 [==============================] - 82s 2s/step - loss: 1.5249 - categorical_accuracy: 0.3127 - val_loss: 1.4581 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00003: saving model to pras_model_init_2020-03-0214_28_53.600044/model-00003-1.52203-0.31825-1.45810-0.44000.h5\n",
      "Epoch 4/30\n",
      "45/45 [==============================] - 104s 2s/step - loss: 1.4027 - categorical_accuracy: 0.3379 - val_loss: 1.2558 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00004: saving model to pras_model_init_2020-03-0214_28_53.600044/model-00004-1.39428-0.34389-1.25584-0.44000.h5\n",
      "Epoch 5/30\n",
      "45/45 [==============================] - 73s 2s/step - loss: 1.2961 - categorical_accuracy: 0.4002 - val_loss: 1.2334 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00005: saving model to pras_model_init_2020-03-0214_28_53.600044/model-00005-1.28557-0.40724-1.23341-0.44000.h5\n",
      "Epoch 6/30\n",
      "45/45 [==============================] - 75s 2s/step - loss: 1.1765 - categorical_accuracy: 0.4828 - val_loss: 1.1129 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00006: saving model to pras_model_init_2020-03-0214_28_53.600044/model-00006-1.18367-0.47360-1.11289-0.57000.h5\n",
      "Epoch 7/30\n",
      "45/45 [==============================] - 107s 2s/step - loss: 1.1666 - categorical_accuracy: 0.4933 - val_loss: 1.1133 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00007: saving model to pras_model_init_2020-03-0214_28_53.600044/model-00007-1.17003-0.49020-1.11328-0.55000.h5\n",
      "Epoch 8/30\n",
      "45/45 [==============================] - 92s 2s/step - loss: 1.0222 - categorical_accuracy: 0.5454 - val_loss: 1.0794 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00008: saving model to pras_model_init_2020-03-0214_28_53.600044/model-00008-1.01030-0.55505-1.07943-0.55000.h5\n",
      "Epoch 9/30\n",
      "45/45 [==============================] - 95s 2s/step - loss: 0.9130 - categorical_accuracy: 0.6252 - val_loss: 0.9445 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00009: saving model to pras_model_init_2020-03-0214_28_53.600044/model-00009-0.91888-0.62443-0.94449-0.61000.h5\n",
      "Epoch 10/30\n",
      "45/45 [==============================] - 73s 2s/step - loss: 0.7571 - categorical_accuracy: 0.6830 - val_loss: 0.9899 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00010: saving model to pras_model_init_2020-03-0214_28_53.600044/model-00010-0.74864-0.68326-0.98990-0.58000.h5\n",
      "Epoch 11/30\n",
      "45/45 [==============================] - 94s 2s/step - loss: 0.6592 - categorical_accuracy: 0.7156 - val_loss: 0.9636 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00011: saving model to pras_model_init_2020-03-0214_28_53.600044/model-00011-0.65726-0.71644-0.96361-0.69000.h5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 12/30\n",
      "45/45 [==============================] - 92s 2s/step - loss: 0.4608 - categorical_accuracy: 0.8310 - val_loss: 0.9332 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00012: saving model to pras_model_init_2020-03-0214_28_53.600044/model-00012-0.46826-0.82805-0.93322-0.71000.h5\n",
      "Epoch 13/30\n",
      "45/45 [==============================] - 85s 2s/step - loss: 0.3945 - categorical_accuracy: 0.8429 - val_loss: 0.9895 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00013: saving model to pras_model_init_2020-03-0214_28_53.600044/model-00013-0.39872-0.84012-0.98953-0.72000.h5\n",
      "Epoch 14/30\n",
      "45/45 [==============================] - 78s 2s/step - loss: 0.3362 - categorical_accuracy: 0.8592 - val_loss: 0.9959 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00014: saving model to pras_model_init_2020-03-0214_28_53.600044/model-00014-0.33925-0.85671-0.99588-0.72000.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 15/30\n",
      "45/45 [==============================] - 105s 2s/step - loss: 0.3118 - categorical_accuracy: 0.8814 - val_loss: 0.9668 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00015: saving model to pras_model_init_2020-03-0214_28_53.600044/model-00015-0.31607-0.87934-0.96676-0.69000.h5\n",
      "Epoch 16/30\n",
      "45/45 [==============================] - 73s 2s/step - loss: 0.3023 - categorical_accuracy: 0.8948 - val_loss: 0.9466 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00016: saving model to pras_model_init_2020-03-0214_28_53.600044/model-00016-0.30024-0.89291-0.94658-0.69000.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 17/30\n",
      "45/45 [==============================] - 71s 2s/step - loss: 0.2878 - categorical_accuracy: 0.9051 - val_loss: 0.9454 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00017: saving model to pras_model_init_2020-03-0214_28_53.600044/model-00017-0.28752-0.90347-0.94537-0.70000.h5\n",
      "Epoch 18/30\n",
      "45/45 [==============================] - 75s 2s/step - loss: 0.2646 - categorical_accuracy: 0.9126 - val_loss: 0.9470 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00018: saving model to pras_model_init_2020-03-0214_28_53.600044/model-00018-0.26194-0.91101-0.94705-0.69000.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 19/30\n",
      "45/45 [==============================] - 107s 2s/step - loss: 0.2970 - categorical_accuracy: 0.8847 - val_loss: 0.9474 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00019: saving model to pras_model_init_2020-03-0214_28_53.600044/model-00019-0.28454-0.89442-0.94742-0.69000.h5\n",
      "Epoch 20/30\n",
      "45/45 [==============================] - 108s 2s/step - loss: 0.2623 - categorical_accuracy: 0.9022 - val_loss: 0.9483 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00020: saving model to pras_model_init_2020-03-0214_28_53.600044/model-00020-0.26495-0.90045-0.94833-0.69000.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 21/30\n",
      "45/45 [==============================] - 104s 2s/step - loss: 0.2612 - categorical_accuracy: 0.9081 - val_loss: 0.9485 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00021: saving model to pras_model_init_2020-03-0214_28_53.600044/model-00021-0.26301-0.90649-0.94847-0.69000.h5\n",
      "Epoch 22/30\n",
      "45/45 [==============================] - 75s 2s/step - loss: 0.2793 - categorical_accuracy: 0.8918 - val_loss: 0.9486 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00022: saving model to pras_model_init_2020-03-0214_28_53.600044/model-00022-0.28143-0.88989-0.94857-0.69000.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "Epoch 23/30\n",
      "45/45 [==============================] - 82s 2s/step - loss: 0.2699 - categorical_accuracy: 0.9022 - val_loss: 0.9486 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00023: saving model to pras_model_init_2020-03-0214_28_53.600044/model-00023-0.27127-0.90045-0.94859-0.69000.h5\n",
      "Epoch 24/30\n",
      "45/45 [==============================] - 105s 2s/step - loss: 0.2637 - categorical_accuracy: 0.9111 - val_loss: 0.9486 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00024: saving model to pras_model_init_2020-03-0214_28_53.600044/model-00024-0.25850-0.90950-0.94860-0.69000.h5\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
      "Epoch 25/30\n",
      "45/45 [==============================] - 74s 2s/step - loss: 0.2968 - categorical_accuracy: 0.8891 - val_loss: 0.9486 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00025: saving model to pras_model_init_2020-03-0214_28_53.600044/model-00025-0.28439-0.89894-0.94860-0.69000.h5\n",
      "Epoch 26/30\n",
      "14/45 [========>.....................] - ETA: 1:21 - loss: 0.2673 - categorical_accuracy: 0.9000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-d884d5ccfe88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m model22.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n\u001b[1;32m      3\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n\u001b[0m",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit or training process\n",
    "model22.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopping the iteration as model not giving better results than the best model. Will now consider the model-20 as the best CNN3D model. Will then compare the modeling done by my partner using CNN+RNN and then select the best model accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN + RNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "#from scipy.misc import imread, imresize\n",
    "import imageio as imgio\n",
    "from skimage.transform import resize\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "#We set the random seed so that the results don't vary drastically.\n",
    "\n",
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(30)\n",
    "#tf.random.set_seed(30)\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Dropout, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM,GRU\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.optimizers import Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gesture Recognition\n",
    "#In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started.\n",
    "\n",
    "\n",
    "#In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error.\n",
    "\n",
    "os.getcwd()\n",
    "\n",
    "train_doc = np.random.permutation(open('/mnt/disks/user/project/PROJECT/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('/mnt/disks/user/project/PROJECT/Project_data/val.csv').readlines())\n",
    "\n",
    "batch_size = 15\n",
    "\n",
    "## Generator\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29] #Selecting second half of images as the differentiating actions within video would most likely be represented by these images  \n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),100,100,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    #image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = imgio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = resize(image, (100,100,3))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                #print(batch_labels.shape(0))\n",
    "                #print(image.shape(0))\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        diff = len(folder_list) - (num_batches*batch_size)\n",
    "        last_batch_size = diff\n",
    "        batch_data = np.zeros((last_batch_size,len(img_idx),100,100,3))\n",
    "        batch_labels = np.zeros((last_batch_size,5))\n",
    "        for fd in range(diff):\n",
    "            imgs = os.listdir(source_path+'/'+ t[fd + (num_batches*batch_size)].split(';')[0])\n",
    "            for idx,item in enumerate(img_idx):\n",
    "                image = imgio.imread(source_path+'/'+ t[fd + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                image = resize(image, (100,100,3))\n",
    "                batch_data[fd,idx,:,:,0] = image[:,:,0]/255 \n",
    "                batch_data[fd,idx,:,:,1] = image[:,:,1]/255 \n",
    "                batch_data[fd,idx,:,:,2] = image[:,:,2]/255\n",
    "            batch_labels[fd, int(t[fd + (num_batches*batch_size)].strip().split(';')[2])] = 1    \n",
    "        yield batch_data, batch_labels\n",
    "\n",
    "\n",
    "\n",
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/mnt/disks/user/project/PROJECT/Project_data/train'\n",
    "#train_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "val_path = '/mnt/disks/user/project/PROJECT/Project_data/val'\n",
    "#val_path = 'C://Users/pchadha/Neural_case_study/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 40# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)\n",
    "\n",
    "## Model\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv2D(32, (3, 3), \n",
    "                                 #strides=(1, 1), \n",
    "                                 activation='relu', \n",
    "                                 #padding='valid'\n",
    "                                ), input_shape=(15,100, 100, 3)))\n",
    "model.add(TimeDistributed(Conv2D(32, (3,3), \n",
    "                                  #strides=(1, 1),\n",
    "                                   #padding='valid',\n",
    "                                  activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), \n",
    "                                       #strides=(1, 1)\n",
    "                                      )))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(64, (3,3), \n",
    "                                  #padding='valid', \n",
    "                                   activation='relu')))\n",
    "model.add(TimeDistributed(Conv2D(64, (3,3), \n",
    "                                  padding='valid', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2), \n",
    "                                      #strides=(1, 1)\n",
    "                                      )))\n",
    "\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(GRU(64, return_sequences=False, dropout=0.5))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "#Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train.\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimiser = Adam(lr=learning_rate,clipvalue=1.0)\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())\n",
    "\n",
    "#Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`.\n",
    "\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "\n",
    "\n",
    "model_name = 'cnn_rnn_model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "model_name = 'cnn_3d_model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "#The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make.\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "\n",
    "#Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch.\n",
    "\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 Summary\n",
    "First iteration:\n",
    "Image size:100,100,3\n",
    "Image_index=img_idx = [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
    "No of Images considered per sequence:15\n",
    "Batch_size=15\n",
    "epochs=40\n",
    "model layout:\n",
    "2 conovlution layers of 32 kernel of size (3,3)and relu activaiton,\n",
    "maxpooling layer (2,2),2 comvolution layers with 64 kernels of size (3,3) and relu activaiton,\n",
    "Flatten layer, GRU layer with 64 cells and drop out of 0.5\n",
    "FInal Dense layer with 5 neurons and Softmax output\n",
    "No of Parameters:6,025,765\n",
    "No of Parameters:\n",
    "Learning rate=0.001\n",
    "optimiser=Adam\n",
    "Max train accuracy:99\n",
    "Max validation accuracy:75\n",
    "\n",
    "Considered the Image size as (100,100) and last 15 images of video. We achieved maximum train accuracy of 99 and validation accuracy of 75 with around 6 million parameters. In the Next iteration we will change the image size to 48 to reduce the number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 15\n",
    "\n",
    "## Generator\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [9,10,11,12,13,14,15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29] #Selecting second half of images as the differentiating actions within video would most likely be represented by these images  \n",
    "      \n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),48,48,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    #image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = imgio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = resize(image, (48,48,3))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                #print(batch_labels.shape(0))\n",
    "                #print(image.shape(0))\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        diff = len(folder_list) - (num_batches*batch_size)\n",
    "        last_batch_size = diff\n",
    "        batch_data = np.zeros((last_batch_size,len(img_idx),48,48,3))\n",
    "        batch_labels = np.zeros((last_batch_size,5))\n",
    "        for fd in range(diff):\n",
    "            imgs = os.listdir(source_path+'/'+ t[fd + (num_batches*batch_size)].split(';')[0])\n",
    "            for idx,item in enumerate(img_idx):\n",
    "                image = imgio.imread(source_path+'/'+ t[fd + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                image = resize(image, (48,48,3))\n",
    "                batch_data[fd,idx,:,:,0] = image[:,:,0]/255 \n",
    "                batch_data[fd,idx,:,:,1] = image[:,:,1]/255 \n",
    "                batch_data[fd,idx,:,:,2] = image[:,:,2]/255\n",
    "            batch_labels[fd, int(t[fd + (num_batches*batch_size)].strip().split(';')[2])] = 1    \n",
    "        yield batch_data, batch_labels\n",
    "\n",
    "\n",
    "img_idx = [9,10,11,12,13,14,15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
    "\n",
    "len(img_idx)\n",
    "\n",
    "\n",
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/mnt/disks/user/project/PROJECT/Project_data/train'\n",
    "#train_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "val_path = '/mnt/disks/user/project/PROJECT/Project_data/val'\n",
    "#val_path = 'C://Users/pchadha/Neural_case_study/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 40# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)\n",
    "\n",
    "## Model\n",
    "# to the above model we are adding a additional 2 convolution layers with 64 kernel of 3,3 and max poolling layer \n",
    "model1 = Sequential()\n",
    "model1.add(TimeDistributed(Conv2D(32, (3, 3), \n",
    "                                 #strides=(1, 1), \n",
    "                                 activation='relu', \n",
    "                                 #padding='valid'\n",
    "                                ), input_shape=(21,48,48,3)))\n",
    "model1.add(TimeDistributed(Conv2D(32, (3,3), \n",
    "                                  #strides=(1, 1),\n",
    "                                   #padding='valid',\n",
    "                                  activation='relu')))\n",
    "model1.add(TimeDistributed(MaxPooling2D((2, 2), \n",
    "                                       #strides=(1, 1)\n",
    "                                      )))\n",
    "\n",
    "\n",
    "model1.add(TimeDistributed(Conv2D(64, (3,3), \n",
    "                                 padding='valid', \n",
    "                                 activation='relu')))\n",
    "model1.add(TimeDistributed(Conv2D(64, (3,3), \n",
    "                                 padding='valid', \n",
    "                                 activation='relu')))\n",
    "model1.add(TimeDistributed(MaxPooling2D((2, 2), \n",
    "                                       #strides=(1, 1)\n",
    "                                      )))\n",
    "\n",
    "model1.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(GRU(128, return_sequences=False, dropout=0.5))\n",
    "model1.add(Dense(5, activation='softmax'))\n",
    "model1.summary()\n",
    "\n",
    "#Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train.\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimiser = Adam(lr=learning_rate,clipvalue=1.0)\n",
    "model1.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model1.summary())\n",
    "\n",
    "#Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`.\n",
    "\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'cnn_rnn_model_iter2_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "#The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make.\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "\n",
    "#Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch.\n",
    "\n",
    "model1.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=40, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 Summary\n",
    "In this iteration we are decreasing the image size to reduce the final number of parameters.\n",
    "Image size:48,48,3\n",
    "img_idx = [9,10,11,12,13,14,15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29] # considere more images per video then previous model\n",
    "No of Images considered per sequence:21\n",
    "Batch_size=15\n",
    "epochs=40 \n",
    "model layout:\n",
    "2 conovlution layers of 32 kernel of size (3,3)and relu activaiton,\n",
    "maxpooling layer (2,2),\n",
    "2 comvolution layers with 64 kernels of size (3,3) and relu activaiton,\n",
    "Flatten layer, GRU layer with 64 cells and drop out of 0.5\n",
    "FInal Dense layer with 5 neurons and Softmax output\n",
    "No of Parameters:2,106,405\n",
    "Learning rate=0.001\n",
    "optimiser=Adam\n",
    "train accuracy:90\n",
    "validation accuracy:69\n",
    "\n",
    "In this model there is no significant increase in the validation accuracy even though we have considered more number of images.\n",
    "In the next model we will change the structure of the network to include more layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 15\n",
    "\n",
    "\n",
    "\n",
    "## Generator\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,29] #selecting every alternate frame  \n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),64,64,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    #image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = imgio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = resize(image, (64,64,3))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                #print(batch_labels.shape(0))\n",
    "                #print(image.shape(0))\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        diff = len(folder_list) - (num_batches*batch_size)\n",
    "        last_batch_size = diff\n",
    "        batch_data = np.zeros((last_batch_size,len(img_idx),64,64,3))\n",
    "        batch_labels = np.zeros((last_batch_size,5))\n",
    "        for fd in range(diff):\n",
    "            imgs = os.listdir(source_path+'/'+ t[fd + (num_batches*batch_size)].split(';')[0])\n",
    "            for idx,item in enumerate(img_idx):\n",
    "                image = imgio.imread(source_path+'/'+ t[fd + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                image = resize(image, (64,64,3))\n",
    "                batch_data[fd,idx,:,:,0] = image[:,:,0]/255 \n",
    "                batch_data[fd,idx,:,:,1] = image[:,:,1]/255 \n",
    "                batch_data[fd,idx,:,:,2] = image[:,:,2]/255\n",
    "            batch_labels[fd, int(t[fd + (num_batches*batch_size)].strip().split(';')[2])] = 1    \n",
    "        yield batch_data, batch_labels\n",
    "\n",
    "\n",
    "img_idx = [0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,29] \n",
    "\n",
    "len(img_idx)\n",
    "\n",
    "\n",
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/mnt/disks/user/project/PROJECT/Project_data/train'\n",
    "#train_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "val_path = '/mnt/disks/user/project/PROJECT/Project_data/val'\n",
    "#val_path = 'C://Users/pchadha/Neural_case_study/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 20# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)\n",
    "\n",
    "## Model\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(TimeDistributed(Conv2D(32, (3, 3), \n",
    "                                 #strides=(1, 1), \n",
    "                                 activation='relu', \n",
    "                                 #padding='valid'\n",
    "                                ), input_shape=(16,64,64,3)))\n",
    "model2.add(TimeDistributed(Conv2D(32, (3,3), \n",
    "                                  #strides=(1, 1),\n",
    "                                   #padding='valid',\n",
    "                                  activation='relu')))\n",
    "model2.add(TimeDistributed(MaxPooling2D((2, 2), \n",
    "                                       #strides=(1, 1)\n",
    "                                      )))\n",
    "\n",
    "model2.add(TimeDistributed(Conv2D(64, (3,3), \n",
    "                                 padding='valid', \n",
    "                                 activation='relu')))\n",
    "model2.add(TimeDistributed(Conv2D(64, (3,3), \n",
    "                                 padding='valid', \n",
    "                                 activation='relu')))\n",
    "model2.add(TimeDistributed(MaxPooling2D((2, 2), \n",
    "                                       #strides=(1, 1)\n",
    "                                      )))\n",
    "model2.add(TimeDistributed(Conv2D(128, (3,3), \n",
    "                                 padding='valid', \n",
    "                                 activation='relu')))\n",
    "model2.add(TimeDistributed(Conv2D(128, (3,3), \n",
    "                                 padding='valid', \n",
    "                                 activation='relu')))\n",
    "model2.add(TimeDistributed(MaxPooling2D((2, 2), \n",
    "                                       strides=(1, 1)\n",
    "                                      )))\n",
    "\n",
    "model2.add(TimeDistributed(Conv2D(256, (3,3), \n",
    "                                 padding='valid', \n",
    "                                 activation='relu')))\n",
    "model2.add(TimeDistributed(Conv2D(256, (3,3), \n",
    "                                 padding='valid', \n",
    "                                 activation='relu')))\n",
    "model2.add(TimeDistributed(MaxPooling2D((2, 2), \n",
    "                                       strides=(1, 1)\n",
    "                                      )))\n",
    "model2.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(GRU(128, return_sequences=False, dropout=0.5))\n",
    "model2.add(Dense(5, activation='softmax'))\n",
    "model2.summary()\n",
    "\n",
    "#Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train.\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimiser = Adam(lr=learning_rate,clipvalue=1.0)\n",
    "model2.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model2.summary())\n",
    "\n",
    "#Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`.\n",
    "\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "\n",
    "\n",
    "model_name = 'cnn_rnn_model_iter3_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "#The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make.\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "\n",
    "#Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch.\n",
    "\n",
    "model2.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3 Summary:\n",
    "In this iteration I am adding more convolution layers to reduce the number of parameters at GRU layer and helps in using the  GRU layer with 128 cells\n",
    "Also instead of using the last images of the video I am using evry alternate image of the each video. \n",
    "Image size \n",
    "img_idx = [9,10,11,12,13,14,15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
    "No of Images considered per sequence:21\n",
    "Batch_size=15\n",
    "epochs=20 # reduced number of epochs since there is no considerable change of 20th epoch\n",
    "model layout:\n",
    "2 conovlution layers of 32 kernel of size (3,3)and relu activaiton,\n",
    "maxpooling layer (2,2),\n",
    "2 comvolution layers with 64 kernels of size (3,3) and relu activaiton,\n",
    "2 comvolution layers with 128 kernels of size (3,3) and relu activaiton,\n",
    "2 comvolution layers with 256 kernels of size (3,3) and relu activaiton,\n",
    "Flatten layer, GRU layer with 64 cells and drop out of 0.5\n",
    "FInal Dense layer with 5 neurons and Softmax output\n",
    "No of Parameters:2,107,173\n",
    "Learning rate=0.001\n",
    "optimiser=Adam\n",
    "train accuracy:21\n",
    "validation accuracy:21\n",
    "This model has the worst perfomance of the three instead of using more convolution layers and GRU cells.\n",
    "This Might be due to the less image size and less number of features to GRU layer compared to previous layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 15\n",
    "\n",
    "## Generator\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29] #Selecting second half of images as the differentiating actions within video would most likely be represented by these images  \n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),100,100,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    #image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = imgio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = resize(image, (100,100,3))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                #print(batch_labels.shape(0))\n",
    "                #print(image.shape(0))\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        diff = len(folder_list) - (num_batches*batch_size)\n",
    "        last_batch_size = diff\n",
    "        batch_data = np.zeros((last_batch_size,len(img_idx),100,100,3))\n",
    "        batch_labels = np.zeros((last_batch_size,5))\n",
    "        for fd in range(diff):\n",
    "            imgs = os.listdir(source_path+'/'+ t[fd + (num_batches*batch_size)].split(';')[0])\n",
    "            for idx,item in enumerate(img_idx):\n",
    "                image = imgio.imread(source_path+'/'+ t[fd + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                image = resize(image, (100,100,3))\n",
    "                batch_data[fd,idx,:,:,0] = image[:,:,0]/255 \n",
    "                batch_data[fd,idx,:,:,1] = image[:,:,1]/255 \n",
    "                batch_data[fd,idx,:,:,2] = image[:,:,2]/255\n",
    "            batch_labels[fd, int(t[fd + (num_batches*batch_size)].strip().split(';')[2])] = 1    \n",
    "        yield batch_data, batch_labels\n",
    "\n",
    "\n",
    "#print(len(train_doc)//50)\n",
    "\n",
    "#generator(source_path, train_doc, 50)\n",
    "\n",
    "#Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture.\n",
    "\n",
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/mnt/disks/user/project/PROJECT/Project_data/train'\n",
    "#train_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "val_path = '/mnt/disks/user/project/PROJECT/Project_data/val'\n",
    "#val_path = 'C://Users/pchadha/Neural_case_study/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 40# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)\n",
    "\n",
    "## Model\n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(TimeDistributed(Conv2D(32, (3, 3), \n",
    "                                 #strides=(1, 1), \n",
    "                                 activation='relu', \n",
    "                                 #padding='valid'\n",
    "                                ), input_shape=(15,100, 100, 3)))\n",
    "model4.add(TimeDistributed(Conv2D(32, (3,3), \n",
    "                                  #strides=(1, 1),\n",
    "                                   #padding='valid',\n",
    "                                  activation='relu')))\n",
    "model4.add(TimeDistributed(MaxPooling2D((2, 2), \n",
    "                                       #strides=(1, 1)\n",
    "                                      )))\n",
    "\n",
    "model4.add(TimeDistributed(Conv2D(64, (3,3), \n",
    "                                  #padding='valid', \n",
    "                                   activation='relu')))\n",
    "model4.add(TimeDistributed(Conv2D(64, (3,3), \n",
    "                                  padding='valid', activation='relu')))\n",
    "model4.add(TimeDistributed(MaxPooling2D((2, 2), \n",
    "                                      #strides=(1, 1)\n",
    "                                      )))\n",
    "\n",
    "\n",
    "model4.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model4.add(Dropout(0.5))\n",
    "model4.add(LSTM(64, return_sequences=False, dropout=0.5))\n",
    "model4.add(Dense(5, activation='softmax'))\n",
    "model4.summary()\n",
    "\n",
    "#Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train.\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimiser = Adam(lr=learning_rate,clipvalue=1.0)\n",
    "model4.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "#print (model4.summary())\n",
    "\n",
    "#Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`.\n",
    "\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'cnn_rnn_model_init_iter4' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "#The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make.\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "\n",
    "#### Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch.\n",
    "\n",
    "model4.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Summary:\n",
    "IN this model I tried running model1 by replacing GRU layer with LSTM Layer\n",
    "Even after replacing the GRU with LSTM we got validation accuracy of 65, but number of parameters has increased "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 15\n",
    "\n",
    "## Generator\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(8,30)] #Selecting second half of images as the differentiating actions within video would most likely be represented by these images  \n",
    "    #img_idx = [x for x in range(12,30)] # Selecting three more images apart from second 15\n",
    "    #img_idx = [x for x in range(0,30,2)] #selected for second and third model and fifth model\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            #batch_data = np.zeros((batch_size,len(img_idx),100,100,3))# x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),48,48,3))\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    #image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = imgio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    # Cropping the image for second model analysis by 10%\n",
    "                    center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                    width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                    left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                    top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                    image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:] \n",
    "                    #image = resize(image, (100,100,3)) # first model instance and fourth model\n",
    "                    # resizing image to 64x64 for second  and third iteration. Also for model-11\n",
    "                    image = resize(image, (48,48,3))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                #print(batch_labels.shape(0))\n",
    "                #print(image.shape(0))\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        diff = len(folder_list) - (num_batches*batch_size)\n",
    "        last_batch_size = diff\n",
    "        #batch_data = np.zeros((last_batch_size,len(img_idx),100,100,3)) # first model instance and fourth model\n",
    "        batch_data = np.zeros((last_batch_size,len(img_idx),48,48,3)) #second and third model instance, model 11 as well\n",
    "        batch_labels = np.zeros((last_batch_size,5))\n",
    "        for fd in range(diff):\n",
    "            imgs = os.listdir(source_path+'/'+ t[fd + (num_batches*batch_size)].split(';')[0])\n",
    "            for idx,item in enumerate(img_idx):\n",
    "                image = imgio.imread(source_path+'/'+ t[fd + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                # Cropping the image for second model analysis by 10%\n",
    "                center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:]\n",
    "                #image = resize(image, (100,100,3)) #first model instance and fourth model\n",
    "                image = resize(image, (48,48,3)) # second and third model instance\n",
    "                batch_data[fd,idx,:,:,0] = image[:,:,0]/255 \n",
    "                batch_data[fd,idx,:,:,1] = image[:,:,1]/255 \n",
    "                batch_data[fd,idx,:,:,2] = image[:,:,2]/255\n",
    "            batch_labels[fd, int(t[fd + (num_batches*batch_size)].strip().split(';')[2])] = 1    \n",
    "        yield batch_data, batch_labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/mnt/disks/user/project/PROJECT/Project_data/train'\n",
    "#train_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "val_path = '/mnt/disks/user/project/PROJECT/Project_data/val'\n",
    "#val_path = 'C://Users/pchadha/Neural_case_study/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 40# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)\n",
    "\n",
    "\n",
    "model5 = Sequential()\n",
    "model5.add(TimeDistributed(Conv2D(32, (3, 3), \n",
    "                                 #strides=(1, 1), \n",
    "                                 activation='relu', \n",
    "                                 #padding='valid'\n",
    "                                ), input_shape=(22,48,48, 3)))\n",
    "\n",
    "model5.add(TimeDistributed(Conv2D(64, (3,3), \n",
    "                                  #padding='valid', \n",
    "                                   activation='relu')))\n",
    "\n",
    "model5.add(TimeDistributed(MaxPooling2D((2, 2), \n",
    "                                      #strides=(1, 1)\n",
    "                                      )))\n",
    "model5.add(Dropout(0.30))\n",
    "model5.add(TimeDistributed(Conv2D(64, (3,3), \n",
    "                                  padding='valid', activation='relu')))\n",
    "model5.add(TimeDistributed(MaxPooling2D((2, 2), \n",
    "                                      #strides=(1, 1)\n",
    "                                      )))\n",
    "model5.add(Dropout(0.30))\n",
    "\n",
    "\n",
    "model5.add(TimeDistributed(Flatten()))\n",
    "model5.add(GRU(128, return_sequences=False, dropout=0.5))\n",
    "model5.add(Dense(256,activation='relu'))\n",
    "model5.add(Dropout(0.50))\n",
    "model5.add(Dense(5, activation='softmax'))\n",
    "model5.summary()\n",
    "\n",
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train.\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimiser = Adam(lr=learning_rate,clipvalue=1.0)\n",
    "model5.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "#print (model4.summary())\n",
    "\n",
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`.\n",
    "\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'cnn_rnn_model_init_iter5' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make.\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "\n",
    "#### Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch.\n",
    "\n",
    "num_epochs=30\n",
    "\n",
    "model5.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Summary\n",
    "THis model we changed reduced the Number of Convolution layers and added more droput layer and Dense layer after GRU layer, It helps in increase of Validation accuracy and more consistent accuracy.\n",
    " In the next model we will increase the number of input images and Involve Batch Normalization.ANd increase neurons in Dense layer\n",
    " Train Accuracy:93\n",
    " Validation Accuracy:73"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 15\n",
    "\n",
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy.\n",
    "\n",
    "#source_path\n",
    "#folder_list\n",
    "#source_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(5,30)] #Selecting second half of images as the differentiating actions within video would most likely be represented by these images  \n",
    "    #img_idx = [x for x in range(12,30)] # Selecting three more images apart from second 15\n",
    "    #img_idx = [x for x in range(0,30,2)] #selected for second and third model and fifth model\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            #batch_data = np.zeros((batch_size,len(img_idx),100,100,3))# x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),48,48,3))\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    #image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = imgio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    # Cropping the image for second model analysis by 10%\n",
    "                    center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                    width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                    left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                    top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                    image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:] \n",
    "                    #image = resize(image, (100,100,3)) # first model instance and fourth model\n",
    "                    # resizing image to 64x64 for second  and third iteration. Also for model-11\n",
    "                    image = resize(image, (48,48,3))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                #print(batch_labels.shape(0))\n",
    "                #print(image.shape(0))\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        diff = len(folder_list) - (num_batches*batch_size)\n",
    "        last_batch_size = diff\n",
    "        #batch_data = np.zeros((last_batch_size,len(img_idx),100,100,3)) # first model instance and fourth model\n",
    "        batch_data = np.zeros((last_batch_size,len(img_idx),48,48,3)) #second and third model instance, model 11 as well\n",
    "        batch_labels = np.zeros((last_batch_size,5))\n",
    "        for fd in range(diff):\n",
    "            imgs = os.listdir(source_path+'/'+ t[fd + (num_batches*batch_size)].split(';')[0])\n",
    "            for idx,item in enumerate(img_idx):\n",
    "                image = imgio.imread(source_path+'/'+ t[fd + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                # Cropping the image for second model analysis by 10%\n",
    "                center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:]\n",
    "                #image = resize(image, (100,100,3)) #first model instance and fourth model\n",
    "                image = resize(image, (48,48,3)) # second and third model instance\n",
    "                batch_data[fd,idx,:,:,0] = image[:,:,0]/255 \n",
    "                batch_data[fd,idx,:,:,1] = image[:,:,1]/255 \n",
    "                batch_data[fd,idx,:,:,2] = image[:,:,2]/255\n",
    "            batch_labels[fd, int(t[fd + (num_batches*batch_size)].strip().split(';')[2])] = 1    \n",
    "        yield batch_data, batch_labels\n",
    "\n",
    "\n",
    "\n",
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/mnt/disks/user/project/PROJECT/Project_data/train'\n",
    "#train_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "val_path = '/mnt/disks/user/project/PROJECT/Project_data/val'\n",
    "#val_path = 'C://Users/pchadha/Neural_case_study/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 40# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)\n",
    "\n",
    "## Model\n",
    "\n",
    "model6 = Sequential()\n",
    "model6.add(TimeDistributed(Conv2D(32, (3, 3), \n",
    "                                 #strides=(1, 1), \n",
    "                                 activation='relu', \n",
    "                                 #padding='valid'\n",
    "                                ), input_shape=(25,48,48, 3)))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(TimeDistributed(Conv2D(64, (3,3), \n",
    "                                  #padding='valid', \n",
    "                                   activation='relu')))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(TimeDistributed(MaxPooling2D((2, 2), \n",
    "                                      #strides=(1, 1)\n",
    "                                      )))\n",
    "model6.add(Dropout(0.30))\n",
    "model6.add(TimeDistributed(Conv2D(64, (3,3), \n",
    "                                  padding='valid', activation='relu')))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(TimeDistributed(MaxPooling2D((2, 2), \n",
    "                                      #strides=(1, 1)\n",
    "                                      )))\n",
    "model6.add(Dropout(0.30))\n",
    "\n",
    "\n",
    "model6.add(TimeDistributed(Flatten()))\n",
    "model6.add(GRU(128, return_sequences=False, dropout=0.5))\n",
    "model6.add(Dense(512,activation='relu'))\n",
    "model6.add(Dropout(0.50))\n",
    "model6.add(Dense(5, activation='softmax'))\n",
    "model6.summary()\n",
    "\n",
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train.\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimiser = Adam(lr=learning_rate,clipvalue=1.0)\n",
    "model6.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "#print (model4.summary())\n",
    "\n",
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`.\n",
    "\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'cnn_rnn_model_init_iter6' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make.\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "\n",
    "#### Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch.\n",
    "\n",
    "num_epochs=30\n",
    "\n",
    "model6.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "\n",
    "                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Summary\n",
    "IN this model I have introduced the Batch Normalization and Increased the Number of Input Images. The validation accuracy didn't improve much \n",
    "But I got consistent validation accuracy of 73 across the epochs.\n",
    "The best accuracy is\n",
    "Train accuracy: 87\n",
    "Validation accuracy:73"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Decision\n",
    "Please note the following:\n",
    "1. CNN3D Model architecture is used by Prashant for this gesture classification exercise\n",
    "2. CNN + RNN Model architecture is used by Pitchy for this gesture classification exercise\n",
    "The best performance for CNN3D model is by model-20, i,e., with training accuracy score of 93% and validation score of 78%. \n",
    "The best performance by CNN + RNN model is given by model-6, i.e., with training accuracy score of 87 and validation score of 73%.\n",
    "Now both models need optimiztaion to remove overfitting but considering higher validation score as well as much lower training parameters and computational time required by CNN3D architecture, we are considering CNN3D as main model for classification of video for these sets of gestures.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model, Model-20 (CNN3D) architecture and generator function for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_path\n",
    "#folder_list\n",
    "#source_path = 'C://Users/pchadha/Neural_case_study/Project_data/train'\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(7,30)] #Selecting second half of images as the differentiating actions within video would most likely be represented by these images  \n",
    "    #img_idx = [x for x in range(12,30)] # Selecting three more images apart from second 15\n",
    "    #img_idx = [x for x in range(0,30,2)] #selected for second and third model and fifth model\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            #batch_data = np.zeros((batch_size,len(img_idx),100,100,3))# x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),48,48,3))\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    #image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = imgio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    # Cropping the image for second model analysis by 10%\n",
    "                    center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                    width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                    left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                    top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                    image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:] \n",
    "                    #image = resize(image, (100,100,3)) # first model instance and fourth model\n",
    "                    # resizing image to 64x64 for second  and third iteration. Also for model-11\n",
    "                    image = resize(image, (48,48,3))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                \n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                #print(batch_labels.shape(0))\n",
    "                #print(image.shape(0))\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        diff = len(folder_list) - (num_batches*batch_size)\n",
    "        last_batch_size = diff\n",
    "        #batch_data = np.zeros((last_batch_size,len(img_idx),100,100,3)) # first model instance and fourth model\n",
    "        batch_data = np.zeros((last_batch_size,len(img_idx),48,48,3)) #second and third model instance, model 11 as well\n",
    "        batch_labels = np.zeros((last_batch_size,5))\n",
    "        for fd in range(diff):\n",
    "            imgs = os.listdir(source_path+'/'+ t[fd + (num_batches*batch_size)].split(';')[0])\n",
    "            for idx,item in enumerate(img_idx):\n",
    "                image = imgio.imread(source_path+'/'+ t[fd + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                # Cropping the image for second model analysis by 10%\n",
    "                center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n",
    "                width_scaled, height_scaled = image.shape[1] * 0.95, image.shape[0] * 0.95 # Corrected the scaling factor for model 5 iteration\n",
    "                left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "                top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "                image = image[int(top_y):int(bottom_y), int(left_x):int(right_x),:]\n",
    "                #image = resize(image, (100,100,3)) #first model instance and fourth model\n",
    "                image = resize(image, (48,48,3)) # second and third model instance\n",
    "                batch_data[fd,idx,:,:,0] = image[:,:,0]/255 \n",
    "                batch_data[fd,idx,:,:,1] = image[:,:,1]/255 \n",
    "                batch_data[fd,idx,:,:,2] = image[:,:,2]/255\n",
    "            batch_labels[fd, int(t[fd + (num_batches*batch_size)].strip().split(';')[2])] = 1    \n",
    "        yield batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 20\n",
    "model20 = Sequential()\n",
    "#help(Conv3D)\n",
    "# a keras convolutional layer is called Conv3D\n",
    "\n",
    "# note that the first layer needs to be told the input shape explicitly\n",
    "\n",
    "# first conv layer\n",
    "img_rows = 48\n",
    "img_cols = 48\n",
    "imnum = 23 # len(img_idx)\n",
    "num_classes = 5\n",
    "input_shape = (imnum, img_rows, img_cols, 3)\n",
    "model20.add(Conv3D(32, kernel_size=(3, 3, 3),\n",
    "                 activation='relu', \n",
    "                 input_shape=input_shape)) # input shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# second conv layer\n",
    "model20.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model20.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model20.add(Dropout(0.35))\n",
    "\n",
    "# Third conv layer\n",
    "model20.add(Conv3D(64, kernel_size=(3, 3, 3), \n",
    "                 activation='relu'))\n",
    "model20.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model20.add(Dropout(0.35))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model20.add(Flatten())\n",
    "model20.add(Dense(512, activation='relu')) # fully connected\n",
    "model20.add(Dropout(0.5))\n",
    "\n",
    "# softmax layer\n",
    "model20.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
